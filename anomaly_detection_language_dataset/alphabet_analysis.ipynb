{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c43e605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading in english_train from data/corpus.pkl\n",
      "loading in english_train from data/english_train.pkl\n",
      "loading in corpus_sample_df from data/corpus_sample.pkl\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "import nlpsig\n",
    "\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "import roughpy as rp\n",
    "\n",
    "from load_data import seed, english_train, corpus_sample_df\n",
    "\n",
    "import time\n",
    "from signature_mahalanobis_knn import SignatureMahalanobisKNN\n",
    "from signature_mahalanobis_knn.utils import (\n",
    "    compute_auc_given_dists,\n",
    "    plot_cdf_given_dists,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e481f7d-7228-49a5-81a4-c9ccd5b2795a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Anomaly Detection task\n",
    "\n",
    "In natural language processing (NLP) tasks, we often are dealing with high dimensional streams of data. Neural network architectures known as Transformers have been shown to be very effective in NLP tasks, and we can use these to obtain high dimensional streams of embeddings for words/tokenised text. In this notebook, we will look at how we can use path signature techniques to analyse these high dimensional streams of embeddings. In particular, we will look at how we can perform outlier detection on the path signatures of the embeddings obtained from a pre-trained Transformer. \n",
    "\n",
    "In particular, we consider the task of determining whether a word is an english word or not by using the path signature of the stream of _character_ embeddings of the word. That is, each word is represented as a stream of character embeddings. We do this by training a Transformer model (from scratch) on a _masked language modelling_ task (or _Cloze_ task) as described in [[1]](https://arxiv.org/abs/1810.04805) using a corpus of english words, and then use this model to obtain a stream of (character) embeddings for a sample of (english and non-english) words. Path signature techniques are applied to analyse the streams of embeddings and finally we attempt to detect the non-english words as outliers in the space of path signatures.\n",
    "\n",
    "Since we are dealing with high dimensional streams of data, we will use a dimension reduction technique to reduce the dimension of the embeddings before computing the path signature. We will look at how we can perform outlier detection on the path signatures of the dimension-reduced embeddings.\n",
    "\n",
    "The pipeline for this task is as follows:\n",
    "1. Train a Transformer model on a masked language modelling task using a corpus of english words.\n",
    "2. Obtain a stream of character embeddings for a sample of english and non-english words using the trained model (note that we ensure that the english words in this sample are not in the training corpus to pre-train the Transformer).\n",
    "    - The english words in this sample are our _inlier_ class while the non-english words are our _outlier_ class in this example.\n",
    "3. Perform dimension reduction on the streams of embeddings.\n",
    "4. Compute the path signature of the dimension-reduced embeddings.\n",
    "5. Perform outlier detection on the path signatures to detect the non-english words.\n",
    "\n",
    "In this notebook, we illustrate how we can use the [`nlpsig`](https://github.com/datasig-ac-uk/nlpsig) package to utilise transformers in order to obtain streams of high dimensional embeddings, which can then be analysed using path signature techniques.\n",
    "\n",
    "Furthermore, we utilise the [`signature_mahalanobis_knn`](https://github.com/datasig-ac-uk/signature_mahalanobis_knn) library to perform outlier detection on the path signatures. Full details of our approach can be found in [[2]](https://arxiv.org/abs/2006.03487) _Dimensionless Anomaly Detection on Multivariate Streams with Variance Norm and Path Signature_ by Zhen Shao, Ryan Sze-Yin Chan, Thomas Cochrane, Peter Foster, Terry Lyons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d3859",
   "metadata": {},
   "source": [
    "## Language dataset\n",
    "\n",
    "In the `data/` folder, we have several text folders of words from different languages:\n",
    "- `wordlist_de.txt`: German words\n",
    "- `wordlist_en.txt`: English words\n",
    "- `wordlist_fr.txt`: French words\n",
    "- `wordlist_it.txt`: Italian words\n",
    "- `wordlist_pl.txt`: Polish words\n",
    "- `wordlist_sv.txt`: Swedish words\n",
    "\n",
    "We additionally have a `alphabet.txt` file which just stores the alphabet characters ('a', 'b', 'c', ...).\n",
    "\n",
    "The task is to split the words into its individual characters and to obtain an embedding for each of them. We can represent a word by a path of its character embeddings and compute its path signature to use as features in predicting the language for which the word belongs.\n",
    "\n",
    "Here we look at obtaining embeddings using a Transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aed597b",
   "metadata": {},
   "source": [
    "## Prepare training data and test data\n",
    "\n",
    "We prepare our data in the [`language_dataset_anomalies_data.ipynb`](language_dataset_anomalies_data.ipynb) notebook and load in our data using the [`load_data.py`](load_data.py) script, so look in there for more details.\n",
    "\n",
    "Our test data will consist a sample of 10000 english words and 10000 non-english words (2000 from each of the remaining languages). We will use the remaining english words as our training data to train the Transformer model and as our corpus of \"normality\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "739f58df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>knots</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stalemating</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>whoops</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>implantation</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>levers</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70610</th>\n",
       "      <td>forcefulness</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70611</th>\n",
       "      <td>fat</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70612</th>\n",
       "      <td>creakier</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70613</th>\n",
       "      <td>ramming</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70614</th>\n",
       "      <td>facsimiles</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70615 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               word language\n",
       "0             knots       en\n",
       "1       stalemating       en\n",
       "2            whoops       en\n",
       "3      implantation       en\n",
       "4            levers       en\n",
       "...             ...      ...\n",
       "70610  forcefulness       en\n",
       "70611           fat       en\n",
       "70612      creakier       en\n",
       "70613       ramming       en\n",
       "70614    facsimiles       en\n",
       "\n",
       "[70615 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b0681e",
   "metadata": {},
   "source": [
    "To make the dataset bit more manageable, we take a sample of each of the languages. In our resulting corpus, we have an equal amount of english (inliers) and non-english words (outliers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e53354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "language\n",
       "en    10000\n",
       "de     2000\n",
       "fr     2000\n",
       "it     2000\n",
       "pl     2000\n",
       "sv     2000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sample_df[\"language\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b80c6a0f-cdee-41dc-9c25-467a1cda7344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abblendet</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bestechendes</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>narrensicheren</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inakzeptable</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abbestelle</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word language\n",
       "0       abblendet       de\n",
       "1    bestechendes       de\n",
       "2  narrensicheren       de\n",
       "3    inakzeptable       de\n",
       "4      abbestelle       de"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdcaf11",
   "metadata": {},
   "source": [
    "## Training a language model\n",
    "\n",
    "We want to train a masked language model for our corpus of English words. In particular, we mask out particular letters and ask our model to try predict the masked letter.\n",
    "We do this using the [`nlpsig.TextEncoder`](https://nlpsig.readthedocs.io/en/latest/encode_text.html#nlpsig.encode_text.TextEncoder) class which provides a wrapper around the [`transformers`](https://huggingface.co/docs/transformers/index) library, and have done this in a separate notebook - see [`train_english_char_bert.ipynb`](train_english_char_bert.ipynb). \n",
    "\n",
    "In that notebook, we trained a [Roberta](https://huggingface.co/docs/transformers/model_doc/roberta) language model described in [[3]](https://arxiv.org/abs/1907.11692), and uploaded the model to the [Huggingface model hub](https://huggingface.co/rchan26/english-char-roberta). We can easily load this model in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e46427e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"rchan26/english-char-roberta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "416c87ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train_encoder = nlpsig.TextEncoder(\n",
    "    df=english_train,\n",
    "    feature_name=\"word\",\n",
    "    model_name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e29d131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] By default, `.load_pretrained_model()` uses `AutoModel` to load in the model. If you want to load the model for a specific task, reset the `.model` attribute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at rchan26/english-char-roberta and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "english_train_encoder.load_pretrained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f13cbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(57, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(52, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_train_encoder.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738fcff4-26a6-413c-80ce-15525e07ae7c",
   "metadata": {},
   "source": [
    "However, by default, the [`nlpsig.TextEncoder.load_pretrained_model`](https://nlpsig.readthedocs.io/en/latest/encode_text.html#nlpsig.encode_text.TextEncoder.load_pretrained_model) method will use the [`transformers.AutoModel.from_pretrained`](https://huggingface.co/docs/transformers/v4.36.1/en/model_doc/auto#transformers.AutoModel) method which may randomly initialise some new weights. For more control, you can just do this manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14186065",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train_encoder.model = RobertaForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bca8c4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(57, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(52, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=57, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_train_encoder.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5646433f",
   "metadata": {},
   "source": [
    "## Obtaining token and word embeddings\n",
    "\n",
    "There are many ways in which one can get token embeddings (here, each character is a token) from the transformer network, as the output is the layers for the full network. A few ways are by using the [`nlpsig.TextEncoder.obtain_embeddings`](https://nlpsig.readthedocs.io/en/latest/encode_text.html#nlpsig.encode_text.TextEncoder.obtain_embeddings) method:\n",
    "\n",
    "- Returning the output of a particular hidden layer\n",
    "    - use `.obtain_embeddings(method = \"hidden_layer\", layers = l)` where `l` is the layer you want\n",
    "    - If no layer is requested, it will just give you the second-to-last hidden layer of the transformer network.\n",
    "- Concatenate the output of several hidden layers\n",
    "    - use `.obtain_embeddings(method = \"concatenate\", layers = [l_1, l_2, ...])` where `[l_1, l_2, ...]` is a list of layers you want to concatenate\n",
    "- Element-wise sum the output of several hidden layers\n",
    "    - use `.obtain_embeddings(method = \"sum\" , layers = [l_1, l_2, ...])` where `[l_1, l_2, ...]` is a list of layers you want to sum\n",
    "- Mean the output of several hidden layers\n",
    "    - use `.obtain_embeddings(method = \"mean\" , layers = [l_1, l_2, ...])` where `[l_1, l_2, ...]` is a list of layers you want to take the mean average over\n",
    "\n",
    "Each of these methods will return a 2-dimensional array with dimensions `[token, embedding]`.\n",
    "\n",
    "If a more custom way to obtain embeddings from the hidden layers, you can specify what layers you want, and it will return them (i.e. using `.obtain_embeddings(method = \"hidden_layer\", layers = [l_1, l_2, ...])` where `[l_1, l_2, ...]` is a list of hidden layers you want) and so the output will be a 3-dimensional array with dimensions `[layer, token, embedding]` for which you would need to combine in such a way that you would have an embedding for each token. The above methods would return a 2-dimensional array with dimensions `[token, embedding]`.\n",
    "\n",
    "In the below, we just obtain the last hidden layer of the network (the 6th one in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad57446b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Setting return_special_tokens_mask=True\n",
      "[INFO] Tokenizing the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6417aa779e374b99b4a049b2beba485b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70615 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving the tokenized text for each sentence into `.df['tokens']`...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e7c270253e4dcd9ad531480a703bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70615 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating tokenized dataframe and setting in `.tokenized_df` attribute...\n",
      "[INFO] Note: 'text_id' is the column name for denoting the corresponding text id\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['word', 'language', 'input_ids', 'attention_mask', 'special_tokens_mask', 'tokens'],\n",
       "    num_rows: 70615\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_train_encoder.tokenize_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06e80725-925f-4ada-a58a-02e0b71ff023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af90e3d91a14448796f609b38ec38aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/707 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# setting the model to use the CPU\n",
    "english_train_encoder.model.to(\"cpu\")\n",
    "english_token_embeddings = english_train_encoder.obtain_embeddings(\n",
    "    method=\"hidden_layer\", layers=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73afe43d-26f8-437b-9a32-ceee73fb88d1",
   "metadata": {},
   "source": [
    "By inspecting the shape of this, we can see that we have a 2-dimensional array with dimensions `[token, embedding]` where the embeddings are 768 dimensional in this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "282d5b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(601949, 768)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_token_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f09e23",
   "metadata": {},
   "source": [
    "Now that we have token embeddings for each text, it is possible to pool these embeddings to obtain an embedding for the full text (for this case, this embedding would represent the word itself. We can use the [`nlpsig.TextEncoder.pool_token_embeddings`](https://nlpsig.readthedocs.io/en/latest/encode_text.html#nlpsig.encode_text.TextEncoder.pool_token_embeddings) method for doing this.\n",
    "\n",
    "Again, there are several methods and full details can be found in the documentation, but a few are:\n",
    "\n",
    "- Taking the mean of the token embeddings\n",
    "    - use `.pool_token_embeddings(method = \"mean\")`\n",
    "- Taking the element-wise max of the token embeddings\n",
    "    - use `.pool_token_embeddings(method = \"max\")`\n",
    "- Taking the element-wise sum of the token embeddings\n",
    "    - use `.pool_token_embeddings(method = \"sum\")`\n",
    "- Taking the token-embedding for the CLS token (a special token that is used in some transformers like BERT and RoBERTa)\n",
    "    - but this is only available to us if we set `skip_special_tokens=False` when tokenizing the text with `.tokenize_text()` method (note by default, this is set to `True` and so we don't have access to this method here)\n",
    "    - use `.pool_token_embeddings(method = \"cls\")`\n",
    "        - note this will produce an error if the CLS token is not available...\n",
    "\n",
    "For example, to pool the character embeddings by taking the mean of the token embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcfd84d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9435011183f8447887106959a48293de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70615 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pooled_english_mean = english_train_encoder.pool_token_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b491dc",
   "metadata": {},
   "source": [
    "Again, we can inspect the shape and we can see that we have embeddings for each of our words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c876954d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70615, 768)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_english_mean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c0fffd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dimension reduction\n",
    "\n",
    "We can perform dimension reduction with the [`nlpsig.DimReduce`](https://nlpsig.readthedocs.io/en/latest/dimensionality_reduction.html#nlpsig.dimensionality_reduction.DimReduce) class. Here, we will use Gaussian Random Projections (implemented using [`scikit-learn`](https://scikit-learn.org/stable/modules/random_projection.html)) by setting `method=\"gaussian_random_projection\"`, but there are other standard methods available:\n",
    "- UMAP [[4]](https://arxiv.org/abs/1802.03426) (implemented using the [`umap-learn`](https://umap-learn.readthedocs.io/en/latest/api.html))\n",
    "    - `method=\"umap\"`\n",
    "- PCA [[5]](http://www.miketipping.com/papers/met-mppca.pdf) (implemented using [`scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html))\n",
    "    - `method=\"pca\"`\n",
    "- TSNE [[6]](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) (implemented using [`scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html))\n",
    "    - `method=\"tsne\"`\n",
    "- Post Processing Algorithm (PPA) with PCA (PPA-PCA) [[7]](https://arxiv.org/abs/1702.01417)\n",
    "    - `method=\"ppapca\"`\n",
    "- PPA-PCA-PPA [[8]](https://aclanthology.org/W19-4328/)\n",
    "    - `method=\"ppapacppa\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "981f1c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction = nlpsig.DimReduce(\n",
    "    method=\"gaussian_random_projection\",\n",
    "    n_components=25,\n",
    ")\n",
    "\n",
    "english_token_embeddings_reduced = reduction.fit_transform(\n",
    "    english_token_embeddings, random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b21f27a-2133-4cf1-a693-98b08407216d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(601949, 25)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_token_embeddings_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1aaa4c-f9b9-4c50-b8b0-b8add1923e08",
   "metadata": {},
   "source": [
    "We can save these embeddings for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6c279ac-2184-4ac2-802a-1d044caee6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"english_token_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(english_token_embeddings, f)\n",
    "with open(f\"english_reduced_token_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(english_token_embeddings_reduced, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81af6467",
   "metadata": {},
   "source": [
    "As we have embeddings for each token, we can obtain a path for each word by constructing a path of the token embeddings. To do this, we can use the [`nlpsig.PrepareData`](https://nlpsig.readthedocs.io/en/latest/data_preparation.html#nlpsig.data_preparation.PrepareData) class and pass in our tokenized dataframe (the dataframe where we have each token in our data and we also have the corresponding id for each word which is saved in the `text_id` column of the tokenized dataframe.\n",
    "\n",
    "We pass in the column which defines the ids, `text_id`, the column which defines the labels, `language`, the token embeddings and the dimension-reduced embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b2d8c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Note 'datetime' is not a column in `.df`, so only 'timeline_index' is added.\n",
      "[INFO] As 'datetime' is not a column in `.df`, we assume that the data is ordered by time with respect to the id.\n",
      "[INFO] Adding 'timeline_index' feature...\n"
     ]
    }
   ],
   "source": [
    "english_dataset = nlpsig.PrepareData(\n",
    "    original_df=english_train_encoder.tokenized_df,\n",
    "    id_column=\"text_id\",\n",
    "    embeddings=english_token_embeddings,\n",
    "    embeddings_reduced=english_token_embeddings_reduced,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a0e3c-288a-4a23-8851-a68aa6956b61",
   "metadata": {},
   "source": [
    "The class concatenates the embeddings and the dimension-reduced embeddings that are passed into to the class initalisation and stores it in the `.df` attribute of `english_dataset`.\n",
    "\n",
    "Here, the columns beginning with `d` denote the dimensions of the dimension reduced transformer embeddings, whereas the columns beginning with `e` denote the dimensions of embeddings obtained from the transformer.\n",
    "\n",
    "Furthermore, we can see from the printed out information that a `timeline_index` column was added to the dataframe, which is the last column here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05b0bd97-3e75-4b64-9454-91ba79f0c610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "      <th>tokens</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>...</th>\n",
       "      <th>e760</th>\n",
       "      <th>e761</th>\n",
       "      <th>e762</th>\n",
       "      <th>e763</th>\n",
       "      <th>e764</th>\n",
       "      <th>e765</th>\n",
       "      <th>e766</th>\n",
       "      <th>e767</th>\n",
       "      <th>e768</th>\n",
       "      <th>timeline_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>k</td>\n",
       "      <td>6.100298</td>\n",
       "      <td>-14.199132</td>\n",
       "      <td>8.581287</td>\n",
       "      <td>-6.761333</td>\n",
       "      <td>2.903222</td>\n",
       "      <td>8.518986</td>\n",
       "      <td>-1.497842</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.533906</td>\n",
       "      <td>-0.016500</td>\n",
       "      <td>-0.841333</td>\n",
       "      <td>0.967386</td>\n",
       "      <td>-1.594480</td>\n",
       "      <td>1.674844</td>\n",
       "      <td>0.409466</td>\n",
       "      <td>-1.230852</td>\n",
       "      <td>0.450279</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>n</td>\n",
       "      <td>6.067597</td>\n",
       "      <td>-7.157618</td>\n",
       "      <td>3.922382</td>\n",
       "      <td>2.873782</td>\n",
       "      <td>4.717306</td>\n",
       "      <td>-0.953772</td>\n",
       "      <td>-6.488945</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.743229</td>\n",
       "      <td>-0.415965</td>\n",
       "      <td>-0.220593</td>\n",
       "      <td>0.931776</td>\n",
       "      <td>0.089646</td>\n",
       "      <td>-0.172894</td>\n",
       "      <td>1.922119</td>\n",
       "      <td>-0.317665</td>\n",
       "      <td>1.383081</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>o</td>\n",
       "      <td>9.407446</td>\n",
       "      <td>-8.487071</td>\n",
       "      <td>3.602750</td>\n",
       "      <td>-6.851043</td>\n",
       "      <td>7.014906</td>\n",
       "      <td>0.959817</td>\n",
       "      <td>1.894073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.522140</td>\n",
       "      <td>0.108025</td>\n",
       "      <td>-0.334635</td>\n",
       "      <td>1.223601</td>\n",
       "      <td>0.198564</td>\n",
       "      <td>0.080706</td>\n",
       "      <td>2.160928</td>\n",
       "      <td>0.805575</td>\n",
       "      <td>-0.801284</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>t</td>\n",
       "      <td>1.125513</td>\n",
       "      <td>0.428988</td>\n",
       "      <td>3.385085</td>\n",
       "      <td>1.941234</td>\n",
       "      <td>4.131640</td>\n",
       "      <td>-0.509308</td>\n",
       "      <td>7.753160</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.341119</td>\n",
       "      <td>0.648110</td>\n",
       "      <td>0.571438</td>\n",
       "      <td>0.584918</td>\n",
       "      <td>-0.637328</td>\n",
       "      <td>0.751738</td>\n",
       "      <td>1.818443</td>\n",
       "      <td>0.990760</td>\n",
       "      <td>0.115542</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>s</td>\n",
       "      <td>2.313937</td>\n",
       "      <td>0.054572</td>\n",
       "      <td>-2.763758</td>\n",
       "      <td>-3.833287</td>\n",
       "      <td>4.874951</td>\n",
       "      <td>-3.330217</td>\n",
       "      <td>-1.284547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345745</td>\n",
       "      <td>0.988667</td>\n",
       "      <td>-0.534669</td>\n",
       "      <td>2.455460</td>\n",
       "      <td>-0.635259</td>\n",
       "      <td>0.844671</td>\n",
       "      <td>0.778486</td>\n",
       "      <td>0.768183</td>\n",
       "      <td>-1.666858</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601944</th>\n",
       "      <td>70614</td>\n",
       "      <td>en</td>\n",
       "      <td>m</td>\n",
       "      <td>1.639216</td>\n",
       "      <td>0.717788</td>\n",
       "      <td>-1.328302</td>\n",
       "      <td>-12.343803</td>\n",
       "      <td>-4.282854</td>\n",
       "      <td>-9.771315</td>\n",
       "      <td>-2.443641</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038248</td>\n",
       "      <td>0.369698</td>\n",
       "      <td>-1.192643</td>\n",
       "      <td>0.888171</td>\n",
       "      <td>-0.688071</td>\n",
       "      <td>-0.225089</td>\n",
       "      <td>1.232774</td>\n",
       "      <td>0.170042</td>\n",
       "      <td>-0.456451</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601945</th>\n",
       "      <td>70614</td>\n",
       "      <td>en</td>\n",
       "      <td>i</td>\n",
       "      <td>-7.668247</td>\n",
       "      <td>-9.949147</td>\n",
       "      <td>4.931896</td>\n",
       "      <td>-6.880606</td>\n",
       "      <td>0.885773</td>\n",
       "      <td>1.801310</td>\n",
       "      <td>-4.665092</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.070264</td>\n",
       "      <td>-0.510370</td>\n",
       "      <td>-0.729678</td>\n",
       "      <td>0.607366</td>\n",
       "      <td>1.258828</td>\n",
       "      <td>0.519082</td>\n",
       "      <td>0.996714</td>\n",
       "      <td>1.287260</td>\n",
       "      <td>-0.600449</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601946</th>\n",
       "      <td>70614</td>\n",
       "      <td>en</td>\n",
       "      <td>l</td>\n",
       "      <td>1.901520</td>\n",
       "      <td>2.117802</td>\n",
       "      <td>-1.211096</td>\n",
       "      <td>-7.594829</td>\n",
       "      <td>8.067634</td>\n",
       "      <td>-1.348465</td>\n",
       "      <td>-8.490309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026670</td>\n",
       "      <td>-0.873979</td>\n",
       "      <td>-0.078845</td>\n",
       "      <td>-1.068502</td>\n",
       "      <td>-0.159796</td>\n",
       "      <td>0.163264</td>\n",
       "      <td>2.670635</td>\n",
       "      <td>2.315694</td>\n",
       "      <td>1.644772</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601947</th>\n",
       "      <td>70614</td>\n",
       "      <td>en</td>\n",
       "      <td>e</td>\n",
       "      <td>1.672272</td>\n",
       "      <td>-4.530347</td>\n",
       "      <td>-5.324213</td>\n",
       "      <td>5.228519</td>\n",
       "      <td>8.263039</td>\n",
       "      <td>2.614076</td>\n",
       "      <td>-5.229039</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.922539</td>\n",
       "      <td>-0.309774</td>\n",
       "      <td>0.435103</td>\n",
       "      <td>-0.866393</td>\n",
       "      <td>-0.739616</td>\n",
       "      <td>-2.028092</td>\n",
       "      <td>-0.655358</td>\n",
       "      <td>0.945581</td>\n",
       "      <td>-1.189239</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601948</th>\n",
       "      <td>70614</td>\n",
       "      <td>en</td>\n",
       "      <td>s</td>\n",
       "      <td>-0.452957</td>\n",
       "      <td>-0.818362</td>\n",
       "      <td>-7.153192</td>\n",
       "      <td>4.422605</td>\n",
       "      <td>-4.524617</td>\n",
       "      <td>-6.140039</td>\n",
       "      <td>-3.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.930752</td>\n",
       "      <td>1.927993</td>\n",
       "      <td>0.516500</td>\n",
       "      <td>-0.898273</td>\n",
       "      <td>-0.413304</td>\n",
       "      <td>-0.436183</td>\n",
       "      <td>0.755781</td>\n",
       "      <td>2.223152</td>\n",
       "      <td>-0.613211</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>601949 rows Ã— 797 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id language tokens        d1         d2        d3         d4  \\\n",
       "0             0       en      k  6.100298 -14.199132  8.581287  -6.761333   \n",
       "1             0       en      n  6.067597  -7.157618  3.922382   2.873782   \n",
       "2             0       en      o  9.407446  -8.487071  3.602750  -6.851043   \n",
       "3             0       en      t  1.125513   0.428988  3.385085   1.941234   \n",
       "4             0       en      s  2.313937   0.054572 -2.763758  -3.833287   \n",
       "...         ...      ...    ...       ...        ...       ...        ...   \n",
       "601944    70614       en      m  1.639216   0.717788 -1.328302 -12.343803   \n",
       "601945    70614       en      i -7.668247  -9.949147  4.931896  -6.880606   \n",
       "601946    70614       en      l  1.901520   2.117802 -1.211096  -7.594829   \n",
       "601947    70614       en      e  1.672272  -4.530347 -5.324213   5.228519   \n",
       "601948    70614       en      s -0.452957  -0.818362 -7.153192   4.422605   \n",
       "\n",
       "              d5        d6        d7  ...      e760      e761      e762  \\\n",
       "0       2.903222  8.518986 -1.497842  ... -1.533906 -0.016500 -0.841333   \n",
       "1       4.717306 -0.953772 -6.488945  ... -0.743229 -0.415965 -0.220593   \n",
       "2       7.014906  0.959817  1.894073  ... -0.522140  0.108025 -0.334635   \n",
       "3       4.131640 -0.509308  7.753160  ... -0.341119  0.648110  0.571438   \n",
       "4       4.874951 -3.330217 -1.284547  ...  0.345745  0.988667 -0.534669   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "601944 -4.282854 -9.771315 -2.443641  ... -0.038248  0.369698 -1.192643   \n",
       "601945  0.885773  1.801310 -4.665092  ... -1.070264 -0.510370 -0.729678   \n",
       "601946  8.067634 -1.348465 -8.490309  ...  0.026670 -0.873979 -0.078845   \n",
       "601947  8.263039  2.614076 -5.229039  ... -1.922539 -0.309774  0.435103   \n",
       "601948 -4.524617 -6.140039 -3.085102  ... -1.930752  1.927993  0.516500   \n",
       "\n",
       "            e763      e764      e765      e766      e767      e768  \\\n",
       "0       0.967386 -1.594480  1.674844  0.409466 -1.230852  0.450279   \n",
       "1       0.931776  0.089646 -0.172894  1.922119 -0.317665  1.383081   \n",
       "2       1.223601  0.198564  0.080706  2.160928  0.805575 -0.801284   \n",
       "3       0.584918 -0.637328  0.751738  1.818443  0.990760  0.115542   \n",
       "4       2.455460 -0.635259  0.844671  0.778486  0.768183 -1.666858   \n",
       "...          ...       ...       ...       ...       ...       ...   \n",
       "601944  0.888171 -0.688071 -0.225089  1.232774  0.170042 -0.456451   \n",
       "601945  0.607366  1.258828  0.519082  0.996714  1.287260 -0.600449   \n",
       "601946 -1.068502 -0.159796  0.163264  2.670635  2.315694  1.644772   \n",
       "601947 -0.866393 -0.739616 -2.028092 -0.655358  0.945581 -1.189239   \n",
       "601948 -0.898273 -0.413304 -0.436183  0.755781  2.223152 -0.613211   \n",
       "\n",
       "        timeline_index  \n",
       "0                    1  \n",
       "1                    2  \n",
       "2                    3  \n",
       "3                    4  \n",
       "4                    5  \n",
       "...                ...  \n",
       "601944               6  \n",
       "601945               7  \n",
       "601946               8  \n",
       "601947               9  \n",
       "601948              10  \n",
       "\n",
       "[601949 rows x 797 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_dataset.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb187e7",
   "metadata": {},
   "source": [
    "We can construct a path by using the [`nlpsig.PrepareData.pad`](https://nlpsig.readthedocs.io/en/latest/data_preparation.html#nlpsig.data_preparation.PrepareData.pad) method, and result of this is a multi-dimensional array or tensor (in particular a numpy array or PyTorch tensor) which can be then used in some downstream task. It is called \"pad\" because arrays and tensors are rectangular and if there are cases where there isn't enough data (e.g. if a word only has 3 letters/tokens and we want to make paths of length 4), we \"pad\" with either the last token embedding (if we set `zero_padding=False`) or with a vector of zeros (if we set `zero_padding=True`).\n",
    "\n",
    "Here, we construct paths by setting a length of the paths (we call this method `k_last` in the code and we have to specify the length with `k=50` - the maximum sequence length that we used when defining the transformer model).\n",
    "\n",
    "We alternatively can construct to the longest word possible (by setting `method=\"max\"`). The `time_feature` argument allows us to specify what time features we want to keep. Here we don't have any besides the index in which the word is, which is given by `timeline_index` and we choose not to standardise that by specifying `standardise_time_feature=False`.\n",
    "\n",
    "The `pad_by` argument specifies that we are padding for each word (as each word is given a particular `text_id` in the tokenized dataframe above). There is an alternative option to construct a path by looking at the history of a particular embedding (i.e. the stream embeddings that occurred before), but this is not useful here in this context, and we will cover that in another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6ca0623",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_specifics = {\n",
    "    \"pad_by\": \"id\",\n",
    "    \"zero_padding\": True,\n",
    "    \"method\": \"k_last\",\n",
    "    \"k\": 50,\n",
    "    \"features\": [\"timeline_index\"],\n",
    "    \"standardise_method\": [None],\n",
    "    \"embeddings\": \"dim_reduced\",\n",
    "    \"pad_from_below\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19583e9f-f4b9-450e-998c-7416fece349e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734b5ba37dd445df9fd4e389119c5e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70615 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "english_word_path = english_dataset.pad(**path_specifics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccd79294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70615, 50, 27)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_word_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "292fbdd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70615"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_dataset.df[\"text_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7febd295",
   "metadata": {},
   "source": [
    "We also store this array as a dataframe in `.df_padded` so that you can see what the columns correspond to, where columns beginning with `e` denote the dimensions of embeddings obtained from the transformer (here we have none as we only requested to keep the dimension reduced embeddings), and columns beginning with `d` denote the dimensions of the dimension reduced transformer embeddings.\n",
    "\n",
    "We can see for the first word in the dataset (with `text_id==0`), this is a word with 10 letters and we can see how we have padded the word to length 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3217c3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeline_index</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>d8</th>\n",
       "      <th>d9</th>\n",
       "      <th>...</th>\n",
       "      <th>d17</th>\n",
       "      <th>d18</th>\n",
       "      <th>d19</th>\n",
       "      <th>d20</th>\n",
       "      <th>d21</th>\n",
       "      <th>d22</th>\n",
       "      <th>d23</th>\n",
       "      <th>d24</th>\n",
       "      <th>d25</th>\n",
       "      <th>text_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6.100298</td>\n",
       "      <td>-14.199132</td>\n",
       "      <td>8.581287</td>\n",
       "      <td>-6.761333</td>\n",
       "      <td>2.903222</td>\n",
       "      <td>8.518986</td>\n",
       "      <td>-1.497842</td>\n",
       "      <td>2.556130</td>\n",
       "      <td>1.165959</td>\n",
       "      <td>...</td>\n",
       "      <td>3.761808</td>\n",
       "      <td>-3.724018</td>\n",
       "      <td>4.041551</td>\n",
       "      <td>0.700114</td>\n",
       "      <td>5.538017</td>\n",
       "      <td>5.933544</td>\n",
       "      <td>1.699548</td>\n",
       "      <td>-1.927170</td>\n",
       "      <td>-4.084645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6.067597</td>\n",
       "      <td>-7.157618</td>\n",
       "      <td>3.922382</td>\n",
       "      <td>2.873782</td>\n",
       "      <td>4.717306</td>\n",
       "      <td>-0.953772</td>\n",
       "      <td>-6.488945</td>\n",
       "      <td>0.626723</td>\n",
       "      <td>-2.640919</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.538846</td>\n",
       "      <td>8.980839</td>\n",
       "      <td>6.784652</td>\n",
       "      <td>3.221316</td>\n",
       "      <td>9.305283</td>\n",
       "      <td>-9.060441</td>\n",
       "      <td>4.503458</td>\n",
       "      <td>8.616587</td>\n",
       "      <td>4.175737</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9.407446</td>\n",
       "      <td>-8.487071</td>\n",
       "      <td>3.602750</td>\n",
       "      <td>-6.851043</td>\n",
       "      <td>7.014906</td>\n",
       "      <td>0.959817</td>\n",
       "      <td>1.894073</td>\n",
       "      <td>-0.329711</td>\n",
       "      <td>3.118009</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.048263</td>\n",
       "      <td>-2.891371</td>\n",
       "      <td>-0.558437</td>\n",
       "      <td>6.495055</td>\n",
       "      <td>10.367566</td>\n",
       "      <td>0.570897</td>\n",
       "      <td>2.110716</td>\n",
       "      <td>-1.246753</td>\n",
       "      <td>-8.524743</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.125513</td>\n",
       "      <td>0.428988</td>\n",
       "      <td>3.385085</td>\n",
       "      <td>1.941234</td>\n",
       "      <td>4.131640</td>\n",
       "      <td>-0.509308</td>\n",
       "      <td>7.753160</td>\n",
       "      <td>2.754631</td>\n",
       "      <td>-0.687329</td>\n",
       "      <td>...</td>\n",
       "      <td>5.209500</td>\n",
       "      <td>-3.950286</td>\n",
       "      <td>6.697100</td>\n",
       "      <td>1.225492</td>\n",
       "      <td>-7.946608</td>\n",
       "      <td>-0.790830</td>\n",
       "      <td>6.605088</td>\n",
       "      <td>-3.531295</td>\n",
       "      <td>-1.495831</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2.313937</td>\n",
       "      <td>0.054572</td>\n",
       "      <td>-2.763758</td>\n",
       "      <td>-3.833287</td>\n",
       "      <td>4.874951</td>\n",
       "      <td>-3.330217</td>\n",
       "      <td>-1.284547</td>\n",
       "      <td>0.571877</td>\n",
       "      <td>-4.491823</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.038224</td>\n",
       "      <td>6.726155</td>\n",
       "      <td>7.169745</td>\n",
       "      <td>5.381354</td>\n",
       "      <td>13.035870</td>\n",
       "      <td>-2.655374</td>\n",
       "      <td>4.574077</td>\n",
       "      <td>5.040864</td>\n",
       "      <td>-1.897521</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    timeline_index        d1         d2        d3        d4        d5  \\\n",
       "0                1  6.100298 -14.199132  8.581287 -6.761333  2.903222   \n",
       "1                2  6.067597  -7.157618  3.922382  2.873782  4.717306   \n",
       "2                3  9.407446  -8.487071  3.602750 -6.851043  7.014906   \n",
       "3                4  1.125513   0.428988  3.385085  1.941234  4.131640   \n",
       "4                5  2.313937   0.054572 -2.763758 -3.833287  4.874951   \n",
       "5                0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "6                0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "7                0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "8                0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "9                0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "10               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "11               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "12               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "13               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "14               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "15               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "16               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "17               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "18               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "19               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "20               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "21               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "22               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "23               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "24               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "25               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "26               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "27               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "28               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "29               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "30               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "31               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "32               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "33               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "34               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "35               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "36               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "37               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "38               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "39               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "40               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "41               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "42               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "43               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "44               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "45               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "46               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "47               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "48               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "49               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          d6        d7        d8        d9  ...       d17       d18       d19  \\\n",
       "0   8.518986 -1.497842  2.556130  1.165959  ...  3.761808 -3.724018  4.041551   \n",
       "1  -0.953772 -6.488945  0.626723 -2.640919  ... -1.538846  8.980839  6.784652   \n",
       "2   0.959817  1.894073 -0.329711  3.118009  ... -2.048263 -2.891371 -0.558437   \n",
       "3  -0.509308  7.753160  2.754631 -0.687329  ...  5.209500 -3.950286  6.697100   \n",
       "4  -3.330217 -1.284547  0.571877 -4.491823  ... -4.038224  6.726155  7.169745   \n",
       "5   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "6   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "7   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "8   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "9   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "10  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "11  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "12  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "13  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "14  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "15  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "16  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "17  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "18  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "19  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "20  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "21  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "22  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "23  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "24  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "25  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "26  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "27  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "28  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "29  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "30  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "31  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "32  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "33  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "34  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "35  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "36  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "37  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "38  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "39  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "40  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "41  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "42  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "43  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "44  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "45  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "46  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "47  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "48  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "49  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "         d20        d21       d22       d23       d24       d25  text_id  \n",
       "0   0.700114   5.538017  5.933544  1.699548 -1.927170 -4.084645        0  \n",
       "1   3.221316   9.305283 -9.060441  4.503458  8.616587  4.175737        0  \n",
       "2   6.495055  10.367566  0.570897  2.110716 -1.246753 -8.524743        0  \n",
       "3   1.225492  -7.946608 -0.790830  6.605088 -3.531295 -1.495831        0  \n",
       "4   5.381354  13.035870 -2.655374  4.574077  5.040864 -1.897521        0  \n",
       "5   0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "6   0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "7   0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "8   0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "9   0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "10  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "11  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "12  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "13  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "14  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "15  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "16  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "17  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "18  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "19  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "20  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "21  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "22  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "23  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "24  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "25  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "26  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "27  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "28  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "29  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "30  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "31  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "32  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "33  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "34  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "35  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "36  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "37  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "38  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "39  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "40  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "41  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "42  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "43  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "44  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "45  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "46  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "47  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "48  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "49  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "\n",
       "[50 rows x 27 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# still has the labels and the ids\n",
    "english_dataset.df_padded[english_dataset.df_padded[\"text_id\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40969a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word                  knots\n",
       "language                 en\n",
       "tokens      [k, n, o, t, s]\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_train_encoder.df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07994e10",
   "metadata": {},
   "source": [
    "For the padded rows, we give these a label `-1` to denote that they have been added.\n",
    "\n",
    "Note that for padding, the method pads from below by default, but we can pad by above by setting `pad_from_below=False`.\n",
    "\n",
    "To obtain a path as a Numpy array, we use the [`nlpsig.PrepareData.get_path`](https://nlpsig.readthedocs.io/en/latest/data_preparation.html#nlpsig.data_preparation.PrepareData.get_path) method which by default keeps the time features and will remove the id and label columns. We make this more explicit by setting `include_features=True` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9f2ba04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70615, 50, 26)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_word_path = english_dataset.get_path(include_features=True)\n",
    "english_word_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb5e2f91-f171-4ced-9f93-60bf977b6b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.        ,   6.1002984 , -14.19913197, ...,   1.69954813,\n",
       "         -1.92717004,  -4.08464479],\n",
       "       [  2.        ,   6.06759691,  -7.15761757, ...,   4.50345755,\n",
       "          8.61658669,   4.17573738],\n",
       "       [  3.        ,   9.40744591,  -8.48707104, ...,   2.11071587,\n",
       "         -1.24675322,  -8.52474308],\n",
       "       ...,\n",
       "       [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_word_path[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b1ad55",
   "metadata": {},
   "source": [
    "## Obtaining a paths and signatures for words in `corpus_df`\n",
    "\n",
    "Now that we have trained our model and obtained signatures for each word in our sample of english words, we also want to obtain embeddings for the words in `corpus_sample_df`. Currently, [`nlpsig.TextEncoder`](https://nlpsig.readthedocs.io/en/latest/encode_text.html#nlpsig.encode_text.TextEncoder) only works with the data that is passed into the function and stored in the `.df` and `.dataset` attributes, so we need to initialise a new [`nlpsig.TextEncoder`](https://nlpsig.readthedocs.io/en/latest/encode_text.html#nlpsig.encode_text.TextEncoder) object with the `corpus_sample_df` dataframe and also the trained model.\n",
    "\n",
    "We can then obtain embeddings easily (recall from above we first need to tokenize the text, and then use the [`nlpsig.TextEncoder.obtain_embeddings`](https://nlpsig.readthedocs.io/en/latest/encode_text.html#nlpsig.encode_text.TextEncoder.obtain_embeddings) and [`nlpsig.TextEncoder.pool_token_embeddings`](https://nlpsig.readthedocs.io/en/latest/encode_text.html#nlpsig.encode_text.TextEncoder.pool_token_embeddings) methods to do this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "015309f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sample_encoder = nlpsig.TextEncoder(\n",
    "    df=corpus_sample_df,\n",
    "    feature_name=\"word\",\n",
    "    model=english_train_encoder.model,\n",
    "    config=english_train_encoder.config,\n",
    "    tokenizer=english_train_encoder.tokenizer,\n",
    "    data_collator=english_train_encoder.data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83327a8e-2192-4efd-ac04-81f57999d47a",
   "metadata": {},
   "source": [
    "Note that since we're just loading in our pretrained model from above, we could also just have passed in the path to the model directly via the `model_name` argument, and use the [`nlpsig.TextEncoder.load_pretrained_model`](https://nlpsig.readthedocs.io/en/latest/encode_text.html#nlpsig.encode_text.TextEncoder.load_pretrained_model) method which loads in the model, config, tokenizer and data collator that was used. However, we can also use the approach as we did earlier and used the [`nlpsig.TextEncoder.load_pretrained_model`](https://nlpsig.readthedocs.io/en/latest/encode_text.html#nlpsig.encode_text.TextEncoder.load_pretrained_model) method would load the model with [`transformers.AutoModel.from_pretrained`](https://huggingface.co/docs/transformers/v4.36.1/en/model_doc/auto#transformers.AutoModel) and also the tokenizer related to that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d0d9042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Setting return_special_tokens_mask=True\n",
      "[INFO] Tokenizing the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20fa058a7db8454ea5029b54c55066f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving the tokenized text for each sentence into `.df['tokens']`...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f1eff5621b4c189209aff1b6277ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating tokenized dataframe and setting in `.tokenized_df` attribute...\n",
      "[INFO] Note: 'text_id' is the column name for denoting the corresponding text id\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['word', 'language', 'input_ids', 'attention_mask', 'special_tokens_mask', 'tokens'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sample_encoder.tokenize_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ed8f78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198506</th>\n",
       "      <td>19999</td>\n",
       "      <td>en</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198507</th>\n",
       "      <td>19999</td>\n",
       "      <td>en</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198508</th>\n",
       "      <td>19999</td>\n",
       "      <td>en</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198509</th>\n",
       "      <td>19999</td>\n",
       "      <td>en</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198510</th>\n",
       "      <td>19999</td>\n",
       "      <td>en</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198511 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id language tokens\n",
       "0             0       de      a\n",
       "1             0       de      b\n",
       "2             0       de      b\n",
       "3             0       de      l\n",
       "4             0       de      e\n",
       "...         ...      ...    ...\n",
       "198506    19999       en      m\n",
       "198507    19999       en      u\n",
       "198508    19999       en      r\n",
       "198509    19999       en      g\n",
       "198510    19999       en      y\n",
       "\n",
       "[198511 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sample_encoder.tokenized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a089032-0f58-47e7-9922-3778da16483c",
   "metadata": {},
   "source": [
    "After tokenizing, we can obtain token embeddings and also pool these token embeddings with `[`nlpsig.TextEncoder.obtain_embeddings`](https://nlpsig.readthedocs.io/en/latest/encode_text.html#nlpsig.encode_text.TextEncoder.obtain_embeddings) and [`nlpsig.TextEncoder.pool_token_embeddings`](https://nlpsig.readthedocs.io/en/latest/encode_text.html#nlpsig.encode_text.TextEncoder.pool_token_embeddings) methods available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0334ac45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31b6f65dc334cddbf4e08591616b4a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_embeddings = corpus_sample_encoder.obtain_embeddings(\n",
    "    method=\"hidden_layer\", layers=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c1885be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198511, 768)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7838beaf",
   "metadata": {},
   "source": [
    "To reduce the embeddings, we want to use the same transform that we used earlier on. Recall that we used Gaussian random projections using the [`scikit-learn`](https://scikit-learn.org/stable/modules/random_projection.html) package. After fitting and transforming with the vectors in `english_token_embeddings`, we stored the `sklearn.random_projection.GaussianRandomProjection` object in `reduction.reducer` which we can use again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22d7cbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.random_projection.GaussianRandomProjection"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(reduction.reducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74359714",
   "metadata": {},
   "source": [
    "We can then transform new data using the `.transform()` method of the `sklearn.random_projection.GaussianRandomProjection` class which will use the same transformation that we fitted to above when applying dimension reduction to the token embeddings for our corpus of english words (in `english_train`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d27b6ad-8a26-41d5-bf5c-54bc8ada6b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_reduced = reduction.reducer.transform(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b3853cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198511, 25)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf35fb7-0d6f-4fe9-ad11-6f0919e2fe7d",
   "metadata": {},
   "source": [
    "Optionally, we can save these embeddings for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e35da37-a73b-48d4-ad62-de97f99c78db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"corpus_sample_token_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(token_embeddings, f)\n",
    "with open(f\"corpus_sample_reduced_token_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embeddings_reduced, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3886dbb3",
   "metadata": {},
   "source": [
    "We again obtain paths with the `PrepareData` class, and pass in the tokenized dataframe created in `corpus_sample_encoder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9d352f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198506</th>\n",
       "      <td>19999</td>\n",
       "      <td>en</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198507</th>\n",
       "      <td>19999</td>\n",
       "      <td>en</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198508</th>\n",
       "      <td>19999</td>\n",
       "      <td>en</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198509</th>\n",
       "      <td>19999</td>\n",
       "      <td>en</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198510</th>\n",
       "      <td>19999</td>\n",
       "      <td>en</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198511 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id language tokens\n",
       "0             0       de      a\n",
       "1             0       de      b\n",
       "2             0       de      b\n",
       "3             0       de      l\n",
       "4             0       de      e\n",
       "...         ...      ...    ...\n",
       "198506    19999       en      m\n",
       "198507    19999       en      u\n",
       "198508    19999       en      r\n",
       "198509    19999       en      g\n",
       "198510    19999       en      y\n",
       "\n",
       "[198511 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sample_encoder.tokenized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83bd0e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Note 'datetime' is not a column in `.df`, so only 'timeline_index' is added.\n",
      "[INFO] As 'datetime' is not a column in `.df`, we assume that the data is ordered by time with respect to the id.\n",
      "[INFO] Adding 'timeline_index' feature...\n"
     ]
    }
   ],
   "source": [
    "corpus_dataset = nlpsig.PrepareData(\n",
    "    original_df=corpus_sample_encoder.tokenized_df,\n",
    "    id_column=\"text_id\",\n",
    "    label_column=\"language\",\n",
    "    embeddings=token_embeddings,\n",
    "    embeddings_reduced=embeddings_reduced,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "acebfe66",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922e53e9e0994b148b4b70d9830f00ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_word_path = corpus_dataset.pad(**path_specifics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746a5e6f",
   "metadata": {},
   "source": [
    "By inspecting the shape of `corpus_word_path`, we see that we have a path for each word and the dimension of the array is `[batch, length of path, channels]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99428872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 50, 28)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_word_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "523b4cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_dataset.df[\"text_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caac355",
   "metadata": {},
   "source": [
    "To obtain a path as a numpy array, we use the [`nlpsig.TextEncoder.get_path`](https://nlpsig.readthedocs.io/en/latest/data_preparation.html#nlpsig.data_preparation.PrepareData.get_path) method which by default keeps the time features and will remove the id and label columns from the path that is generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be6c23db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 50, 26)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_path = corpus_dataset.get_path(include_features=True)\n",
    "word_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cbab692c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.        ,  -8.13140392,  -6.97939634, ...,  -4.73969507,\n",
       "         -0.09743738,   8.49338818],\n",
       "       [  2.        ,   4.9608326 ,  -1.95551634, ...,  -3.08118773,\n",
       "          1.4404881 ,   2.2313807 ],\n",
       "       [  3.        , -13.2059536 ,   0.93051302, ...,  -6.17853022,\n",
       "          2.53940964,  -2.99868464],\n",
       "       ...,\n",
       "       [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_path[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4936df71-ec25-4fb0-a4c5-139da617fb2e",
   "metadata": {},
   "source": [
    "We obtain the paths for our inliers and outliers by first getting the indices for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df24ce47-638b-45ca-95fa-45d1989c4693",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_word_indices = corpus_sample_df[corpus_sample_df[\"language\"] == \"en\"].index\n",
    "non_english_word_indices = corpus_sample_df[corpus_sample_df[\"language\"] != \"en\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e251c6a-7046-4db2-bef4-7c6341aac635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>abate</td>\n",
       "      <td>en</td>\n",
       "      <td>[a, b, a, t, e]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>abatement</td>\n",
       "      <td>en</td>\n",
       "      <td>[a, b, a, t, e, m, e, n, t]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>abbe</td>\n",
       "      <td>en</td>\n",
       "      <td>[a, b, b, e]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>abbrevs</td>\n",
       "      <td>en</td>\n",
       "      <td>[a, b, b, r, e, v, s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>abdicated</td>\n",
       "      <td>en</td>\n",
       "      <td>[a, b, d, i, c, a, t, e, d]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>zillion</td>\n",
       "      <td>en</td>\n",
       "      <td>[z, i, l, l, i, o, n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>zincked</td>\n",
       "      <td>en</td>\n",
       "      <td>[z, i, n, c, k, e, d]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>zines</td>\n",
       "      <td>en</td>\n",
       "      <td>[z, i, n, e, s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>zingers</td>\n",
       "      <td>en</td>\n",
       "      <td>[z, i, n, g, e, r, s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>zymurgy</td>\n",
       "      <td>en</td>\n",
       "      <td>[z, y, m, u, r, g, y]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word language                       tokens\n",
       "10000      abate       en              [a, b, a, t, e]\n",
       "10001  abatement       en  [a, b, a, t, e, m, e, n, t]\n",
       "10002       abbe       en                 [a, b, b, e]\n",
       "10003    abbrevs       en        [a, b, b, r, e, v, s]\n",
       "10004  abdicated       en  [a, b, d, i, c, a, t, e, d]\n",
       "...          ...      ...                          ...\n",
       "19995    zillion       en        [z, i, l, l, i, o, n]\n",
       "19996    zincked       en        [z, i, n, c, k, e, d]\n",
       "19997      zines       en              [z, i, n, e, s]\n",
       "19998    zingers       en        [z, i, n, g, e, r, s]\n",
       "19999    zymurgy       en        [z, y, m, u, r, g, y]\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sample_df.iloc[english_word_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "343908d6-4627-443c-9086-be5405fdbdab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abblendet</td>\n",
       "      <td>de</td>\n",
       "      <td>[a, b, b, l, e, n, d, e, t]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bestechendes</td>\n",
       "      <td>de</td>\n",
       "      <td>[b, e, s, t, e, c, h, e, n, d, e, s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>narrensicheren</td>\n",
       "      <td>de</td>\n",
       "      <td>[n, a, r, r, e, n, s, i, c, h, e, r, e, n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inakzeptable</td>\n",
       "      <td>de</td>\n",
       "      <td>[i, n, a, k, z, e, p, t, a, b, l, e]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abbestelle</td>\n",
       "      <td>de</td>\n",
       "      <td>[a, b, b, e, s, t, e, l, l, e]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>stakens</td>\n",
       "      <td>sv</td>\n",
       "      <td>[s, t, a, k, e, n, s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>uppbryts</td>\n",
       "      <td>sv</td>\n",
       "      <td>[u, p, p, b, r, y, t, s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>uppeldade</td>\n",
       "      <td>sv</td>\n",
       "      <td>[u, p, p, e, l, d, a, d, e]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>mexikaner</td>\n",
       "      <td>sv</td>\n",
       "      <td>[m, e, x, i, k, a, n, e, r]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>kulsprutan</td>\n",
       "      <td>sv</td>\n",
       "      <td>[k, u, l, s, p, r, u, t, a, n]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                word language                                      tokens\n",
       "0          abblendet       de                 [a, b, b, l, e, n, d, e, t]\n",
       "1       bestechendes       de        [b, e, s, t, e, c, h, e, n, d, e, s]\n",
       "2     narrensicheren       de  [n, a, r, r, e, n, s, i, c, h, e, r, e, n]\n",
       "3       inakzeptable       de        [i, n, a, k, z, e, p, t, a, b, l, e]\n",
       "4         abbestelle       de              [a, b, b, e, s, t, e, l, l, e]\n",
       "...              ...      ...                                         ...\n",
       "9995         stakens       sv                       [s, t, a, k, e, n, s]\n",
       "9996        uppbryts       sv                    [u, p, p, b, r, y, t, s]\n",
       "9997       uppeldade       sv                 [u, p, p, e, l, d, a, d, e]\n",
       "9998       mexikaner       sv                 [m, e, x, i, k, a, n, e, r]\n",
       "9999      kulsprutan       sv              [k, u, l, s, p, r, u, t, a, n]\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sample_df.iloc[non_english_word_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a35225a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain paths for english words and non-english words in corpus_sample_df\n",
    "inlier_paths = word_path[english_word_indices]\n",
    "outlier_paths = word_path[non_english_word_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6632eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 50, 26)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inlier_paths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a3dcab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 50, 26)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlier_paths.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ff2c98-fbfc-442a-93d0-f2017e02ee02",
   "metadata": {},
   "source": [
    "## Obtaining path signatures for the corpus, inliers and outliers\n",
    "\n",
    "Now that we have obtained our paths for our corpus, the inliers and the outliers, we can obtain the path signatures for each of these. In this notebook, we use [RoughPy](https://roughpy.org/) developed by Sam Morley, Terry Lyons et al. to compute path signatures. Before we compute our path signatures, it is often useful to normalise the channels of the paths. For this example, we will use min-max scaling to normalise the channels of the paths. We compute the minimum and maximum using the corpus paths (our training data) and then use these to normalise the paths for the inliers and outliers (our test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a6cdc5d9-b5b5-4053-b14e-8faba8ebe0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(\n",
    "    paths: np.array, minimum: float | None = None, maximum: float | None = None\n",
    ") -> tuple[np.array, float, float]:\n",
    "    \"\"\"\n",
    "    Apply min-max scaling to the features of the paths.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    paths : np.array\n",
    "        Array containing the paths of shape [batch, length, features]\n",
    "    minimum : float | None, optional\n",
    "        Optional float to use as the minimum, by default None\n",
    "    maximum : float | None, optional\n",
    "        Optional float to use as the maximum, by default None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[np.array, float, float]\n",
    "        Normalised paths, minimum, and maximum\n",
    "    \"\"\"\n",
    "    # apply min-max scaling to the features of the paths\n",
    "    # paths is a three dimensional tensor (batch, length, features)\n",
    "    if minimum is None and maximum is None:\n",
    "        # reshape to (batch * length, features)\n",
    "        paths_stacked = paths.reshape(-1, paths.shape[-1])\n",
    "        minimum = np.min(paths_stacked, axis=0)\n",
    "        maximum = np.max(paths_stacked, axis=0)\n",
    "\n",
    "    # compute min-max scaling\n",
    "    paths = (paths - minimum) / (maximum - minimum)\n",
    "\n",
    "    return paths, minimum, maximum\n",
    "\n",
    "\n",
    "def normalise_data(\n",
    "    corpus: np.array, inliers: np.array, outliers: np.array\n",
    ") -> tuple[np.array, np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Apply min-max scaling to the features of the paths for\n",
    "    the corpus, inliers, and outliers.\n",
    "\n",
    "    The minimum and maximum values are computed from the corpus.\n",
    "    This means that there is a possibility that the inliers and\n",
    "    outliers may have values that are outside the range of the\n",
    "    corpus.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : np.array\n",
    "        Array containing the training corpus paths of shape\n",
    "        [batch, length, features]\n",
    "    inliers : np.array\n",
    "        Array containing the test inlier paths of shape\n",
    "        [batch, length, features]\n",
    "    outliers : np.array\n",
    "        Array containing the test outlier paths of shape\n",
    "        [batch, length, features]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[np.array, np.array, np.array]\n",
    "        Normalised corpus, inliers, and outliers\n",
    "    \"\"\"\n",
    "    corpus, minimum, maximum = normalise(corpus)\n",
    "    inliers, *_ = normalise(inliers, minimum, maximum)\n",
    "    outliers, *_ = normalise(outliers, minimum, maximum)\n",
    "\n",
    "    return corpus, inliers, outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "66a90f69-4800-4040-bcd0-49cbd65a7884",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, inliers, outliers = normalise_data(\n",
    "    english_word_path, inlier_paths, outlier_paths\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6661dd6a-0354-4877-8b81-87234238cae5",
   "metadata": {},
   "source": [
    "## Obtaining path signatures for the english words\n",
    "\n",
    "We use [RoughPy](https://roughpy.org/) to compute path signatures, which we compute up to depth $2$ here on paths with $26$ channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36f163f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_signature(stream: np.array, depth: int) -> np.array:\n",
    "    \"\"\"\n",
    "    Given a stream of data, compute the signature of the stream truncated\n",
    "    to the given depth using RoughPy.\n",
    "\n",
    "    RoughPy works with increments of the stream, and we assume here that\n",
    "    stream is the raw stream so incremements are computed within this function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    stream : np.array\n",
    "        A two-dimensional numpy array of shape [length x dimension]\n",
    "    depth : int\n",
    "        The depth to which the signature should be truncated\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        A one-dimensional numpy array of length 1+dim+dim^2+...+dim^depth\n",
    "        containing the signature of the stream truncated to the given depth\n",
    "    \"\"\"\n",
    "    # define context for RoughPy increment stream\n",
    "    context = rp.get_context(width=stream.shape[1], depth=depth, coeffs=rp.DPReal)\n",
    "    # compute increments of the stream\n",
    "    increments = np.diff(stream, axis=0)\n",
    "    # define RoughPy increment stream object\n",
    "    lie_incremement_stream = rp.LieIncrementStream.from_increments(\n",
    "        increments, ctx=context\n",
    "    )\n",
    "    # compute signature and convert to numpy array\n",
    "    return np.array(lie_incremement_stream.signature())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6981567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_signatures_parallel(streams: np.array, depth: int, n_jobs: int) -> np.array:\n",
    "    \"\"\"\n",
    "    Given a batch of streams of data, compute the signature of each stream\n",
    "    truncated to the given depth using RoughPy in parallel.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    streams : np.array\n",
    "        A three-dimensional numpy array of shape [batch x length x dimension]\n",
    "    depth : int\n",
    "        The depth to which the signature should be truncated\n",
    "    n_jobs : int\n",
    "        The number of jobs to run in parallel\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        A three-dimensional numpy array of shape [batch x signature_length]\n",
    "        containing the signatures of the streams truncated to the given depth\n",
    "    \"\"\"\n",
    "    signatures = Parallel(n_jobs=n_jobs)(\n",
    "        [\n",
    "            delayed(compute_signature)(stream=streams[k], depth=depth)\n",
    "            for k in range(streams.shape[0])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return np.stack(signatures, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "16e89214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "n_jobs = cpu_count()\n",
    "sig_depth = 2\n",
    "\n",
    "corpus_sig = compute_signatures_parallel(corpus, sig_depth, n_jobs)\n",
    "inlier_sig = compute_signatures_parallel(inliers, sig_depth, n_jobs)\n",
    "outlier_sig = compute_signatures_parallel(outliers, sig_depth, n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2e9a5242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70615, 703)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c6a09d3c-65dd-487f-a290-5976127edeef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 703)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inlier_sig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "49ad0524-6613-4fb4-919a-6c476fe93fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 703)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlier_sig.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428e5c23",
   "metadata": {},
   "source": [
    "## Anomaly detection task\n",
    "\n",
    "To recap the task at hand:\n",
    "- We trained a language model using a corpus of english words stored in the `english_train` dataframe.\n",
    "- We have another set of english words (inliers) and some non-english words (outliers) which are stored in the `corpus_sample_df` dataframe.\n",
    "- We now want to see how we could detect the non-english words efficiently, in particular, we use the following method:\n",
    "    - For each word in `english_train` and `corpus_sample_df`, we have a vector representation for them (e.g. we've computed the path signatures for each of them and they are stored in `english_word_sig`).\n",
    "    - For each word in `corpus_sample_df`, we compute the minimum Mahalanobis distance between its path signature to path signatures for our corpus of known English words (i.e. each row in `english_word_sig`).\n",
    "    - We then look the [ROC curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) to see how well separated are the english words to the non-english words. For a good performance, we hope that there is good separation, and so we measure the success of this method using the [ROCAUC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html).\n",
    " \n",
    "Full details of the anomaly detection methodology can be found in [[2]](https://arxiv.org/abs/2006.03487) _Dimensionless Anomaly Detection on Multivariate Streams with Variance Norm and Path Signature_ by Zhen Shao, Ryan Sze-Yin Chan, Thomas Cochrane, Peter Foster, Terry Lyons. Implementation of this approach can be found in the [`signature_mahalanobis_knn`](https://github.com/datasig-ac-uk/signature_mahalanobis_knn) library.\n",
    "\n",
    "Note that in Section 5.2.4 of [[2]](https://arxiv.org/abs/2006.03487), we consider the same language dataset example as we do here, but use one-hot encoding vector representations for the characters. We will see our dimension reduced transformer embeddings give us a boost in performance as seen in the paper.\n",
    "\n",
    "**Warning**: in this notebook, we will use all CPU cores available to fit the model, so you may want to change that in the next cell. Fiting may take a while to run - when developing this notebook, we used a Apple M1 Pro chip with 10 CPU cores and 32GB of RAM and took around 4 hours to fit. Inference afterwards is much quicker. If you have a less powerful machine, or just want to run this quicker, you can reduce the corpus size by taking a sample of the corpus signatures to fit the model on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "173c6c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbours = 5\n",
    "n_jobs = -1\n",
    "knn_library = \"sklearn\"\n",
    "bootstrap_samples = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5481f3cd-f82e-4b1d-88bf-148b3d601ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_maha_knn = SignatureMahalanobisKNN(n_jobs=n_jobs, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e8fd1d-c4aa-4e36-8fd1-ef245dec74bc",
   "metadata": {},
   "source": [
    "We use the `SignatureMahalanobisKNN.fit` method to fit the model and pass in our path signature representations for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f95fdbd6-c116-48f1-bd0f-64f72f31bd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time: 15011.952597856522\n"
     ]
    }
   ],
   "source": [
    "# measure the time spent on fit\n",
    "start_time = time.time()\n",
    "signature_maha_knn.fit(\n",
    "    knn_library=knn_library,\n",
    "    signatures_train=corpus_sig,\n",
    ")\n",
    "fit_time = time.time() - start_time\n",
    "print(f\"fit_time: {fit_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef8dd2e-7bd1-4439-95a4-b9d8fa02d89c",
   "metadata": {},
   "source": [
    "We can then simply use the `SignatureMahalanobisKNN.conformance` method to compute the anomaly scores for both the inlier signatures and outlier signatures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "336bb8f4-27d8-46ea-bcaa-438f138caca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_score_time: 18.070377826690674\n"
     ]
    }
   ],
   "source": [
    "# measure the time spent on conformance\n",
    "start_time = time.time()\n",
    "scores_inliers = signature_maha_knn.conformance(\n",
    "    signatures_test=inlier_sig, n_neighbors=n_neighbours\n",
    ")\n",
    "scores_outliers = signature_maha_knn.conformance(\n",
    "    signatures_test=outlier_sig, n_neighbors=n_neighbours\n",
    ")\n",
    "compute_score_time = time.time() - start_time\n",
    "print(f\"compute_score_time: {compute_score_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbac1993-e8bf-4a7c-931e-7f5ec30ceff6",
   "metadata": {},
   "source": [
    "Given the anomaly scores for the inliers and outliers, we can compute the AUC and bootstrap this score to get a standard error based on 10000 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "30c710ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.953 +/- 0.001\n"
     ]
    }
   ],
   "source": [
    "auc, se = compute_auc_given_dists(\n",
    "    scores_inliers,\n",
    "    scores_outliers,\n",
    "    bootstrap=True,\n",
    "    bootstrap_samples=bootstrap_samples,\n",
    "    random_state=seed,\n",
    "    plot=True,\n",
    "    title=(\n",
    "        \"English vs Non-English\\n\"\n",
    "        f\"(using depth={sig_depth} signatures of dim-reduced transformer embeddings)\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"AUC: {auc:.3f} +/- {se:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc4f521-4a24-4dd2-aa4e-257ab4b80b83",
   "metadata": {},
   "source": [
    "As noted above, in Section 5.2.4 of [[2]](https://arxiv.org/abs/2006.03487), we consider the same language dataset example as we do here and achieved an AUC score of $0.878$ +/- $0.002$, so we have achieved a boost in performance using our dimension-reduced transformer embeddings.\n",
    "\n",
    "Looking at the CDF we can see there is good separation between the two classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9cc33f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot CDF\n",
    "plot_cdf_given_dists(\n",
    "    scores_inliers,\n",
    "    scores_outliers,\n",
    "    bins=10000000,\n",
    "    title=f\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253964b4-8ccf-4e9e-ad8c-6ae86bc375ce",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "The computations described in this notebook were performed using the Baskerville Tier 2 HPC service (https://www.baskerville.ac.uk/). Baskerville was funded by the EPSRC and UKRI through the World Class Labs scheme (EP/T022221/1) and the Digital Research Infrastructure programme (EP/W032244/1) and is operated by Advanced Research Computing at the University of Birmingham.\n",
    "\n",
    "## References\n",
    "\n",
    "[1] Devlin, J., Chang, M.W., Lee, K. and Toutanova, K., 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. _arXiv preprint arXiv:1810.04805_.\n",
    "\n",
    "[2] Shao, Z., Chan, R.S.Y., Cochrane, T., Foster, P., Lyons, T. 2023. Dimensionless Anomaly Detection on Multivariate Streams with Variance Norm and Path Signature. _arXiv preprint arXiv:2006.03487_.\n",
    "\n",
    "[3] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L. and Stoyanov, V., 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. _arXiv preprint arXiv:1907.11692_.\n",
    "\n",
    "[4] McInnes, L., and Healy, J. 2018. UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction, _arXiv preprint arXiv:1802.03426_.\n",
    "\n",
    "[5] Tipping, M. E., and Bishop, C. M., 1999. Probabilistic Principal Component Analysis. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 61(3), 611-622.\n",
    "\n",
    "[6] van der Maaten, L.J.P., and Hinton, G.E., 2008. Visualizing High-Dimensional Data using t-SNE. _Journal of Machine Learning Research_, 9:2579-2605.\n",
    "\n",
    "[7] Mu, J., Bhat, S., and Viswanath, P. (2017). All-but-the-top: Simple and effective postprocessing for word representations. _arXiv preprint arXiv:1702.01417_.\n",
    "\n",
    "[8] Raunak, V., Gupta, V., and Metze, F. (2019). Effective dimensionality reduction for word embeddings. In _Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP- 2019)_, 235â€“243."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang-analysis-nlpsig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
