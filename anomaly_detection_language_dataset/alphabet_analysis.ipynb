{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "from transformers import (\n",
    "    RobertaConfig,\n",
    "    RobertaForMaskedLM,\n",
    "    RobertaTokenizerFast,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding,\n",
    "    pipeline,\n",
    ")\n",
    "import transformers\n",
    "import tokenizers\n",
    "import torch\n",
    "\n",
    "import nlpsig\n",
    "from nlpsig import set_seed\n",
    "\n",
    "from signax import signature\n",
    "\n",
    "seed = 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e481f7d-7228-49a5-81a4-c9ccd5b2795a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Anomaly Detection task\n",
    "\n",
    "In natural language processing (NLP) tasks, we often are dealing with high dimensional streams of data. Neural network architectures known as Transformers have been shown to be very effective in NLP tasks, and we can use these to obtain high dimensional streams of embeddings for words/tokenised text. In this notebook, we will look at how we can use path signature techniques to analyse these high dimensional streams of embeddings. In particular, we will look at how we can perform outlier detection on the path signatures of the embeddings obtained from a pre-trained Transformer. \n",
    "\n",
    "In particular, in this notebook, we consider the task of determining whether a word is an english word or not by using the path signature of the stream of _character_ embeddings of the word. That is, each word is represented as a stream of character embeddings. We do this by training a Transformer model (from scratch) on a _masked language modelling_ task (or _Cloze_ task) as described in [[1]](https://arxiv.org/abs/1810.04805) using a corpus of english words, and then use this model to obtain a stream of (character) embeddings for a sample of (english and non-english) words. Path signature techniques are applied to analyse the streams of embeddings and finally we attempt to detect the non-english words as outliers in the space of path signatures.\n",
    "\n",
    "Since we are dealing with high dimensional streams of data, we will use a dimension reduction technique to reduce the dimension of the embeddings before computing the path signature. We will look at how we can perform outlier detection on the path signatures of the dimension-reduced embeddings.\n",
    "\n",
    "The pipeline for this task is as follows:\n",
    "1. Train a Transformer model on a masked language modelling task using a corpus of english words.\n",
    "2. Obtain a stream of character embeddings for a sample of english and non-english words using the trained model (note that we ensure that the english words in this sample are not in the training corpus to pre-train the Transforemr).\n",
    "    - The english words in this sample are our _inlier_ class while the non-english words are our _outlier_ class in this example.\n",
    "3. Perform dimension reduction on the streams of embeddings.\n",
    "4. Compute the path signature of the dimension-reduced embeddings.\n",
    "5. Perform outlier detection on the path signatures to detect the non-english words.\n",
    "\n",
    "## `nlpsig` library\n",
    "\n",
    "In this notebook, we illustrate how we can use the [`nlpsig`](https://github.com/datasig-ac-uk/nlpsig) package to utilise transformers in order to obtain streams of high dimensional embeddings, which can then be analysed using path signature techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d3859",
   "metadata": {},
   "source": [
    "## Language dataset\n",
    "\n",
    "In the `data/` folder, we have several text folders of words from different languages:\n",
    "- `wordlist_de.txt`: German words\n",
    "- `wordlist_en.txt`: English words\n",
    "- `wordlist_fr.txt`: French words\n",
    "- `wordlist_it.txt`: Italian words\n",
    "- `wordlist_pl.txt`: Polish words\n",
    "- `wordlist_sv.txt`: Swedish words\n",
    "\n",
    "We additionally have a `alphabet.txt` file which just stores the alphabet characters ('a', 'b', 'c', ...).\n",
    "\n",
    "The task is to split the words into its individual characters and to obtain an embedding for each of them. We can represent a word by a path of its character embeddings and compute its path signature to use as features in predicting the language for which the word belongs.\n",
    "\n",
    "Here we look at obtaining embeddings using a Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992ad8c2-3747-41c3-bbc8-fdb7ad29690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a6aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET_FILE = f\"{data_folder}/alphabet.txt\"\n",
    "with open(ALPHABET_FILE) as f:\n",
    "    alphabet = f.read().splitlines()\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b2270d",
   "metadata": {},
   "source": [
    "## Set up Tokenizer for word corpora\n",
    "\n",
    "If we were to fine-tune an existing pretrained transformer, we could use the same tokenizer that the model was pretrained with. However, in this notebook, we will train a Transformer from stratch, and so using a tokenizer that was pretrained on a corpus that looks quite different to ours is suboptimal. In this example, we want to tokenize our words into characters, and so we need to train a _character-based_ tokenizer that is able to do this.\n",
    "\n",
    "Here, we need to use the [`tokenizers`](https://huggingface.co/docs/tokenizers/index) library to set up and train a new tokenizer for our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c39ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise character based tokenizer\n",
    "tokenizer = tokenizers.CharBPETokenizer()\n",
    "tokenizer.train(files=[ALPHABET_FILE],\n",
    "                show_progress=False,\n",
    "                special_tokens=['<s>', '</s>', '<unk>', '<pad>', '<mask>'])\n",
    "\n",
    "# save the tokenizer to \"CHAR_BERT/\" folder\n",
    "if not os.path.exists(\"CHAR_BERT\"):\n",
    "    os.makedirs(\"CHAR_BERT\")\n",
    "\n",
    "tokenizer.save_model(\"CHAR_BERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aed597b",
   "metadata": {},
   "source": [
    "## Prepare training data and test data\n",
    "\n",
    "Our test data will consist a sample of 10000 english words and 10000 non-english words (2000 from each of the remaining languages). We will use the remaining english words as our training data to train the Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941eb204",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist_files = [\n",
    "    f\"{data_folder}/wordlist_de.txt\",\n",
    "    f\"{data_folder}/wordlist_en.txt\",\n",
    "    f\"{data_folder}/wordlist_fr.txt\",\n",
    "    f\"{data_folder}/wordlist_it.txt\",\n",
    "    f\"{data_folder}/wordlist_pl.txt\",\n",
    "    f\"{data_folder}/wordlist_sv.txt\",\n",
    "]\n",
    "\n",
    "wordlist_dfs = []\n",
    "for filename in wordlist_files:\n",
    "    with open(filename, \"r\") as f:\n",
    "        words = f.read().splitlines()\n",
    "        words_df = pd.DataFrame({\"word\": words})\n",
    "        words_df[\"language\"] = filename.split(\"_\")[1][0:2]\n",
    "        wordlist_dfs.append(words_df)\n",
    "\n",
    "corpus_df = pd.concat(wordlist_dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a7a38f",
   "metadata": {},
   "source": [
    "We can see that there are relatively fewer English words than the other languages..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3c6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df[\"language\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f4c498",
   "metadata": {},
   "source": [
    "We are going to train our language model on the English words, so taking out a sample of English words from the corpus..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066d9241",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train_pickle_file = f\"{data_folder}/english_train.pkl\"\n",
    "if os.path.isfile(english_train_pickle_file):\n",
    "    english_train = pd.read_pickle(english_train_pickle_file)\n",
    "else:\n",
    "    # set seed for sampling\n",
    "    set_seed(seed)\n",
    "    n_words = len(corpus_df[corpus_df[\"language\"]==\"en\"])-10000\n",
    "    \n",
    "    # sample english words from the corpus\n",
    "    english_train = corpus_df[corpus_df[\"language\"]==\"en\"].sample(n_words)\n",
    "    english_train = english_train.reset_index(drop=True)\n",
    "    \n",
    "    # save data for later\n",
    "    english_train.to_pickle(english_train_pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739f58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b0681e",
   "metadata": {},
   "source": [
    "To make the dataset bit more manageable, I'll just take a sample of each of the languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccac4f00-e15d-4e1f-9a84-bc899ef6cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the words that we use to train language model from the corpus\n",
    "cond = corpus_df[\"word\"].isin(english_train[\"word\"])\n",
    "corpus_df = corpus_df.drop(corpus_df[cond].index)\n",
    "corpus_df = corpus_df.reset_index(drop=True)\n",
    "corpus_df[\"language\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e60fabc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus_sample_pickle_file = f\"{data_folder}/corpus_sample.pkl\"\n",
    "if os.path.isfile(corpus_sample_pickle_file):\n",
    "    corpus_sample_df = pd.read_pickle(corpus_sample_pickle_file)\n",
    "else:\n",
    "    # set seed for sampling\n",
    "    set_seed(seed)\n",
    "    n_english = 10000\n",
    "    n_remaining = 10000\n",
    "    # sampling non-english words\n",
    "    languages = corpus_df[\"language\"].unique()\n",
    "    words_per_language = math.floor(n_remaining / (len(languages)-1))\n",
    "    non_english_df = pd.concat(\n",
    "        [corpus_df[corpus_df[\"language\"]==lang].sample(words_per_language, random_state=seed)\n",
    "         for lang in languages if lang != \"en\"]\n",
    "    )\n",
    "    # take the remaining english words\n",
    "    english_df = corpus_df[corpus_df[\"language\"]==\"en\"]\n",
    "    corpus_sample_df = pd.concat([non_english_df, english_df]).reset_index(drop=True)\n",
    "\n",
    "    # save data for later\n",
    "    corpus_sample_df.to_pickle(corpus_sample_pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6663d2e-d35f-425c-a8ff-397d73d9db7f",
   "metadata": {},
   "source": [
    "In our resulting corpus, we have an equal amount of english (inliers) and non-english words (outliers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e53354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_sample_df[\"language\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80c6a0f-cdee-41dc-9c25-467a1cda7344",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa40272c",
   "metadata": {},
   "source": [
    "## Training a language model\n",
    "\n",
    "We want to train a masked language model for our corpus of English words. In particular, we mask out particular letters and ask our model to try predict the masked letter.\n",
    "\n",
    "Here, we initialise our tokenizer (here we tokenize by character), data collator (with padding) and set up our transformer model by specifying the config (we use the [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta) here) described in [[2]](https://arxiv.org/abs/1907.11692)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548ff04a-5e7d-461a-8378-96ce88c0f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df[\"word\"].apply(len).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2914af75-87c5-483d-8ce0-e8a7f03ad56d",
   "metadata": {},
   "source": [
    "As the longest word in our corpus is 39, we will set the maximum sequence length in the transformer as 50 for a bit of headroom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5008adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the maximum length as the longest word in our dataset\n",
    "max_length = 50\n",
    "\n",
    "# set dimension of hidden states for Transformer\n",
    "hidden_size = 768\n",
    "\n",
    "# load in tokenizer for architecture\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('CHAR_BERT/', max_len=max_length)\n",
    "\n",
    "# set up data_collator to use (intially just one that adds padding)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# initialise transformer architecture (random weights)\n",
    "config_args = {\n",
    "    \"vocab_size\": tokenizer.backend_tokenizer.get_vocab_size(),\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"max_length\": max_length,\n",
    "    \"max_position_embeddings\": max_length + 2,\n",
    "    \"intermediate_size\": 4*hidden_size,\n",
    "    \"hidden_dropout_prob\": 0.1,\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"num_hidden_layers\": 6,\n",
    "    \"type_vocab_size\": 1\n",
    "}\n",
    "\n",
    "config = RobertaConfig(**config_args)\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "model_name = \"CHAR_BERT_trained\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487e8e70-3cf0-4c69-8939-7b6c180f0ccf",
   "metadata": {},
   "source": [
    "If you have already ran this notebook before and have trained the transformer previously, you can just load in the pretrained transformer using the line below - just uncomment in order to load in the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540b33c9-5e99-4ded-96f9-0fdc9680522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RobertaForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312e8ba",
   "metadata": {},
   "source": [
    "## Using the `TextEncoder` class\n",
    "\n",
    "The `TextEncoder` class in the `nlpsig` package is able to take a dataframe with a column of text. We can use this class to obtain embeddings for the input text, or to train the model with the input text.\n",
    "\n",
    "In this example, we will first use the class to train our transformer model with the corpus of English words, which we have stored in the `english_train` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79eb814",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3040a0ca-e5c0-489b-a6e0-142f7ce08c32",
   "metadata": {},
   "source": [
    "To initialise the object, we pass in the dataframe, `english_train`, and the column name that stores our text, `\"word\"` in this case. We pass in our model, config, tokenizer and data collator which are necessary to train our model.\n",
    "\n",
    "We note that in the case where we are not training a model, we could optionally just pass in a string to the `model_name` argument either specifying a model in the [Huggingface model hub](https://huggingface.co/models), e.g. [`\"bert-base-uncased\"`](https://huggingface.co/bert-base-uncased), or specifying a path to which a model is stored in, e.g. `\"CHAR_BERT_trained\"`. We can then load in our pretrained model using the `.load_pretrained_model()` method - we will see this later on when we will use this class again in order to obtain embeddings for the words in `corpus_sample_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059975c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = nlpsig.TextEncoder(\n",
    "    df=english_train,\n",
    "    feature_name=\"word\",\n",
    "    model=model,\n",
    "    config=config,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9201959b",
   "metadata": {},
   "source": [
    "We can tokenize the text with the `.tokenize_text()` method, which tokenizes each of the sentences in the column of the dataframe that we passed in (note here that we just have words and we are tokenizing on the characters). So in the above, we tokenize each string in the `word` column of the `english_train` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7c5e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder.tokenize_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3396bd0d",
   "metadata": {},
   "source": [
    "Note that the `text_encoder` object (instance of `TextEncoder`) also keeps the data as a [Huggingface `Dataset`](https://huggingface.co/docs/datasets/index) object too which is stored in the `.dataset` attribute of the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec831c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f19f545",
   "metadata": {},
   "source": [
    "Note that when initialising the `Text_Encoder` object, we could've optionally passed in the data as a `Dataset` object using the `dataset` argument. So if the dataset that you want to use is already in that form, there is no need to first convert that to a dataframe before using the class.\n",
    "\n",
    "We can see that we have tokenized this as there are `input_ids`, `attention_mask`, `special_tokens_mask`, and `tokens` features in the `Dataset` object.\n",
    "\n",
    "Lets have a look at the first word in this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57359738",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder.dataset[\"word\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376415cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder.dataset[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ea34f6",
   "metadata": {},
   "source": [
    "We can see that this word has been tokenized by character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d717a3d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_encoder.dataset[\"tokens\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d11263",
   "metadata": {},
   "source": [
    "We can also see that we have saved the tokenized text in the `'token'` column of the dataframe stored in `.df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be65324",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3077c5",
   "metadata": {},
   "source": [
    "We also store the tokens in `.tokens` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e935d45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6048cae",
   "metadata": {},
   "source": [
    "After applying the `.tokenize_text()` method, we store a tokenized dataframe in the `.tokenized_df` attribue. Here, we have each token in our corpus and their associated `'text_id'` (which is just the index they were given in the original dataframe that we pass):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d527e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder.tokenized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42df9b7e",
   "metadata": {},
   "source": [
    "So if we looked at `text_id==0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23c2895",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder.tokenized_df[text_encoder.tokenized_df[\"text_id\"]==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927da67c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training the model\n",
    "\n",
    "The above embeddings will not be good for any downstream task as the model itself has not been trained to the text. For this we will use other methods in the `TextEncoder` class which allows us to do this by using the [Huggingface trainer API](https://huggingface.co/docs/transformers/main_classes/trainer).\n",
    "\n",
    "Note that if you're re-running this notebook after pre-training the model previously, you can skip this section.\n",
    "\n",
    "Otherwise, to train the model, we need to set up a data collator for training our model. We train the model on the masked language modelling task and so use the `DataCollatorForLanguageModeling` class which masks tokens with a certain probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5677cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data_collator for language modelling (has dynamic padding)\n",
    "data_collator_for_LM = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf38d4-772b-4037-8227-11e862677b3b",
   "metadata": {},
   "source": [
    "To train our dataset, we will split it into a train, validation and test sets with the `.split_dataset()` method. This stores the split Dataset objects in `.dataset_split` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526c6af6-bef0-4407-b0c9-a6e254298195",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder.split_dataset(random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b377e79-4a06-4eda-a581-ff16e966ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text_encoder.dataset_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96e897f",
   "metadata": {},
   "source": [
    "We can set up the trainer's arguments with `.set_up_training_args()` which sets up a `TrainingArguments` object (from the `transformers` package) and stores it in the `.training_args` attribute of the `text_encoder` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996f53c6-2859-4df7-a890-a41cbf21bc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder.set_up_training_args(\n",
    "    output_dir=model_name,\n",
    "    num_train_epochs=600,\n",
    "    per_device_train_batch_size=128,\n",
    "    disable_tqdm=False,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10000,\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd73ffec-b0f9-4164-9dc8-a958d58a4ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text_encoder.training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6996b0-8892-46e0-a7f7-59ba0925a2ec",
   "metadata": {},
   "source": [
    "And lastly, we set up a `Trainer` object (from the `transformers` package) and store it in the `.trainer` attribute in the `text_encoder` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490c2393",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder.set_up_trainer(data_collator=data_collator_for_LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bde5d6f-5a96-4bfc-a693-d78eff1382e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text_encoder.trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cee6a3",
   "metadata": {},
   "source": [
    "Once everything is set up, we just train our model by calling `.fit_transformer_with_trainer_api()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a34fa1-cd9c-4f62-92f3-607b5d117c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eb7b16-2b48-4f09-b13f-6850a19b6b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01101469",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cuda')\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05dd2bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set to only report errors to avoid excessing logging\n",
    "transformers.utils.logging.set_verbosity(40)\n",
    "\n",
    "# fit the model\n",
    "text_encoder.fit_transformer_with_trainer_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8497e3",
   "metadata": {},
   "source": [
    "Saving our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2330b558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_encoder.trainer.save_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6978236",
   "metadata": {},
   "source": [
    "## Evaluating trained model\n",
    "\n",
    "Evaluating the performance on predicting the masked letter for the test dataset. To do this, for each word in our test dataset, we will mask each letter on its own and ask the model to predict the masked letter. So for a 5 letter word, we have 5 predictions to make - one for each letter given the other letters.\n",
    "\n",
    "For our tokenizer, we see that `<mask>` is used as the mask token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccfacdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder.tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb04bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_masked_character_accuracy(fill_mask, words):\n",
    "    was_correct = []\n",
    "    print(f\"Evaluating with {len(words)} words\")\n",
    "    for word in tqdm(words):\n",
    "        masked_strings = [word[:i] + '<mask>' + word[i+1:] for i in range(len(word))]\n",
    "        predictions = [fill_mask(word)[0]['sequence'] for word in masked_strings]\n",
    "        was_correct += [pred == word for pred in predictions]\n",
    "    \n",
    "    acc = np.sum(was_correct) / len(was_correct)\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba734ae4-fa4e-4eb6-824f-ba732139eb8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\"fill-mask\",\n",
    "                     model=model_name,\n",
    "                     tokenizer=model_name)\n",
    "\n",
    "compute_masked_character_accuracy(fill_mask, text_encoder.dataset_split[\"test\"][\"word\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b279fd46-e3ff-4add-a23b-e082aa9ba6d4",
   "metadata": {},
   "source": [
    "We can see that we have a 75% accuracy on this masked language modelling task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5646433f",
   "metadata": {},
   "source": [
    "## Obtaining token and word embeddings\n",
    "\n",
    "There are many ways in which one can get embeddings from the transformer network, as the output is the layers for the full network. A few ways are:\n",
    "\n",
    "- Returning the output of a particular hidden layer\n",
    "    - use `.obtain_embeddings(method = \"hidden_layer\", layers = l)` where `l` is the layer you want\n",
    "    - If no layer is requested, it will just give you the second-to-last hidden layer of the transformer network.\n",
    "- Concatenate the output of several hidden layers\n",
    "    - use `.obtain_embeddings(method = \"concatenate\", layers = [l_1, l_2, ...])` where `[l_1, l_2, ...]` is a list of layers you want to concatenate\n",
    "- Element-wise sum the output of several hidden layers\n",
    "    - use `.obtain_embeddings(method = \"sum\" , layers = [l_1, l_2, ...])` where `[l_1, l_2, ...]` is a list of layers you want to sum\n",
    "- Mean the output of several hidden layers\n",
    "    - use `.obtain_embeddings(method = \"mean\" , layers = [l_1, l_2, ...])` where `[l_1, l_2, ...]` is a list of layers you want to mean\n",
    "\n",
    "Each of these methods will return a 2-dimensional array with dimensions `[token, embedding]`.\n",
    "\n",
    "If a more custom way to obtain embeddings from the hidden layers, you can specify what layers you want, and it will return them (i.e. using `.obtain_embeddings(method = \"hidden_layer\", layers = [l_1, l_2, ...])` where `[l_1, l_2, ...]` is a list of hidden layers you want) and so the output will be a 3-dimensional array with dimensions `[layer, token, embedding]` for which you would need to combine in such a way that you would have an embedding for each token. The above methods would return a 2-dimensional array with dimensions `[token, embedding]`.\n",
    "\n",
    "Note that if we had passed in a pre-trained model (remember above, we just initialised one with a config and so have random weight), we could've directly obtain token embeddings by the `.obtain_embeddings()` method without the need to train our model first. We will do this later when obtaining embeddings for the words in `corpus_sample_df`.\n",
    "\n",
    "In the below, we just obtain the last hidden layer of the network (the 6th one in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e80725-925f-4ada-a58a-02e0b71ff023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the model to CPU (might not be always necessary to run this)\n",
    "text_encoder.model.to('cpu')\n",
    "english_token_embeddings = text_encoder.obtain_embeddings(method=\"hidden_layer\", layers=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73afe43d-26f8-437b-9a32-ceee73fb88d1",
   "metadata": {},
   "source": [
    "By inspecting the shape of this, we can see that we have a 2-dimensional array with dimensions `[token, embedding]` where the embeddings are 768 dimensional in this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d5b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_token_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f09e23",
   "metadata": {},
   "source": [
    "Now that we have token embeddings for each text, it is possible to pool these embeddings to obtain an embedding for the full text (for this case, this embedding would represent the word itself. We can use the `.pool_token_embeddings()` method for doing this.\n",
    "\n",
    "Again, there are several methods and full details can be found in the documentation, but a few are:\n",
    "\n",
    "- Taking the mean of the token embeddings\n",
    "    - use `.pool_token_embeddings(method = \"mean\")`\n",
    "- Taking the element-wise max of the token embeddings\n",
    "    - use `.pool_token_embeddings(method = \"max\")`\n",
    "- Taking the element-wise sum of the token embeddings\n",
    "    - use `.pool_token_embeddings(method = \"sum\")`\n",
    "- Taking the token-embedding for the CLS token (a special token that is used in some transformers like BERT and RoBERTa)\n",
    "    - but this is only available to us if we set `skip_special_tokens=False` when tokenizing the text with `.tokenize_text()` method (note by default, this is set to `True` and so we don't have access to this method here)\n",
    "    - use `.pool_token_embeddings(method = \"cls\")`\n",
    "        - note this will produce an error if the CLS token is not available...\n",
    "\n",
    "For example, to pool the character embeddings by taking the mean of the token embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfd84d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pooled_english_mean = text_encoder.pool_token_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b491dc",
   "metadata": {},
   "source": [
    "Again, we can inspect the shape and we can see that we have embeddings for each of our words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c876954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_english_mean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c0fffd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dimension reduction\n",
    "\n",
    "We can perform dimension reduction with `nlpsig` using the `DimReduce` class. Here, we will use Gaussian Random Projections (implemented using [`scikit-learn`](https://scikit-learn.org/stable/modules/random_projection.html)) by setting `method=\"gaussian_random_projection\"`, but there are other standard methods available:\n",
    "- UMAP [[3]](https://arxiv.org/abs/1802.03426) (implemented using the [`umap-learn`](https://umap-learn.readthedocs.io/en/latest/api.html))\n",
    "    - `method=\"umap\"`\n",
    "- PCA [[4]](http://www.miketipping.com/papers/met-mppca.pdf) (implemented using [`scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html))\n",
    "    - `method=\"pca\"`\n",
    "- TSNE [[5]](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) (implemented using [`scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html))\n",
    "    - `method=\"tsne\"`\n",
    "- Post Processing Algorithm (PPA) with PCA (PPA-PCA) [[6]](https://arxiv.org/abs/1702.01417)\n",
    "    - `method=\"ppapca\"`\n",
    "- PPA-PCA-PPA [[7]](https://aclanthology.org/W19-4328/)\n",
    "    - `method=\"ppapacppa\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f1c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction = nlpsig.DimReduce(\n",
    "    method=\"gaussian_random_projection\",\n",
    "    n_components=25,\n",
    ")\n",
    "\n",
    "english_token_embeddings_reduced = reduction.fit_transform(english_token_embeddings, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b21f27a-2133-4cf1-a693-98b08407216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_token_embeddings_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1aaa4c-f9b9-4c50-b8b0-b8add1923e08",
   "metadata": {},
   "source": [
    "We can save these embeddings for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c279ac-2184-4ac2-802a-1d044caee6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"english_token_embeddings_{hidden_size}.pkl\",'wb') as f:\n",
    "    pickle.dump(english_token_embeddings, f)\n",
    "with open(f\"english_reduced_token_embeddings_{hidden_size}.pkl\",'wb') as f:\n",
    "    pickle.dump(english_token_embeddings_reduced, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81af6467",
   "metadata": {},
   "source": [
    "As we have embeddings for each token, we can obtain a path for each word by constructing a path of the token embeddings. To do this, we can use the `PrepareData` class and pass in our tokenized dataframe (the dataframe where we have each token in our data and we also have the corresponding id for each word which is saved in the `text_id` column of the tokenized dataframe.\n",
    "\n",
    "We pass in the column which defines the ids, `text_id`, the column which defines the labels, `language`, the token embeddings and the dimension-reduced embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2d8c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_dataset = nlpsig.PrepareData(\n",
    "    text_encoder.tokenized_df,\n",
    "    id_column=\"text_id\",\n",
    "    embeddings=english_token_embeddings,\n",
    "    embeddings_reduced=english_token_embeddings_reduced\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a0e3c-288a-4a23-8851-a68aa6956b61",
   "metadata": {},
   "source": [
    "The class concatenates the embeddings and the dimension-reduced embeddings that are passed into to the class initalisation and stores it in the `.df` attribute of `english_dataset`.\n",
    "\n",
    "Here, the columns beginning with `d` denote the dimensions of the dimension reduced transformer embeddings, whereas the columns beginning with `e` denote the dimensions of embeddings obtained from the transformer.\n",
    "\n",
    "Furthermore, we can see from the printed out information that a `timeline_index` column was added to the dataframe, which is the last column here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b0bd97-3e75-4b64-9454-91ba79f0c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_dataset.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb187e7",
   "metadata": {},
   "source": [
    "We can construct a path by using the `.pad()` method, and result of this is a multi-dimensional array or tensor (in particular a numpy array or PyTorch tensor) which can be then used in some downstream task. It is called \"pad\" because arrays and tensors are rectangular and if there are cases where there isn't enough data (e.g. if a word only has 3 letters/tokens and we want to make paths of length 4), we \"pad\" with either the last token embedding (if we set `zero_padding=False`) or with a vector of zeros (if we set `zero_padding=True`).\n",
    "\n",
    "Here, we construct paths by setting a length of the paths (we call this method `k_last` in the code and we have to specify the length with `k=50` - the maximum sequence length that we used when defining the transformer model).\n",
    "\n",
    "We alternatively can construct to the longest word possible (by setting `method=\"max\"`). The `time_feature` argument allows us to specify what time features we want to keep. Here we don't have any besides the index in which the word is, which is given by `timeline_index` and we choose not to standardise that by specifying `standardise_time_feature=False`.\n",
    "\n",
    "The `pad_by` argument specifies that we are padding for each word (as each word is given a particular `text_id` in the tokenized dataframe above). There is an alternative option to construct a path by looking at the history of a particular embedding (i.e. the stream embeddings that occurred before), but this is not useful here and we will cover that in another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca0623",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_specifics = {\n",
    "    \"pad_by\": \"id\",\n",
    "    \"zero_padding\": True,\n",
    "    \"method\": \"k_last\",\n",
    "    \"k\": 50,\n",
    "    \"features\": [\"timeline_index\"],\n",
    "    \"standardise_method\": [None],\n",
    "    \"embeddings\": \"dim_reduced\",\n",
    "    \"pad_from_below\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19583e9f-f4b9-450e-998c-7416fece349e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "english_word_path = english_dataset.pad(**path_specifics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd79294",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_word_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292fbdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(english_dataset.df[\"text_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7febd295",
   "metadata": {},
   "source": [
    "We also store this array as a dataframe in `.df_padded` so that you can see what the columns correspond to, where columns beginning with `e` denote the dimensions of embeddings obtained from the transformer (here we have none as we only requested to keep the dimension reduced embeddings), and columns beginning with `d` denote the dimensions of the dimension reduced transformer embeddings.\n",
    "\n",
    "We can see for the first word in the dataset (with `text_id==0`), this is a word with 10 letters and we can see how we have padded the word to length 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3217c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# still has the labels and the ids\n",
    "english_dataset.df_padded[english_dataset.df_padded[\"text_id\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40969a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder.df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07994e10",
   "metadata": {},
   "source": [
    "For the padded rows, we give these a label `-1` to denote that they have been added.\n",
    "\n",
    "Note that for padding, the method pads from below by default, but we can pad by above by setting `pad_from_below=False`.\n",
    "\n",
    "To obtain a path as a Numpy array, we use the `.get_path()` method which by default keeps the time features and will remove the id and label columns. We make this more explicit by setting `include_features=True` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_word_path = english_dataset.get_path(include_features=True)\n",
    "english_word_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5e2f91-f171-4ced-9f93-60bf977b6b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_word_path[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d52d9a",
   "metadata": {},
   "source": [
    "## Obtaining path signatures for the english words\n",
    "\n",
    "We use [`signax`](https://github.com/anh-tong/signax) to compute path signatures, which we compute up to depth 2 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e89214",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_depth = 2\n",
    "english_word_sig = np.array(signature(english_word_path, sig_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9a5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_word_sig.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b1ad55",
   "metadata": {},
   "source": [
    "## Obtaining a paths and signatures for words in `corpus_df`\n",
    "\n",
    "Now that we have trained our model and obtained signatures for each word in our sample of english words, we also want to obtain embeddings for the words in `corpus_sample_df`. Currently, `TextEncoder` only works with the data that is passed into the function and stored in `.df` and `.dataset`, so we need to initialise a new `TextEncoder` object with the `corpus_sample_df` dataframe and also the trained model.\n",
    "\n",
    "We can then obtain embeddings easily (recall from above we first need to tokenize the text, and then use the `.obtain_embeddings()` and `.pool_token_embeddings()` methods to do this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015309f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_2 = nlpsig.TextEncoder(\n",
    "    df=corpus_sample_df,\n",
    "    feature_name=\"word\",\n",
    "    model=text_encoder.model,\n",
    "    config=text_encoder.config,\n",
    "    tokenizer=text_encoder.tokenizer,\n",
    "    data_collator=text_encoder.data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83327a8e-2192-4efd-ac04-81f57999d47a",
   "metadata": {},
   "source": [
    "Note that since we're just loading in our pretrained model from above, we could also just have passed in the path to the model directly via the `model_name` argument, and use the `.load_pretrained_model()` method which loads in the model, config, tokenizer and data collator that was used. So the below initialisation achieves the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c762d670-5eba-42f6-8d67-99c5a3f251fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_2 = nlpsig.TextEncoder(\n",
    "    df=corpus_sample_df,\n",
    "    feature_name=\"word\",\n",
    "    model_name=model_name\n",
    ")\n",
    "text_encoder_2.load_pretrained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d9042",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_2.tokenize_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed8f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_2.tokenized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a089032-0f58-47e7-9922-3778da16483c",
   "metadata": {},
   "source": [
    "After tokenizing, we can obtain token embeddings and also pool these token embeddings with `.obtain_embeddings()` and `.pool_token_embeddings()` methods available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0334ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = text_encoder_2.obtain_embeddings(method=\"hidden_layer\", layers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1885be",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7838beaf",
   "metadata": {},
   "source": [
    "To reduce the embeddings, we want to use the same transform that we used earlier on. Recall that we used Gaussian random projections using the [`scikit-learn`](https://scikit-learn.org/stable/modules/random_projection.html) package. After fitting and transforming with the vectors in `english_token_embeddings`, we stored the `sklearn.random_projection.GaussianRandomProjection` object in `reduction.reducer` which we can use again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d7cbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(reduction.reducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74359714",
   "metadata": {},
   "source": [
    "We can then transform new data using the `.transform()` method of the `sklearn.random_projection.GaussianRandomProjection` class which will use the same transformation that we fitted to above when applying dimension reduction to the token embeddings for our corpus of english words (in `english_train`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d27b6ad-8a26-41d5-bf5c-54bc8ada6b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_reduced = reduction.reducer.transform(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3853cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf35fb7-0d6f-4fe9-ad11-6f0919e2fe7d",
   "metadata": {},
   "source": [
    "Optionally, we can save these embeddings for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e35da37-a73b-48d4-ad62-de97f99c78db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"corpus_sample_token_embeddings_{hidden_size}.pkl\",'wb') as f:\n",
    "    pickle.dump(token_embeddings, f)\n",
    "with open(f\"corpus_sample_reduced_token_embeddings_{hidden_size}.pkl\",'wb') as f:\n",
    "    pickle.dump(embeddings_reduced, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3886dbb3",
   "metadata": {},
   "source": [
    "We again obtain paths with the `PrepareData` class, and pass in the tokenized dataframe created in `text_encoder_2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d352f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_2.tokenized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd0e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dataset = nlpsig.PrepareData(\n",
    "    text_encoder_2.tokenized_df,\n",
    "    id_column=\"text_id\",\n",
    "    label_column=\"language\",\n",
    "    embeddings=token_embeddings,\n",
    "    embeddings_reduced=embeddings_reduced\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acebfe66",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus_word_path = corpus_dataset.pad(**path_specifics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746a5e6f",
   "metadata": {},
   "source": [
    "By inspecting the shape of `corpus_word_path`, we see that we have a path for each word and the dimension of the array is `[batch, length of path, channels]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99428872",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_word_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523b4cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus_dataset.df[\"text_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caac355",
   "metadata": {},
   "source": [
    "To obtain a path as a torch tensor, we use the `.get_path()` method which by default keeps the time features and will remove the id and label columns from the path that is generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6c23db",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_path = corpus_dataset.get_path(include_features=True)\n",
    "word_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbab692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9153b35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute path signatures\n",
    "corpus_signatures = np.array(signature(word_path, sig_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f813233",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_signatures.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4936df71-ec25-4fb0-a4c5-139da617fb2e",
   "metadata": {},
   "source": [
    "We obtain the signatures for our inliers and outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df24ce47-638b-45ca-95fa-45d1989c4693",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_word_indices = corpus_sample_df[corpus_sample_df[\"language\"]==\"en\"].index\n",
    "non_english_word_indices = corpus_sample_df[corpus_sample_df[\"language\"]!=\"en\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e251c6a-7046-4db2-bef4-7c6341aac635",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sample_df.iloc[english_word_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343908d6-4627-443c-9086-be5405fdbdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sample_df.iloc[non_english_word_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35225a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain signatures for english words and non-english words in corpus_sample_df\n",
    "inlier_signatures = corpus_signatures[english_word_indices]\n",
    "outlier_signatures = corpus_signatures[non_english_word_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6632eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inlier_signatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3dcab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_signatures.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428e5c23",
   "metadata": {},
   "source": [
    "## Anomaly detection task\n",
    "\n",
    "To recap the task at hand:\n",
    "- We trained a language model using a corpus of english words stored in the `english_train` dataframe.\n",
    "- We have another set of english words (inliers) and some non-english words (outliers) which are stored in the `corpus_sample_df` dataframe.\n",
    "- We now want to see how we could detect the non-english words efficiently, in particular, we use the following method:\n",
    "    - For each word in `english_train` and `corpus_sample_df`, we have a vector representation for them (e.g. we've computed the path signatures for each of them and they are stored in `english_word_sig`).\n",
    "    - For each word in `corpus_sample_df`, we compute the minimum (Euclidean) distance of between its path signature to path signatures for our corpus of known English words (i.e. each row in `english_word_sig`).\n",
    "    - We then look the [ROC curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) to see how well separated are the english words to the non-english words. For a good performance, we hope that there is good separation, and so we measure the success of this method using the [ROCAUC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf2d653-568f-4609-8666-0ec92e4b30e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(inlier_scores, outlier_scores, title=\"\"):\n",
    "    # concatenate scores and labels\n",
    "    y_true = np.concatenate([np.zeros(len(inlier_scores)),\n",
    "                             np.ones(len(outlier_scores))])\n",
    "    scores = np.concatenate([np.array(inlier_scores),\n",
    "                             np.array(outlier_scores)])\n",
    "    \n",
    "    # compute and plot metrics\n",
    "    fpr, tpr, threshold = roc_curve(y_true, scores)\n",
    "    roc_auc = roc_auc_score(y_true, scores)\n",
    "    \n",
    "    plt.title(f\"Receiver Operating Characteristic {title}\")\n",
    "    plt.plot(fpr, tpr, 'b', label = f\"AUC = {round(roc_auc, 2)}\")\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    \n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1991a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_min_euclidean_dist(main_corpus, embedding):\n",
    "    # compute score of individual word\n",
    "    # compute euclidean distance between the embedding to each row in main_corpus\n",
    "    diff = main_corpus - embedding.repeat(main_corpus.shape[0], 1)\n",
    "    euclidean_dist = distances = diff.pow(2).sum(1).sqrt()\n",
    "    return distances.min().item()\n",
    "\n",
    "def compute_scores(main_corpus, inliers, outliers, plot=False, title=\"\"):\n",
    "    # compute scores for inliers and outliers\n",
    "    inlier_scores = [compute_min_euclidean_dist(main_corpus=main_corpus,\n",
    "                                                embedding=embedding)\n",
    "                     for embedding in tqdm(inliers)]\n",
    "    outlier_scores = [compute_min_euclidean_dist(main_corpus=main_corpus,\n",
    "                                                 embedding=embedding)\n",
    "                      for embedding in tqdm(outliers)]\n",
    "    if plot:\n",
    "        return plot_roc_curve(inlier_scores=inlier_scores,\n",
    "                              outlier_scores=outlier_scores,\n",
    "                              title=title)\n",
    "    else:\n",
    "        return inlier_scores, outlier_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae29ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_scores(main_corpus=english_word_sig,\n",
    "               inliers=inlier_signatures,\n",
    "               outliers=outlier_signatures,\n",
    "               plot=True,\n",
    "               title=\"\\n(using depth=2 signatures of dimension reduced streams)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09ca002-eb65-430e-b012-222dfdb27ce3",
   "metadata": {},
   "source": [
    "## Using one-hot encodings\n",
    "\n",
    "Here, we simply construct a path of one-hot encodings of the characters and so the number of channels in the path is 26. We also take a cumulative sum transformation on the path (which has length 50 again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fd5779-fbc8-4729-ac2c-cb9dbc1c4f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_path(char_seq, alpha_len=26):\n",
    "    # construct path via one-hot encoding of characters\n",
    "    n = len(char_seq)\n",
    "    its = np.zeros(n, np.int64)\n",
    "    for i in range(n):\n",
    "        its[i] = ord(char_seq[i]) - 97\n",
    "    A = np.zeros((n, alpha_len))\n",
    "    j = 0\n",
    "    for i in its:\n",
    "        A[j, i] += 1\n",
    "        j += 1\n",
    "\n",
    "    return A\n",
    "\n",
    "def get_one_hot_paths_from_words(words,\n",
    "                                 max_word_len,\n",
    "                                 pad_from_below=True,\n",
    "                                 alpha_len=26,\n",
    "                                 cumsum_transform=True):\n",
    "    # compute path for each word in words\n",
    "    path = np.array(\n",
    "        [\n",
    "            np.vstack(\n",
    "                [\n",
    "                    construct_path(word),\n",
    "                    np.zeros((100 - len(word), alpha_len)),\n",
    "                ]\n",
    "            )\n",
    "            if pad_from_below else\n",
    "            np.vstack(\n",
    "                [\n",
    "                    np.zeros((100 - len(word), alpha_len)),\n",
    "                    construct_path(word),\n",
    "                ]\n",
    "            )\n",
    "            for word in tqdm(words)\n",
    "        ]\n",
    "    )\n",
    "    if pad_from_below:\n",
    "        path = path[:, :max_word_len, :alpha_len]\n",
    "    else:\n",
    "        path = path[:, -max_word_len:, :alpha_len]\n",
    "    if cumsum_transform:\n",
    "        path = np.cumsum(path, axis=1)\n",
    "\n",
    "    return torch.tensor(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3773c45d-5335-4b57-92c6-a2ee82fceca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_word_one_hot_paths = get_one_hot_paths_from_words(words=english_train[\"word\"],\n",
    "                                                          max_word_len=20,\n",
    "                                                          pad_from_below=True,\n",
    "                                                          cumsum_transform=True)\n",
    "corpus_one_hot_paths = get_one_hot_paths_from_words(words=corpus_sample_df[\"word\"],\n",
    "                                                    max_word_len=20,\n",
    "                                                    pad_from_below=True,\n",
    "                                                    cumsum_transform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f11e713-0484-4bbc-852a-d19cd8817302",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_word_one_hot_paths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceef64a-f2f6-492d-85b0-6d693b1023e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_one_hot_paths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05675578-44a5-4e6a-b557-8e31ebf22382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute signatures for english words\n",
    "english_word_one_hot_signatures = signature(english_word_one_hot_paths, 2)\n",
    "\n",
    "# compute signatures for inliers and outliers\n",
    "corpus_one_hot_signatures = signature(corpus_one_hot_paths, 2)\n",
    "inlier_one_hot_signatures = corpus_one_hot_signatures[english_word_indices]\n",
    "outlier_one_hot_signatures = corpus_one_hot_signatures[non_english_word_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c0b07c-3990-42e1-893c-1132bf25c28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_word_one_hot_signatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09e2bcd-b27f-46da-b4de-0fcdaadb6889",
   "metadata": {},
   "outputs": [],
   "source": [
    "inlier_one_hot_signatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eddd1ca-fc2e-446d-a5f9-2398a5cb36c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_one_hot_signatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1661e518-bbed-4128-9386-f46acd865418",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_scores(main_corpus=english_word_one_hot_signatures,\n",
    "               inliers=inlier_one_hot_signatures,\n",
    "               outliers=outlier_one_hot_signatures,\n",
    "               plot=True,\n",
    "               title=\"\\n(using depth=2 signatures of one-hot encoding streams)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253964b4-8ccf-4e9e-ad8c-6ae86bc375ce",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "The computations described in this notebook were performed using the Baskerville Tier 2 HPC service (https://www.baskerville.ac.uk/). Baskerville was funded by the EPSRC and UKRI through the World Class Labs scheme (EP/T022221/1) and the Digital Research Infrastructure programme (EP/W032244/1) and is operated by Advanced Research Computing at the University of Birmingham.\n",
    "\n",
    "## References\n",
    "\n",
    "[1] Devlin, J., Chang, M.W., Lee, K. and Toutanova, K., 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. _arXiv preprint arXiv:1810.04805_.\n",
    "\n",
    "[2] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L. and Stoyanov, V., 2019. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_.\n",
    "\n",
    "[3] McInnes, L., and Healy, J. 2018. UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction, _arXiv preprint arXiv:1802.03426_.\n",
    "\n",
    "[4] Tipping, M. E., and Bishop, C. M., 1999. Probabilistic principal component analysis. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 61(3), 611-622.\n",
    "\n",
    "[5] van der Maaten, L.J.P., and Hinton, G.E., 2008. Visualizing High-Dimensional Data using t-SNE. _Journal of Machine Learning Research_, 9:2579-2605.\n",
    "\n",
    "\n",
    "[6] Mu, J., Bhat, S., and Viswanath, P. (2017). All-but-the-top: Simple and effective postprocessing for word representations. _arXiv preprint arXiv:1702.01417_.\n",
    "\n",
    "[7] Raunak, V., Gupta, V., and Metze, F. (2019). Effective dimensionality reduction for word embeddings. In _Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP- 2019)_, 235–243."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang-analysis-nlpsig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
