{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c43e605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading in english_train from data/corpus.pkl\n",
      "loading in english_train from data/english_train.pkl\n",
      "loading in corpus_sample_df from data/corpus_sample.pkl\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import pipeline, RobertaForMaskedLM\n",
    "\n",
    "import nlpsig\n",
    "\n",
    "from signax import signature\n",
    "from signature_mahalanobis_knn import SignatureMahalanobisKNN\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from load_data import seed, english_train, corpus_sample_df\n",
    "\n",
    "import time\n",
    "from signature_mahalanobis_knn import SignatureMahalanobisKNN\n",
    "from signature_mahalanobis_knn.utils import (\n",
    "    compute_auc_given_dists,\n",
    "    plot_cdf_given_dists,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e481f7d-7228-49a5-81a4-c9ccd5b2795a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Anomaly Detection task\n",
    "\n",
    "In natural language processing (NLP) tasks, we often are dealing with high dimensional streams of data. Neural network architectures known as Transformers have been shown to be very effective in NLP tasks, and we can use these to obtain high dimensional streams of embeddings for words/tokenised text. In this notebook, we will look at how we can use path signature techniques to analyse these high dimensional streams of embeddings. In particular, we will look at how we can perform outlier detection on the path signatures of the embeddings obtained from a pre-trained Transformer. \n",
    "\n",
    "In particular, we consider the task of determining whether a word is an english word or not by using the path signature of the stream of _character_ embeddings of the word. That is, each word is represented as a stream of character embeddings. We do this by training a Transformer model (from scratch) on a _masked language modelling_ task (or _Cloze_ task) as described in [[1]](https://arxiv.org/abs/1810.04805) using a corpus of english words, and then use this model to obtain a stream of (character) embeddings for a sample of (english and non-english) words. Path signature techniques are applied to analyse the streams of embeddings and finally we attempt to detect the non-english words as outliers in the space of path signatures.\n",
    "\n",
    "Since we are dealing with high dimensional streams of data, we will use a dimension reduction technique to reduce the dimension of the embeddings before computing the path signature. We will look at how we can perform outlier detection on the path signatures of the dimension-reduced embeddings.\n",
    "\n",
    "The pipeline for this task is as follows:\n",
    "1. Train a Transformer model on a masked language modelling task using a corpus of english words.\n",
    "2. Obtain a stream of character embeddings for a sample of english and non-english words using the trained model (note that we ensure that the english words in this sample are not in the training corpus to pre-train the Transforemr).\n",
    "    - The english words in this sample are our _inlier_ class while the non-english words are our _outlier_ class in this example.\n",
    "3. Perform dimension reduction on the streams of embeddings.\n",
    "4. Compute the path signature of the dimension-reduced embeddings.\n",
    "5. Perform outlier detection on the path signatures to detect the non-english words.\n",
    "\n",
    "In this notebook, we illustrate how we can use the [`nlpsig`](https://github.com/datasig-ac-uk/nlpsig) package to utilise transformers in order to obtain streams of high dimensional embeddings, which can then be analysed using path signature techniques.\n",
    "\n",
    "Furthermore, we utilise the [`signature_mahalanobis_knn`](https://github.com/datasig-ac-uk/signature_mahalanobis_knn) library to perform outlier detection on the path signatures. Full details of our approach can be found in [[2]](https://arxiv.org/abs/2006.03487) _Dimensionless Anomaly Detection on Multivariate Streams with Variance Norm and Path Signature_ by Zhen Shao, Ryan Sze-Yin Chan, Thomas Cochrane, Peter Foster, Terry Lyons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d3859",
   "metadata": {},
   "source": [
    "## Language dataset\n",
    "\n",
    "In the `data/` folder, we have several text folders of words from different languages:\n",
    "- `wordlist_de.txt`: German words\n",
    "- `wordlist_en.txt`: English words\n",
    "- `wordlist_fr.txt`: French words\n",
    "- `wordlist_it.txt`: Italian words\n",
    "- `wordlist_pl.txt`: Polish words\n",
    "- `wordlist_sv.txt`: Swedish words\n",
    "\n",
    "We additionally have a `alphabet.txt` file which just stores the alphabet characters ('a', 'b', 'c', ...).\n",
    "\n",
    "The task is to split the words into its individual characters and to obtain an embedding for each of them. We can represent a word by a path of its character embeddings and compute its path signature to use as features in predicting the language for which the word belongs.\n",
    "\n",
    "Here we look at obtaining embeddings using a Transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aed597b",
   "metadata": {},
   "source": [
    "## Prepare training data and test data\n",
    "\n",
    "We prepare our data in the [`language_dataset_anomalies_data.ipynb`](language_dataset_anomalies_data.ipynb) notebook and load in our data using the [`load_data.py`](load_data.py) script, so look in there for more details.\n",
    "\n",
    "Our test data will consist a sample of 10000 english words and 10000 non-english words (2000 from each of the remaining languages). We will use the remaining english words as our training data to train the Transformer model and as our corpus of \"normality\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "739f58df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>knots</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stalemating</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>whoops</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>implantation</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>levers</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70610</th>\n",
       "      <td>forcefulness</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70611</th>\n",
       "      <td>fat</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70612</th>\n",
       "      <td>creakier</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70613</th>\n",
       "      <td>ramming</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70614</th>\n",
       "      <td>facsimiles</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70615 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               word language\n",
       "0             knots       en\n",
       "1       stalemating       en\n",
       "2            whoops       en\n",
       "3      implantation       en\n",
       "4            levers       en\n",
       "...             ...      ...\n",
       "70610  forcefulness       en\n",
       "70611           fat       en\n",
       "70612      creakier       en\n",
       "70613       ramming       en\n",
       "70614    facsimiles       en\n",
       "\n",
       "[70615 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b0681e",
   "metadata": {},
   "source": [
    "To make the dataset bit more manageable, we take a sample of each of the languages. In our resulting corpus, we have an equal amount of english (inliers) and non-english words (outliers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e53354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "language\n",
       "en    10000\n",
       "de     2000\n",
       "fr     2000\n",
       "it     2000\n",
       "pl     2000\n",
       "sv     2000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sample_df[\"language\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b80c6a0f-cdee-41dc-9c25-467a1cda7344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abblendet</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bestechendes</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>narrensicheren</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inakzeptable</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abbestelle</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word language\n",
       "0       abblendet       de\n",
       "1    bestechendes       de\n",
       "2  narrensicheren       de\n",
       "3    inakzeptable       de\n",
       "4      abbestelle       de"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdcaf11",
   "metadata": {},
   "source": [
    "## Training a language model\n",
    "\n",
    "We want to train a masked language model for our corpus of English words. In particular, we mask out particular letters and ask our model to try predict the masked letter.\n",
    "We do this using the [`nlpsig.TextEncoder`](https://nlpsig.readthedocs.io/en/latest/encode_text.html#nlpsig.encode_text.TextEncoder) class which provides a wrapper around the `transformers` library, and have done this in a separate notebook - see [`train_english_char_bert.ipynb`](train_english_char_bert.ipynb). \n",
    "\n",
    "In that notebook, we trained a [Roberta](https://huggingface.co/docs/transformers/model_doc/roberta) language model described in [[3]](https://arxiv.org/abs/1907.11692), and uploaded the model to the [Huggingface model hub](https://huggingface.co/rchan26/english-char-roberta). We can easily load this model in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e46427e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"rchan26/english-char-roberta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "416c87ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train_encoder = nlpsig.TextEncoder(\n",
    "    df=english_train,\n",
    "    feature_name=\"word\",\n",
    "    model_name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e29d131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] By default, `.load_pretrained_model()` uses `AutoModel` to load in the model. If you want to load the model for a specific task, reset the `.model` attribute.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870685dc880846fdb143b064b47f5c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb13109f8fb46e29aec6f47a528cd0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/470 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb7bd96c5dc46e6be5394818f289a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf6d939330042c1b00484dcef828898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading added_tokens.json:   0%|          | 0.00/75.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8b22e6b5d445e7acc51f2399ce1543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/173M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at rchan26/english-char-roberta and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "english_train_encoder.load_pretrained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f13cbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(57, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(52, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_train_encoder.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14186065",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train_encoder.model = RobertaForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bca8c4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(57, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(52, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=57, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_train_encoder.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5646433f",
   "metadata": {},
   "source": [
    "## Obtaining token and word embeddings\n",
    "\n",
    "There are many ways in which one can get token embeddings (here, each character is a token) from the transformer network, as the output is the layers for the full network. A few ways are by using the [`nlpsig.TextEncoder.obtain_embeddings`](https://nlpsig.readthedocs.io/en/latest/encode_text.html#nlpsig.encode_text.TextEncoder.obtain_embeddings) method:\n",
    "\n",
    "- Returning the output of a particular hidden layer\n",
    "    - use `.obtain_embeddings(method = \"hidden_layer\", layers = l)` where `l` is the layer you want\n",
    "    - If no layer is requested, it will just give you the second-to-last hidden layer of the transformer network.\n",
    "- Concatenate the output of several hidden layers\n",
    "    - use `.obtain_embeddings(method = \"concatenate\", layers = [l_1, l_2, ...])` where `[l_1, l_2, ...]` is a list of layers you want to concatenate\n",
    "- Element-wise sum the output of several hidden layers\n",
    "    - use `.obtain_embeddings(method = \"sum\" , layers = [l_1, l_2, ...])` where `[l_1, l_2, ...]` is a list of layers you want to sum\n",
    "- Mean the output of several hidden layers\n",
    "    - use `.obtain_embeddings(method = \"mean\" , layers = [l_1, l_2, ...])` where `[l_1, l_2, ...]` is a list of layers you want to take the mean average over\n",
    "\n",
    "Each of these methods will return a 2-dimensional array with dimensions `[token, embedding]`.\n",
    "\n",
    "If a more custom way to obtain embeddings from the hidden layers, you can specify what layers you want, and it will return them (i.e. using `.obtain_embeddings(method = \"hidden_layer\", layers = [l_1, l_2, ...])` where `[l_1, l_2, ...]` is a list of hidden layers you want) and so the output will be a 3-dimensional array with dimensions `[layer, token, embedding]` for which you would need to combine in such a way that you would have an embedding for each token. The above methods would return a 2-dimensional array with dimensions `[token, embedding]`.\n",
    "\n",
    "In the below, we just obtain the last hidden layer of the network (the 6th one in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad57446b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Setting return_special_tokens_mask=True\n",
      "[INFO] Tokenizing the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d77478807a8440d80b9efa910907548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70615 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving the tokenized text for each sentence into `.df['tokens']`...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3e2de55fc84297af2aaf59e1cc040d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70615 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating tokenized dataframe and setting in `.tokenized_df` attribute...\n",
      "[INFO] Note: 'text_id' is the column name for denoting the corresponding text id\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['word', 'language', 'input_ids', 'attention_mask', 'special_tokens_mask', 'tokens'],\n",
       "    num_rows: 70615\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_train_encoder.tokenize_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06e80725-925f-4ada-a58a-02e0b71ff023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d424873b494caba3a575853070e357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/707 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# setting the model to use the CPU\n",
    "english_train_encoder.model.to(\"cpu\")\n",
    "english_token_embeddings = english_train_encoder.obtain_embeddings(method=\"hidden_layer\", layers=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73afe43d-26f8-437b-9a32-ceee73fb88d1",
   "metadata": {},
   "source": [
    "By inspecting the shape of this, we can see that we have a 2-dimensional array with dimensions `[token, embedding]` where the embeddings are 768 dimensional in this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "282d5b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(601949, 768)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_token_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f09e23",
   "metadata": {},
   "source": [
    "Now that we have token embeddings for each text, it is possible to pool these embeddings to obtain an embedding for the full text (for this case, this embedding would represent the word itself. We can use the [`nlpsig.TextEncoder.pool_token_embeddings`](https://nlpsig.readthedocs.io/en/latest/encode_text.html#nlpsig.encode_text.TextEncoder.pool_token_embeddings) method for doing this.\n",
    "\n",
    "Again, there are several methods and full details can be found in the documentation, but a few are:\n",
    "\n",
    "- Taking the mean of the token embeddings\n",
    "    - use `.pool_token_embeddings(method = \"mean\")`\n",
    "- Taking the element-wise max of the token embeddings\n",
    "    - use `.pool_token_embeddings(method = \"max\")`\n",
    "- Taking the element-wise sum of the token embeddings\n",
    "    - use `.pool_token_embeddings(method = \"sum\")`\n",
    "- Taking the token-embedding for the CLS token (a special token that is used in some transformers like BERT and RoBERTa)\n",
    "    - but this is only available to us if we set `skip_special_tokens=False` when tokenizing the text with `.tokenize_text()` method (note by default, this is set to `True` and so we don't have access to this method here)\n",
    "    - use `.pool_token_embeddings(method = \"cls\")`\n",
    "        - note this will produce an error if the CLS token is not available...\n",
    "\n",
    "For example, to pool the character embeddings by taking the mean of the token embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcfd84d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d3c881a8834f81af828ae1854d9627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70615 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pooled_english_mean = english_train_encoder.pool_token_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b491dc",
   "metadata": {},
   "source": [
    "Again, we can inspect the shape and we can see that we have embeddings for each of our words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c876954d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70615, 768)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_english_mean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c0fffd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dimension reduction\n",
    "\n",
    "We can perform dimension reduction with the [`nlpsig.DimReduce`](https://nlpsig.readthedocs.io/en/latest/dimensionality_reduction.html#nlpsig.dimensionality_reduction.DimReduce) class. Here, we will use Gaussian Random Projections (implemented using [`scikit-learn`](https://scikit-learn.org/stable/modules/random_projection.html)) by setting `method=\"gaussian_random_projection\"`, but there are other standard methods available:\n",
    "- UMAP [[4]](https://arxiv.org/abs/1802.03426) (implemented using the [`umap-learn`](https://umap-learn.readthedocs.io/en/latest/api.html))\n",
    "    - `method=\"umap\"`\n",
    "- PCA [[5]](http://www.miketipping.com/papers/met-mppca.pdf) (implemented using [`scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html))\n",
    "    - `method=\"pca\"`\n",
    "- TSNE [[6]](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) (implemented using [`scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html))\n",
    "    - `method=\"tsne\"`\n",
    "- Post Processing Algorithm (PPA) with PCA (PPA-PCA) [[7]](https://arxiv.org/abs/1702.01417)\n",
    "    - `method=\"ppapca\"`\n",
    "- PPA-PCA-PPA [[7]](https://aclanthology.org/W19-4328/)\n",
    "    - `method=\"ppapacppa\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "981f1c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction = nlpsig.DimReduce(\n",
    "    method=\"gaussian_random_projection\",\n",
    "    n_components=25,\n",
    ")\n",
    "\n",
    "english_token_embeddings_reduced = reduction.fit_transform(english_token_embeddings, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b21f27a-2133-4cf1-a693-98b08407216d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(601949, 25)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_token_embeddings_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1aaa4c-f9b9-4c50-b8b0-b8add1923e08",
   "metadata": {},
   "source": [
    "We can save these embeddings for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6c279ac-2184-4ac2-802a-1d044caee6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"english_token_embeddings.pkl\",'wb') as f:\n",
    "    pickle.dump(english_token_embeddings, f)\n",
    "with open(f\"english_reduced_token_embeddings.pkl\",'wb') as f:\n",
    "    pickle.dump(english_token_embeddings_reduced, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81af6467",
   "metadata": {},
   "source": [
    "As we have embeddings for each token, we can obtain a path for each word by constructing a path of the token embeddings. To do this, we can use the [`nlpsig.PrepareData`](https://nlpsig.readthedocs.io/en/latest/data_preparation.html#nlpsig.data_preparation.PrepareData) class and pass in our tokenized dataframe (the dataframe where we have each token in our data and we also have the corresponding id for each word which is saved in the `text_id` column of the tokenized dataframe.\n",
    "\n",
    "We pass in the column which defines the ids, `text_id`, the column which defines the labels, `language`, the token embeddings and the dimension-reduced embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b2d8c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Note 'datetime' is not a column in `.df`, so only 'timeline_index' is added.\n",
      "[INFO] As 'datetime' is not a column in `.df`, we assume that the data is ordered by time with respect to the id.\n",
      "[INFO] Adding 'timeline_index' feature...\n"
     ]
    }
   ],
   "source": [
    "english_dataset = nlpsig.PrepareData(\n",
    "    original_df=english_train_encoder.tokenized_df,\n",
    "    id_column=\"text_id\",\n",
    "    embeddings=english_token_embeddings,\n",
    "    embeddings_reduced=english_token_embeddings_reduced\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a0e3c-288a-4a23-8851-a68aa6956b61",
   "metadata": {},
   "source": [
    "The class concatenates the embeddings and the dimension-reduced embeddings that are passed into to the class initalisation and stores it in the `.df` attribute of `english_dataset`.\n",
    "\n",
    "Here, the columns beginning with `d` denote the dimensions of the dimension reduced transformer embeddings, whereas the columns beginning with `e` denote the dimensions of embeddings obtained from the transformer.\n",
    "\n",
    "Furthermore, we can see from the printed out information that a `timeline_index` column was added to the dataframe, which is the last column here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05b0bd97-3e75-4b64-9454-91ba79f0c610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "      <th>tokens</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>...</th>\n",
       "      <th>e760</th>\n",
       "      <th>e761</th>\n",
       "      <th>e762</th>\n",
       "      <th>e763</th>\n",
       "      <th>e764</th>\n",
       "      <th>e765</th>\n",
       "      <th>e766</th>\n",
       "      <th>e767</th>\n",
       "      <th>e768</th>\n",
       "      <th>timeline_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>k</td>\n",
       "      <td>6.100298</td>\n",
       "      <td>-14.199132</td>\n",
       "      <td>8.581287</td>\n",
       "      <td>-6.761333</td>\n",
       "      <td>2.903222</td>\n",
       "      <td>8.518986</td>\n",
       "      <td>-1.497842</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.533906</td>\n",
       "      <td>-0.016500</td>\n",
       "      <td>-0.841333</td>\n",
       "      <td>0.967386</td>\n",
       "      <td>-1.594480</td>\n",
       "      <td>1.674844</td>\n",
       "      <td>0.409466</td>\n",
       "      <td>-1.230852</td>\n",
       "      <td>0.450279</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>n</td>\n",
       "      <td>6.067597</td>\n",
       "      <td>-7.157618</td>\n",
       "      <td>3.922382</td>\n",
       "      <td>2.873782</td>\n",
       "      <td>4.717306</td>\n",
       "      <td>-0.953772</td>\n",
       "      <td>-6.488945</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.743229</td>\n",
       "      <td>-0.415965</td>\n",
       "      <td>-0.220593</td>\n",
       "      <td>0.931776</td>\n",
       "      <td>0.089646</td>\n",
       "      <td>-0.172894</td>\n",
       "      <td>1.922119</td>\n",
       "      <td>-0.317665</td>\n",
       "      <td>1.383081</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>o</td>\n",
       "      <td>9.407446</td>\n",
       "      <td>-8.487071</td>\n",
       "      <td>3.602750</td>\n",
       "      <td>-6.851043</td>\n",
       "      <td>7.014906</td>\n",
       "      <td>0.959817</td>\n",
       "      <td>1.894073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.522140</td>\n",
       "      <td>0.108025</td>\n",
       "      <td>-0.334635</td>\n",
       "      <td>1.223601</td>\n",
       "      <td>0.198564</td>\n",
       "      <td>0.080706</td>\n",
       "      <td>2.160928</td>\n",
       "      <td>0.805575</td>\n",
       "      <td>-0.801284</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>t</td>\n",
       "      <td>1.125513</td>\n",
       "      <td>0.428988</td>\n",
       "      <td>3.385085</td>\n",
       "      <td>1.941234</td>\n",
       "      <td>4.131640</td>\n",
       "      <td>-0.509308</td>\n",
       "      <td>7.753160</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.341119</td>\n",
       "      <td>0.648110</td>\n",
       "      <td>0.571438</td>\n",
       "      <td>0.584918</td>\n",
       "      <td>-0.637328</td>\n",
       "      <td>0.751738</td>\n",
       "      <td>1.818443</td>\n",
       "      <td>0.990760</td>\n",
       "      <td>0.115542</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>s</td>\n",
       "      <td>2.313937</td>\n",
       "      <td>0.054572</td>\n",
       "      <td>-2.763758</td>\n",
       "      <td>-3.833287</td>\n",
       "      <td>4.874951</td>\n",
       "      <td>-3.330217</td>\n",
       "      <td>-1.284547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345745</td>\n",
       "      <td>0.988667</td>\n",
       "      <td>-0.534669</td>\n",
       "      <td>2.455460</td>\n",
       "      <td>-0.635259</td>\n",
       "      <td>0.844671</td>\n",
       "      <td>0.778486</td>\n",
       "      <td>0.768183</td>\n",
       "      <td>-1.666858</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601944</th>\n",
       "      <td>70614</td>\n",
       "      <td>en</td>\n",
       "      <td>m</td>\n",
       "      <td>1.639216</td>\n",
       "      <td>0.717788</td>\n",
       "      <td>-1.328302</td>\n",
       "      <td>-12.343803</td>\n",
       "      <td>-4.282854</td>\n",
       "      <td>-9.771315</td>\n",
       "      <td>-2.443641</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038248</td>\n",
       "      <td>0.369698</td>\n",
       "      <td>-1.192643</td>\n",
       "      <td>0.888171</td>\n",
       "      <td>-0.688071</td>\n",
       "      <td>-0.225089</td>\n",
       "      <td>1.232774</td>\n",
       "      <td>0.170042</td>\n",
       "      <td>-0.456451</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601945</th>\n",
       "      <td>70614</td>\n",
       "      <td>en</td>\n",
       "      <td>i</td>\n",
       "      <td>-7.668247</td>\n",
       "      <td>-9.949147</td>\n",
       "      <td>4.931896</td>\n",
       "      <td>-6.880606</td>\n",
       "      <td>0.885773</td>\n",
       "      <td>1.801310</td>\n",
       "      <td>-4.665092</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.070264</td>\n",
       "      <td>-0.510370</td>\n",
       "      <td>-0.729678</td>\n",
       "      <td>0.607366</td>\n",
       "      <td>1.258828</td>\n",
       "      <td>0.519082</td>\n",
       "      <td>0.996714</td>\n",
       "      <td>1.287260</td>\n",
       "      <td>-0.600449</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601946</th>\n",
       "      <td>70614</td>\n",
       "      <td>en</td>\n",
       "      <td>l</td>\n",
       "      <td>1.901520</td>\n",
       "      <td>2.117802</td>\n",
       "      <td>-1.211096</td>\n",
       "      <td>-7.594829</td>\n",
       "      <td>8.067634</td>\n",
       "      <td>-1.348465</td>\n",
       "      <td>-8.490309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026670</td>\n",
       "      <td>-0.873979</td>\n",
       "      <td>-0.078845</td>\n",
       "      <td>-1.068502</td>\n",
       "      <td>-0.159796</td>\n",
       "      <td>0.163264</td>\n",
       "      <td>2.670635</td>\n",
       "      <td>2.315694</td>\n",
       "      <td>1.644772</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601947</th>\n",
       "      <td>70614</td>\n",
       "      <td>en</td>\n",
       "      <td>e</td>\n",
       "      <td>1.672272</td>\n",
       "      <td>-4.530347</td>\n",
       "      <td>-5.324213</td>\n",
       "      <td>5.228519</td>\n",
       "      <td>8.263039</td>\n",
       "      <td>2.614076</td>\n",
       "      <td>-5.229039</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.922539</td>\n",
       "      <td>-0.309774</td>\n",
       "      <td>0.435103</td>\n",
       "      <td>-0.866393</td>\n",
       "      <td>-0.739616</td>\n",
       "      <td>-2.028092</td>\n",
       "      <td>-0.655358</td>\n",
       "      <td>0.945581</td>\n",
       "      <td>-1.189239</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601948</th>\n",
       "      <td>70614</td>\n",
       "      <td>en</td>\n",
       "      <td>s</td>\n",
       "      <td>-0.452957</td>\n",
       "      <td>-0.818362</td>\n",
       "      <td>-7.153192</td>\n",
       "      <td>4.422605</td>\n",
       "      <td>-4.524617</td>\n",
       "      <td>-6.140039</td>\n",
       "      <td>-3.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.930752</td>\n",
       "      <td>1.927993</td>\n",
       "      <td>0.516500</td>\n",
       "      <td>-0.898273</td>\n",
       "      <td>-0.413304</td>\n",
       "      <td>-0.436183</td>\n",
       "      <td>0.755781</td>\n",
       "      <td>2.223152</td>\n",
       "      <td>-0.613211</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>601949 rows × 797 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id language tokens        d1         d2        d3         d4  \\\n",
       "0             0       en      k  6.100298 -14.199132  8.581287  -6.761333   \n",
       "1             0       en      n  6.067597  -7.157618  3.922382   2.873782   \n",
       "2             0       en      o  9.407446  -8.487071  3.602750  -6.851043   \n",
       "3             0       en      t  1.125513   0.428988  3.385085   1.941234   \n",
       "4             0       en      s  2.313937   0.054572 -2.763758  -3.833287   \n",
       "...         ...      ...    ...       ...        ...       ...        ...   \n",
       "601944    70614       en      m  1.639216   0.717788 -1.328302 -12.343803   \n",
       "601945    70614       en      i -7.668247  -9.949147  4.931896  -6.880606   \n",
       "601946    70614       en      l  1.901520   2.117802 -1.211096  -7.594829   \n",
       "601947    70614       en      e  1.672272  -4.530347 -5.324213   5.228519   \n",
       "601948    70614       en      s -0.452957  -0.818362 -7.153192   4.422605   \n",
       "\n",
       "              d5        d6        d7  ...      e760      e761      e762  \\\n",
       "0       2.903222  8.518986 -1.497842  ... -1.533906 -0.016500 -0.841333   \n",
       "1       4.717306 -0.953772 -6.488945  ... -0.743229 -0.415965 -0.220593   \n",
       "2       7.014906  0.959817  1.894073  ... -0.522140  0.108025 -0.334635   \n",
       "3       4.131640 -0.509308  7.753160  ... -0.341119  0.648110  0.571438   \n",
       "4       4.874951 -3.330217 -1.284547  ...  0.345745  0.988667 -0.534669   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "601944 -4.282854 -9.771315 -2.443641  ... -0.038248  0.369698 -1.192643   \n",
       "601945  0.885773  1.801310 -4.665092  ... -1.070264 -0.510370 -0.729678   \n",
       "601946  8.067634 -1.348465 -8.490309  ...  0.026670 -0.873979 -0.078845   \n",
       "601947  8.263039  2.614076 -5.229039  ... -1.922539 -0.309774  0.435103   \n",
       "601948 -4.524617 -6.140039 -3.085102  ... -1.930752  1.927993  0.516500   \n",
       "\n",
       "            e763      e764      e765      e766      e767      e768  \\\n",
       "0       0.967386 -1.594480  1.674844  0.409466 -1.230852  0.450279   \n",
       "1       0.931776  0.089646 -0.172894  1.922119 -0.317665  1.383081   \n",
       "2       1.223601  0.198564  0.080706  2.160928  0.805575 -0.801284   \n",
       "3       0.584918 -0.637328  0.751738  1.818443  0.990760  0.115542   \n",
       "4       2.455460 -0.635259  0.844671  0.778486  0.768183 -1.666858   \n",
       "...          ...       ...       ...       ...       ...       ...   \n",
       "601944  0.888171 -0.688071 -0.225089  1.232774  0.170042 -0.456451   \n",
       "601945  0.607366  1.258828  0.519082  0.996714  1.287260 -0.600449   \n",
       "601946 -1.068502 -0.159796  0.163264  2.670635  2.315694  1.644772   \n",
       "601947 -0.866393 -0.739616 -2.028092 -0.655358  0.945581 -1.189239   \n",
       "601948 -0.898273 -0.413304 -0.436183  0.755781  2.223152 -0.613211   \n",
       "\n",
       "        timeline_index  \n",
       "0                    1  \n",
       "1                    2  \n",
       "2                    3  \n",
       "3                    4  \n",
       "4                    5  \n",
       "...                ...  \n",
       "601944               6  \n",
       "601945               7  \n",
       "601946               8  \n",
       "601947               9  \n",
       "601948              10  \n",
       "\n",
       "[601949 rows x 797 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_dataset.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb187e7",
   "metadata": {},
   "source": [
    "We can construct a path by using the [`nlpsig.PrepareData.pad`](https://nlpsig.readthedocs.io/en/latest/data_preparation.html#nlpsig.data_preparation.PrepareData.pad) method, and result of this is a multi-dimensional array or tensor (in particular a numpy array or PyTorch tensor) which can be then used in some downstream task. It is called \"pad\" because arrays and tensors are rectangular and if there are cases where there isn't enough data (e.g. if a word only has 3 letters/tokens and we want to make paths of length 4), we \"pad\" with either the last token embedding (if we set `zero_padding=False`) or with a vector of zeros (if we set `zero_padding=True`).\n",
    "\n",
    "Here, we construct paths by setting a length of the paths (we call this method `k_last` in the code and we have to specify the length with `k=50` - the maximum sequence length that we used when defining the transformer model).\n",
    "\n",
    "We alternatively can construct to the longest word possible (by setting `method=\"max\"`). The `time_feature` argument allows us to specify what time features we want to keep. Here we don't have any besides the index in which the word is, which is given by `timeline_index` and we choose not to standardise that by specifying `standardise_time_feature=False`.\n",
    "\n",
    "The `pad_by` argument specifies that we are padding for each word (as each word is given a particular `text_id` in the tokenized dataframe above). There is an alternative option to construct a path by looking at the history of a particular embedding (i.e. the stream embeddings that occurred before), but this is not useful here in this context, and we will cover that in another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6ca0623",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_specifics = {\n",
    "    \"pad_by\": \"id\",\n",
    "    \"zero_padding\": True,\n",
    "    \"method\": \"k_last\",\n",
    "    \"k\": 50,\n",
    "    \"features\": [\"timeline_index\"],\n",
    "    \"standardise_method\": [None],\n",
    "    \"embeddings\": \"dim_reduced\",\n",
    "    \"pad_from_below\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19583e9f-f4b9-450e-998c-7416fece349e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ffcc820fbf4979b522253b30618986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70615 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "english_word_path = english_dataset.pad(**path_specifics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccd79294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70615, 50, 27)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_word_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "292fbdd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70615"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_dataset.df[\"text_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7febd295",
   "metadata": {},
   "source": [
    "We also store this array as a dataframe in `.df_padded` so that you can see what the columns correspond to, where columns beginning with `e` denote the dimensions of embeddings obtained from the transformer (here we have none as we only requested to keep the dimension reduced embeddings), and columns beginning with `d` denote the dimensions of the dimension reduced transformer embeddings.\n",
    "\n",
    "We can see for the first word in the dataset (with `text_id==0`), this is a word with 10 letters and we can see how we have padded the word to length 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3217c3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeline_index</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>d8</th>\n",
       "      <th>d9</th>\n",
       "      <th>...</th>\n",
       "      <th>d17</th>\n",
       "      <th>d18</th>\n",
       "      <th>d19</th>\n",
       "      <th>d20</th>\n",
       "      <th>d21</th>\n",
       "      <th>d22</th>\n",
       "      <th>d23</th>\n",
       "      <th>d24</th>\n",
       "      <th>d25</th>\n",
       "      <th>text_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6.100298</td>\n",
       "      <td>-14.199132</td>\n",
       "      <td>8.581287</td>\n",
       "      <td>-6.761333</td>\n",
       "      <td>2.903222</td>\n",
       "      <td>8.518986</td>\n",
       "      <td>-1.497842</td>\n",
       "      <td>2.556130</td>\n",
       "      <td>1.165959</td>\n",
       "      <td>...</td>\n",
       "      <td>3.761808</td>\n",
       "      <td>-3.724018</td>\n",
       "      <td>4.041551</td>\n",
       "      <td>0.700114</td>\n",
       "      <td>5.538017</td>\n",
       "      <td>5.933544</td>\n",
       "      <td>1.699548</td>\n",
       "      <td>-1.927170</td>\n",
       "      <td>-4.084645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6.067597</td>\n",
       "      <td>-7.157618</td>\n",
       "      <td>3.922382</td>\n",
       "      <td>2.873782</td>\n",
       "      <td>4.717306</td>\n",
       "      <td>-0.953772</td>\n",
       "      <td>-6.488945</td>\n",
       "      <td>0.626723</td>\n",
       "      <td>-2.640919</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.538846</td>\n",
       "      <td>8.980839</td>\n",
       "      <td>6.784652</td>\n",
       "      <td>3.221316</td>\n",
       "      <td>9.305283</td>\n",
       "      <td>-9.060441</td>\n",
       "      <td>4.503458</td>\n",
       "      <td>8.616587</td>\n",
       "      <td>4.175737</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9.407446</td>\n",
       "      <td>-8.487071</td>\n",
       "      <td>3.602750</td>\n",
       "      <td>-6.851043</td>\n",
       "      <td>7.014906</td>\n",
       "      <td>0.959817</td>\n",
       "      <td>1.894073</td>\n",
       "      <td>-0.329711</td>\n",
       "      <td>3.118009</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.048263</td>\n",
       "      <td>-2.891371</td>\n",
       "      <td>-0.558437</td>\n",
       "      <td>6.495055</td>\n",
       "      <td>10.367566</td>\n",
       "      <td>0.570897</td>\n",
       "      <td>2.110716</td>\n",
       "      <td>-1.246753</td>\n",
       "      <td>-8.524743</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.125513</td>\n",
       "      <td>0.428988</td>\n",
       "      <td>3.385085</td>\n",
       "      <td>1.941234</td>\n",
       "      <td>4.131640</td>\n",
       "      <td>-0.509308</td>\n",
       "      <td>7.753160</td>\n",
       "      <td>2.754631</td>\n",
       "      <td>-0.687329</td>\n",
       "      <td>...</td>\n",
       "      <td>5.209500</td>\n",
       "      <td>-3.950286</td>\n",
       "      <td>6.697100</td>\n",
       "      <td>1.225492</td>\n",
       "      <td>-7.946608</td>\n",
       "      <td>-0.790830</td>\n",
       "      <td>6.605088</td>\n",
       "      <td>-3.531295</td>\n",
       "      <td>-1.495831</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2.313937</td>\n",
       "      <td>0.054572</td>\n",
       "      <td>-2.763758</td>\n",
       "      <td>-3.833287</td>\n",
       "      <td>4.874951</td>\n",
       "      <td>-3.330217</td>\n",
       "      <td>-1.284547</td>\n",
       "      <td>0.571877</td>\n",
       "      <td>-4.491823</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.038224</td>\n",
       "      <td>6.726155</td>\n",
       "      <td>7.169745</td>\n",
       "      <td>5.381354</td>\n",
       "      <td>13.035870</td>\n",
       "      <td>-2.655374</td>\n",
       "      <td>4.574077</td>\n",
       "      <td>5.040864</td>\n",
       "      <td>-1.897521</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    timeline_index        d1         d2        d3        d4        d5  \\\n",
       "0                1  6.100298 -14.199132  8.581287 -6.761333  2.903222   \n",
       "1                2  6.067597  -7.157618  3.922382  2.873782  4.717306   \n",
       "2                3  9.407446  -8.487071  3.602750 -6.851043  7.014906   \n",
       "3                4  1.125513   0.428988  3.385085  1.941234  4.131640   \n",
       "4                5  2.313937   0.054572 -2.763758 -3.833287  4.874951   \n",
       "5                0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "6                0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "7                0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "8                0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "9                0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "10               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "11               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "12               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "13               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "14               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "15               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "16               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "17               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "18               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "19               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "20               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "21               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "22               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "23               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "24               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "25               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "26               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "27               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "28               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "29               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "30               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "31               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "32               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "33               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "34               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "35               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "36               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "37               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "38               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "39               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "40               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "41               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "42               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "43               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "44               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "45               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "46               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "47               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "48               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "49               0  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          d6        d7        d8        d9  ...       d17       d18       d19  \\\n",
       "0   8.518986 -1.497842  2.556130  1.165959  ...  3.761808 -3.724018  4.041551   \n",
       "1  -0.953772 -6.488945  0.626723 -2.640919  ... -1.538846  8.980839  6.784652   \n",
       "2   0.959817  1.894073 -0.329711  3.118009  ... -2.048263 -2.891371 -0.558437   \n",
       "3  -0.509308  7.753160  2.754631 -0.687329  ...  5.209500 -3.950286  6.697100   \n",
       "4  -3.330217 -1.284547  0.571877 -4.491823  ... -4.038224  6.726155  7.169745   \n",
       "5   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "6   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "7   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "8   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "9   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "10  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "11  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "12  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "13  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "14  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "15  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "16  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "17  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "18  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "19  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "20  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "21  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "22  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "23  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "24  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "25  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "26  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "27  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "28  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "29  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "30  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "31  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "32  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "33  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "34  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "35  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "36  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "37  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "38  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "39  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "40  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "41  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "42  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "43  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "44  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "45  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "46  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "47  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "48  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "49  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "         d20        d21       d22       d23       d24       d25  text_id  \n",
       "0   0.700114   5.538017  5.933544  1.699548 -1.927170 -4.084645        0  \n",
       "1   3.221316   9.305283 -9.060441  4.503458  8.616587  4.175737        0  \n",
       "2   6.495055  10.367566  0.570897  2.110716 -1.246753 -8.524743        0  \n",
       "3   1.225492  -7.946608 -0.790830  6.605088 -3.531295 -1.495831        0  \n",
       "4   5.381354  13.035870 -2.655374  4.574077  5.040864 -1.897521        0  \n",
       "5   0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "6   0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "7   0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "8   0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "9   0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "10  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "11  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "12  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "13  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "14  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "15  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "16  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "17  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "18  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "19  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "20  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "21  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "22  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "23  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "24  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "25  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "26  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "27  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "28  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "29  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "30  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "31  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "32  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "33  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "34  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "35  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "36  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "37  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "38  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "39  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "40  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "41  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "42  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "43  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "44  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "45  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "46  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "47  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "48  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "49  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000        0  \n",
       "\n",
       "[50 rows x 27 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# still has the labels and the ids\n",
    "english_dataset.df_padded[english_dataset.df_padded[\"text_id\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40969a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word                  knots\n",
       "language                 en\n",
       "tokens      [k, n, o, t, s]\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_train_encoder.df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07994e10",
   "metadata": {},
   "source": [
    "For the padded rows, we give these a label `-1` to denote that they have been added.\n",
    "\n",
    "Note that for padding, the method pads from below by default, but we can pad by above by setting `pad_from_below=False`.\n",
    "\n",
    "To obtain a path as a Numpy array, we use the [`nlpsig.PrepareData.get_path`](https://nlpsig.readthedocs.io/en/latest/data_preparation.html#nlpsig.data_preparation.PrepareData.get_path) method which by default keeps the time features and will remove the id and label columns. We make this more explicit by setting `include_features=True` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9f2ba04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70615, 50, 26)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_word_path = english_dataset.get_path(include_features=True)\n",
    "english_word_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb5e2f91-f171-4ced-9f93-60bf977b6b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.        ,   6.1002984 , -14.19913197, ...,   1.69954813,\n",
       "         -1.92717004,  -4.08464479],\n",
       "       [  2.        ,   6.06759691,  -7.15761757, ...,   4.50345755,\n",
       "          8.61658669,   4.17573738],\n",
       "       [  3.        ,   9.40744591,  -8.48707104, ...,   2.11071587,\n",
       "         -1.24675322,  -8.52474308],\n",
       "       ...,\n",
       "       [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_word_path[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b1ad55",
   "metadata": {},
   "source": [
    "## Obtaining a paths and signatures for words in `corpus_df`\n",
    "\n",
    "Now that we have trained our model and obtained signatures for each word in our sample of english words, we also want to obtain embeddings for the words in `corpus_sample_df`. Currently, [`nlpsig.TextEncoder`](https://nlpsig.readthedocs.io/en/latest/encode_text.html#nlpsig.encode_text.TextEncoder) only works with the data that is passed into the function and stored in the `.df` and `.dataset` attributes, so we need to initialise a new [`nlpsig.TextEncoder`](https://nlpsig.readthedocs.io/en/latest/encode_text.html#nlpsig.encode_text.TextEncoder) object with the `corpus_sample_df` dataframe and also the trained model.\n",
    "\n",
    "We can then obtain embeddings easily (recall from above we first need to tokenize the text, and then use the [`nlpsig.TextEncoder.obtain_embeddings`](https://nlpsig.readthedocs.io/en/latest/encode_text.html#nlpsig.encode_text.TextEncoder.obtain_embeddings) and [`nlpsig.TextEncoder.pool_token_embeddings`](https://nlpsig.readthedocs.io/en/latest/encode_text.html#nlpsig.encode_text.TextEncoder.pool_token_embeddings) methods to do this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "015309f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sample_encoder = nlpsig.TextEncoder(\n",
    "    df=corpus_sample_df,\n",
    "    feature_name=\"word\",\n",
    "    model=english_train_encoder.model,\n",
    "    config=english_train_encoder.config,\n",
    "    tokenizer=english_train_encoder.tokenizer,\n",
    "    data_collator=english_train_encoder.data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83327a8e-2192-4efd-ac04-81f57999d47a",
   "metadata": {},
   "source": [
    "Note that since we're just loading in our pretrained model from above, we could also just have passed in the path to the model directly via the `model_name` argument, and use the [`nlpsig.TextEncoder.load_pretrained_model`](https://nlpsig.readthedocs.io/en/latest/encode_text.html#nlpsig.encode_text.TextEncoder.load_pretrained_model) method which loads in the model, config, tokenizer and data collator that was used. However, we can also be more explicit about loading in that model using [`transformers.RobertaForMaskedLM.from_pretrained`](https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaForMaskedLM) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c762d670-5eba-42f6-8d67-99c5a3f251fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sample_encoder = nlpsig.TextEncoder(\n",
    "    df=corpus_sample_df,\n",
    "    feature_name=\"word\",\n",
    "    model_name=model_name\n",
    ")\n",
    "english_train_encoder.model = RobertaForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d0d9042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Setting return_special_tokens_mask=True\n",
      "[INFO] Tokenizing the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21aae7af430e4406a145f7d1e70eee8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving the tokenized text for each sentence into `.df['tokens']`...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bacd79629964074b4a542def03bfa15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating tokenized dataframe and setting in `.tokenized_df` attribute...\n",
      "[INFO] Note: 'text_id' is the column name for denoting the corresponding text id\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['word', 'language', 'input_ids', 'attention_mask', 'special_tokens_mask', 'tokens'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sample_encoder.tokenize_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ed8f78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198506</th>\n",
       "      <td>19999</td>\n",
       "      <td>en</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198507</th>\n",
       "      <td>19999</td>\n",
       "      <td>en</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198508</th>\n",
       "      <td>19999</td>\n",
       "      <td>en</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198509</th>\n",
       "      <td>19999</td>\n",
       "      <td>en</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198510</th>\n",
       "      <td>19999</td>\n",
       "      <td>en</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198511 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id language tokens\n",
       "0             0       de      a\n",
       "1             0       de      b\n",
       "2             0       de      b\n",
       "3             0       de      l\n",
       "4             0       de      e\n",
       "...         ...      ...    ...\n",
       "198506    19999       en      m\n",
       "198507    19999       en      u\n",
       "198508    19999       en      r\n",
       "198509    19999       en      g\n",
       "198510    19999       en      y\n",
       "\n",
       "[198511 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sample_encoder.tokenized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a089032-0f58-47e7-9922-3778da16483c",
   "metadata": {},
   "source": [
    "After tokenizing, we can obtain token embeddings and also pool these token embeddings with `[`nlpsig.TextEncoder.obtain_embeddings`](https://nlpsig.readthedocs.io/en/latest/encode_text.html#nlpsig.encode_text.TextEncoder.obtain_embeddings) and [`nlpsig.TextEncoder.pool_token_embeddings`](https://nlpsig.readthedocs.io/en/latest/encode_text.html#nlpsig.encode_text.TextEncoder.pool_token_embeddings) methods available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0334ac45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b45d74d72a24003adb687657388e8f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = corpus_sample_encoder.obtain_embeddings(method=\"hidden_layer\", layers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c1885be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198511, 768)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7838beaf",
   "metadata": {},
   "source": [
    "To reduce the embeddings, we want to use the same transform that we used earlier on. Recall that we used Gaussian random projections using the [`scikit-learn`](https://scikit-learn.org/stable/modules/random_projection.html) package. After fitting and transforming with the vectors in `english_token_embeddings`, we stored the `sklearn.random_projection.GaussianRandomProjection` object in `reduction.reducer` which we can use again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22d7cbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.random_projection.GaussianRandomProjection"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(reduction.reducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74359714",
   "metadata": {},
   "source": [
    "We can then transform new data using the `.transform()` method of the `sklearn.random_projection.GaussianRandomProjection` class which will use the same transformation that we fitted to above when applying dimension reduction to the token embeddings for our corpus of english words (in `english_train`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d27b6ad-8a26-41d5-bf5c-54bc8ada6b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_reduced = reduction.reducer.transform(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b3853cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198511, 25)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf35fb7-0d6f-4fe9-ad11-6f0919e2fe7d",
   "metadata": {},
   "source": [
    "Optionally, we can save these embeddings for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e35da37-a73b-48d4-ad62-de97f99c78db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"corpus_sample_token_embeddings.pkl\",'wb') as f:\n",
    "    pickle.dump(token_embeddings, f)\n",
    "with open(f\"corpus_sample_reduced_token_embeddings.pkl\",'wb') as f:\n",
    "    pickle.dump(embeddings_reduced, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3886dbb3",
   "metadata": {},
   "source": [
    "We again obtain paths with the `PrepareData` class, and pass in the tokenized dataframe created in `corpus_sample_encoder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b9d352f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198953</th>\n",
       "      <td>19999</td>\n",
       "      <td>en</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198954</th>\n",
       "      <td>19999</td>\n",
       "      <td>en</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198955</th>\n",
       "      <td>19999</td>\n",
       "      <td>en</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198956</th>\n",
       "      <td>19999</td>\n",
       "      <td>en</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198957</th>\n",
       "      <td>19999</td>\n",
       "      <td>en</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198958 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id language tokens\n",
       "0             0       de      a\n",
       "1             0       de      b\n",
       "2             0       de      b\n",
       "3             0       de      l\n",
       "4             0       de      e\n",
       "...         ...      ...    ...\n",
       "198953    19999       en      m\n",
       "198954    19999       en      u\n",
       "198955    19999       en      r\n",
       "198956    19999       en      g\n",
       "198957    19999       en      y\n",
       "\n",
       "[198958 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sample_encoder.tokenized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83bd0e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Note 'datetime' is not a column in `.df`, so only 'timeline_index' is added.\n",
      "[INFO] As 'datetime' is not a column in `.df`, we assume that the data is ordered by time with respect to the id.\n",
      "[INFO] Adding 'timeline_index' feature...\n"
     ]
    }
   ],
   "source": [
    "corpus_dataset = nlpsig.PrepareData(\n",
    "    original_df=corpus_sample_encoder.tokenized_df,\n",
    "    id_column=\"text_id\",\n",
    "    label_column=\"language\",\n",
    "    embeddings=token_embeddings,\n",
    "    embeddings_reduced=embeddings_reduced\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "acebfe66",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73673f90fc04984991169b0e421d8d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_word_path = corpus_dataset.pad(**path_specifics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746a5e6f",
   "metadata": {},
   "source": [
    "By inspecting the shape of `corpus_word_path`, we see that we have a path for each word and the dimension of the array is `[batch, length of path, channels]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "99428872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 50, 28)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_word_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "523b4cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_dataset.df[\"text_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caac355",
   "metadata": {},
   "source": [
    "To obtain a path as a numpy array, we use the [`nlpsig.TextEncoder.get_path`](https://nlpsig.readthedocs.io/en/latest/data_preparation.html#nlpsig.data_preparation.PrepareData.get_path) method which by default keeps the time features and will remove the id and label columns from the path that is generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be6c23db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 50, 26)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_path = corpus_dataset.get_path(include_features=True)\n",
    "word_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cbab692c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.        ,  -8.13140392,  -6.97939634, ...,  -4.73969507,\n",
       "         -0.09743738,   8.49338818],\n",
       "       [  2.        ,   4.9608326 ,  -1.95551634, ...,  -3.08118773,\n",
       "          1.4404881 ,   2.2313807 ],\n",
       "       [  3.        , -13.2059536 ,   0.93051302, ...,  -6.17853022,\n",
       "          2.53940964,  -2.99868464],\n",
       "       ...,\n",
       "       [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_path[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4936df71-ec25-4fb0-a4c5-139da617fb2e",
   "metadata": {},
   "source": [
    "We obtain the paths for our inliers and outliers by first getting the indices for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "df24ce47-638b-45ca-95fa-45d1989c4693",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_word_indices = corpus_sample_df[corpus_sample_df[\"language\"]==\"en\"].index\n",
    "non_english_word_indices = corpus_sample_df[corpus_sample_df[\"language\"]!=\"en\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6e251c6a-7046-4db2-bef4-7c6341aac635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>abate</td>\n",
       "      <td>en</td>\n",
       "      <td>[a, b, a, t, e]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>abatement</td>\n",
       "      <td>en</td>\n",
       "      <td>[a, b, a, t, e, m, e, n, t]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>abbe</td>\n",
       "      <td>en</td>\n",
       "      <td>[a, b, b, e]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>abbrevs</td>\n",
       "      <td>en</td>\n",
       "      <td>[a, b, b, r, e, v, s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>abdicated</td>\n",
       "      <td>en</td>\n",
       "      <td>[a, b, d, i, c, a, t, e, d]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>zillion</td>\n",
       "      <td>en</td>\n",
       "      <td>[z, i, l, l, i, o, n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>zincked</td>\n",
       "      <td>en</td>\n",
       "      <td>[z, i, n, c, k, e, d]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>zines</td>\n",
       "      <td>en</td>\n",
       "      <td>[z, i, n, e, s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>zingers</td>\n",
       "      <td>en</td>\n",
       "      <td>[z, i, n, g, e, r, s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>zymurgy</td>\n",
       "      <td>en</td>\n",
       "      <td>[z, y, m, u, r, g, y]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word language                       tokens\n",
       "10000      abate       en              [a, b, a, t, e]\n",
       "10001  abatement       en  [a, b, a, t, e, m, e, n, t]\n",
       "10002       abbe       en                 [a, b, b, e]\n",
       "10003    abbrevs       en        [a, b, b, r, e, v, s]\n",
       "10004  abdicated       en  [a, b, d, i, c, a, t, e, d]\n",
       "...          ...      ...                          ...\n",
       "19995    zillion       en        [z, i, l, l, i, o, n]\n",
       "19996    zincked       en        [z, i, n, c, k, e, d]\n",
       "19997      zines       en              [z, i, n, e, s]\n",
       "19998    zingers       en        [z, i, n, g, e, r, s]\n",
       "19999    zymurgy       en        [z, y, m, u, r, g, y]\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sample_df.iloc[english_word_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "343908d6-4627-443c-9086-be5405fdbdab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abblendet</td>\n",
       "      <td>de</td>\n",
       "      <td>[a, b, b, l, e, n, d, e, t]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bestechendes</td>\n",
       "      <td>de</td>\n",
       "      <td>[b, e, s, t, e, c, h, e, n, d, e, s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>narrensicheren</td>\n",
       "      <td>de</td>\n",
       "      <td>[n, a, r, r, e, n, s, i, c, h, e, r, e, n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inakzeptable</td>\n",
       "      <td>de</td>\n",
       "      <td>[i, n, a, k, z, e, p, t, a, b, l, e]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abbestelle</td>\n",
       "      <td>de</td>\n",
       "      <td>[a, b, b, e, s, t, e, l, l, e]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>stakens</td>\n",
       "      <td>sv</td>\n",
       "      <td>[s, t, a, k, e, n, s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>uppbryts</td>\n",
       "      <td>sv</td>\n",
       "      <td>[u, p, p, b, r, y, t, s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>uppeldade</td>\n",
       "      <td>sv</td>\n",
       "      <td>[u, p, p, e, l, d, a, d, e]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>mexikaner</td>\n",
       "      <td>sv</td>\n",
       "      <td>[m, e, x, i, k, a, n, e, r]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>kulsprutan</td>\n",
       "      <td>sv</td>\n",
       "      <td>[k, u, l, s, p, r, u, t, a, n]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                word language                                      tokens\n",
       "0          abblendet       de                 [a, b, b, l, e, n, d, e, t]\n",
       "1       bestechendes       de        [b, e, s, t, e, c, h, e, n, d, e, s]\n",
       "2     narrensicheren       de  [n, a, r, r, e, n, s, i, c, h, e, r, e, n]\n",
       "3       inakzeptable       de        [i, n, a, k, z, e, p, t, a, b, l, e]\n",
       "4         abbestelle       de              [a, b, b, e, s, t, e, l, l, e]\n",
       "...              ...      ...                                         ...\n",
       "9995         stakens       sv                       [s, t, a, k, e, n, s]\n",
       "9996        uppbryts       sv                    [u, p, p, b, r, y, t, s]\n",
       "9997       uppeldade       sv                 [u, p, p, e, l, d, a, d, e]\n",
       "9998       mexikaner       sv                 [m, e, x, i, k, a, n, e, r]\n",
       "9999      kulsprutan       sv              [k, u, l, s, p, r, u, t, a, n]\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sample_df.iloc[non_english_word_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a35225a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain paths for english words and non-english words in corpus_sample_df\n",
    "inlier_paths = word_path[english_word_indices]\n",
    "outlier_paths = word_path[non_english_word_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b6632eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 50, 26)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inlier_paths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8a3dcab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 50, 26)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlier_paths.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ff2c98-fbfc-442a-93d0-f2017e02ee02",
   "metadata": {},
   "source": [
    "## Obtaining path signatures for the corpus, inliers and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a6cdc5d9-b5b5-4053-b14e-8faba8ebe0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(paths: np.array, minimum: float | None = None, maximum: float | None = None) -> tuple[np.array, float, float]:\n",
    "    # apply min-max scaling to the features of the paths\n",
    "    # paths is a three dimensional tensor (batch, length, features)\n",
    "    if minimum is None and maximum is None:\n",
    "        # reshape to (batch * length, features)\n",
    "        paths_stacked = paths.reshape(-1, paths.shape[-1])\n",
    "        minimum = np.min(paths_stacked, axis=0)\n",
    "        maximum = np.max(paths_stacked, axis=0)\n",
    "\n",
    "    # compute min-max scaling\n",
    "    paths = (paths - minimum) / (maximum - minimum)\n",
    "    \n",
    "    return paths, minimum, maximum\n",
    "\n",
    "\n",
    "def normalise_data(corpus: np.array, inliers: np.array, outliers: np.array) -> tuple[np.array, np.array, np.array]:\n",
    "    corpus, minimum, maximum = normalise(corpus)\n",
    "    inliers, *_ = normalise(inliers, minimum, maximum)\n",
    "    outliers, *_ = normalise(outliers, minimum, maximum)\n",
    "\n",
    "    return corpus, inliers, outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "66a90f69-4800-4040-bcd0-49cbd65a7884",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, inliers, outliers = normalise_data(english_word_path, inlier_paths, outlier_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6661dd6a-0354-4877-8b81-87234238cae5",
   "metadata": {},
   "source": [
    "## Obtaining path signatures for the english words\n",
    "\n",
    "We use [`signax`](https://github.com/anh-tong/signax) to compute path signatures, which we compute up to depth $2$ here on paths with $26$ channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "16e89214",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_depth = 2\n",
    "corpus_sig = np.array(signature(corpus, sig_depth))\n",
    "inlier_sig = np.array(signature(inliers, sig_depth))\n",
    "outlier_sig = np.array(signature(outliers, sig_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2e9a5242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70615, 702)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c6a09d3c-65dd-487f-a290-5976127edeef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 702)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inlier_sig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "49ad0524-6613-4fb4-919a-6c476fe93fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 702)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlier_sig.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428e5c23",
   "metadata": {},
   "source": [
    "## Anomaly detection task\n",
    "\n",
    "To recap the task at hand:\n",
    "- We trained a language model using a corpus of english words stored in the `english_train` dataframe.\n",
    "- We have another set of english words (inliers) and some non-english words (outliers) which are stored in the `corpus_sample_df` dataframe.\n",
    "- We now want to see how we could detect the non-english words efficiently, in particular, we use the following method:\n",
    "    - For each word in `english_train` and `corpus_sample_df`, we have a vector representation for them (e.g. we've computed the path signatures for each of them and they are stored in `english_word_sig`).\n",
    "    - For each word in `corpus_sample_df`, we compute the minimum Mahalanobis distance between its path signature to path signatures for our corpus of known English words (i.e. each row in `english_word_sig`).\n",
    "    - We then look the [ROC curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) to see how well separated are the english words to the non-english words. For a good performance, we hope that there is good separation, and so we measure the success of this method using the [ROCAUC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html).\n",
    " \n",
    "Full details of the anomaly detection methodology can be found in [[2]](https://arxiv.org/abs/2006.03487) _Dimensionless Anomaly Detection on Multivariate Streams with Variance Norm and Path Signature_ by Zhen Shao, Ryan Sze-Yin Chan, Thomas Cochrane, Peter Foster, Terry Lyons. Implementation of this approach can be found in the [`signature_mahalanobis_knn`](https://github.com/datasig-ac-uk/signature_mahalanobis_knn) library.\n",
    "\n",
    "Note that in Section 5.2.4 of [[2]](https://arxiv.org/abs/2006.03487), we consider the same language dataset example as we do here, but use one-hot encoding vector representations for the characters. We will see our dimension reduced transformer embeddings give us a boost in performance as seen in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "173c6c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbours = 100\n",
    "n_jobs = -1\n",
    "knn_library = \"sklearn\"\n",
    "bootstrap_samples = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5481f3cd-f82e-4b1d-88bf-148b3d601ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_maha_knn = SignatureMahalanobisKNN(\n",
    "    n_jobs=n_jobs, random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e8fd1d-c4aa-4e36-8fd1-ef245dec74bc",
   "metadata": {},
   "source": [
    "We use the `SignatureMahalanobisKNN.fit` method and pass in our path signature representations for each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f95fdbd6-c116-48f1-bd0f-64f72f31bd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time: 14445.002366065979\n"
     ]
    }
   ],
   "source": [
    "# measure the time spent on fit\n",
    "start_time = time.time()\n",
    "signature_maha_knn.fit(\n",
    "    knn_library=knn_library,\n",
    "    signatures_train=corpus_sig,\n",
    ")\n",
    "fit_time = time.time() - start_time\n",
    "print(f\"fit_time: {fit_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef8dd2e-7bd1-4439-95a4-b9d8fa02d89c",
   "metadata": {},
   "source": [
    "We can then simply use the `SignatureMahalanobisKNN.conformance` method to compute the anomaly scores for both the inlier signatures and outlier signatures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "336bb8f4-27d8-46ea-bcaa-438f138caca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_score_time: 123.82245087623596\n"
     ]
    }
   ],
   "source": [
    "# measure the time spent on conformance\n",
    "start_time = time.time()\n",
    "scores_inliers = signature_maha_knn.conformance(\n",
    "    signatures_test=inlier_sig, n_neighbors=n_neighbours\n",
    ")\n",
    "scores_outliers = signature_maha_knn.conformance(\n",
    "    signatures_test=outlier_sig, n_neighbors=n_neighbours\n",
    ")\n",
    "compute_score_time = time.time() - start_time\n",
    "print(f\"compute_score_time: {compute_score_time}\")  # noqa: T201"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbac1993-e8bf-4a7c-931e-7f5ec30ceff6",
   "metadata": {},
   "source": [
    "Given the anomaly scores for the inliers and outliers, we can compute the AUC and bootstrap this score to get a standard error based on 10000 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "30c710ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk8AAAI4CAYAAABz4A0XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACq8ElEQVR4nOzdd1wT5x8H8E9YYaigDFFEAbUqLvy5N0txb3GLOKp1Va22rorWKq6qHbZWrVqt1r0Higpura2j7o3WDSpLZOZ+f6RJExJGMHCBfN6vly/vjkvyzeWSfPLcc89JBEEQQERERES5YiJ2AURERESFCcMTERERkQ4YnoiIiIh0wPBEREREpAOGJyIiIiIdMDwRERER6YDhiYiIiEgHDE9EREREOmB4IiIiItIBw5OBcHNzw6BBg8Quw+h4e3vD29tb7DJyNHPmTEgkEsTExIhdisGRSCSYOXOmXu4rKioKEokEa9eu1cv9GaLM22vt2rWQSCSIiorK9X0ottOiRYv0XyDlK77++mEU4Umxcyj+mZmZwcXFBYMGDcLTp0/FLs+gvXv3DrNnz0atWrVgbW0NW1tbNG/eHOvWrUNhubLPjRs3MHPmTJ0+HApKRkYG1qxZA29vb5QqVQpSqRRubm4IDg7Gn3/+KXZ5erFx40YsXbpU7DLUiFlT5s+jzP/OnTsnSl1FiWIbW1paav2M9/b2Ro0aNUSojK9/UWEmdgEF6auvvoK7uzuSk5Nx7tw5rF27FqdOncK1a9dgaWkpam23b9+GiYlhZdmXL1/Cz88PN2/eRO/evTF69GgkJydj+/btCAoKwoEDB7BhwwaYmpqKXWq2bty4gVmzZsHb2xtubm5qfzt8+LA4RQF4//49unXrhrCwMLRo0QJTp05FqVKlEBUVhS1btuDXX3/F48ePUa5cOdFq1IeNGzfi2rVrGDduXL7c//v372FmpttHWVY1VahQAe/fv4e5ubkeK9RO8XmUWaVKlfL9sVUNGDAAvXv3hlQqLdDHLQgpKSmYN28evv/+e7FL0cDXv3AzqvDUtm1b1KtXDwAwdOhQODg4YP78+dizZw8CAwNFrU2MHTc5ORkWFhZZhragoCDcvHkTO3fuRKdOnZTLx44di0mTJmHRokWoU6cOvvjii4IqGYC8NczGxkYv92VhYaGX+8mLSZMmISwsDEuWLNH4Eg8JCcGSJUsKtB5BEJCcnAwrK6sCfdy8kMlkSE1NhaWlpV5/+ChaKwqC6ueRmExNTQ3+B1BeeXl5YeXKlZgyZQrKli0rdjlq+PoXbobV1FHAmjdvDgC4f/++2vJbt26hR48eKFWqFCwtLVGvXj3s2bNH4/axsbEYP3483NzcIJVKUa5cOQwcOFCtX0pKSgpCQkJQqVIlSKVSuLq64vPPP0dKSorafan2efrzzz8hkUjw66+/ajzmoUOHIJFIsG/fPuWyp0+fYvDgwShdujSkUimqV6+O1atXq90uMjISEokEmzZtwvTp0+Hi4gJra2vEx8dr3Tbnzp3DoUOHMGjQILXgpBAaGorKlStj/vz5eP/+PQD14+BLlixBhQoVYGVlhZYtW+LatWsa95Gb7axo4j5+/DhGjhwJJycnZUvMo0ePMHLkSFSpUgVWVlawt7dHz5491Q7PrV27Fj179gQA+Pj4KJvGIyMjAWj2eVJspy1btmDOnDkoV64cLC0t4efnh3v37mk8h2XLlsHDwwNWVlZo0KABTp48mat+VE+ePMHPP/+MVq1aaW2RMTU1xcSJEzVanWJjYzFo0CDY2dnB1tYWwcHBSEpKUltnzZo18PX1hZOTE6RSKTw9PfHTTz9pPIabmxs6dOiAQ4cOoV69erCyssLPP/+s030AwMGDB9GyZUsUL14cJUqUQP369bFx40YA8u27f/9+PHr0SLntVVv/cvv+kEgkGD16NDZs2IDq1atDKpUiLCxM+TfVPhwJCQkYN26c8n3p5OSEVq1a4eLFiznWlFWfp1u3biEwMBCOjo6wsrJClSpVMG3aNI11Hj9+rHUb5YXq+2nFihWoWLEipFIp6tevjwsXLmisv3XrVnh6esLS0hI1atTAzp07MWjQII3W1sy09Xn5888/ERAQAAcHB1hZWcHd3R2DBw/Wevvc1KZKl8+3nF7LnEydOhUZGRmYN29ejuump6dj9uzZyufi5uaGqVOnav2s7tChA06dOoUGDRrA0tISHh4eWLduXa5qyq2i+voXFUbV8pSZYmcpWbKkctn169fRtGlTuLi4YPLkybCxscGWLVvQpUsXbN++HV27dgUAJCYmonnz5rh58yYGDx6M//3vf4iJicGePXvw5MkTODg4QCaToVOnTjh16hQ+/vhjVKtWDVevXsWSJUtw584d7Nq1S2td9erVg4eHB7Zs2YKgoCC1v23evBklS5ZEQEAAAPmhtUaNGim/XBwdHXHw4EEMGTIE8fHxGl/Ms2fPhoWFBSZOnIiUlJQsW1727t0LABg4cKDWv5uZmaFv376YNWsWTp8+DX9/f+Xf1q1bh4SEBIwaNQrJycn49ttv4evri6tXr6J06dI6bWeFkSNHwtHRETNmzMC7d+8AABcuXMCZM2fQu3dvlCtXDlFRUfjpp5/g7e2NGzduwNraGi1atMDYsWPx3XffYerUqahWrRoAKP/Pyrx582BiYoKJEyciLi4OCxYsQL9+/XD+/HnlOj/99BNGjx6N5s2bY/z48YiKikKXLl1QsmTJHA+1HTx4EOnp6RgwYEC262UWGBgId3d3hIaG4uLFi1i1ahWcnJwwf/58tbqqV6+OTp06wczMDHv37sXIkSMhk8kwatQotfu7ffs2+vTpg+HDh2PYsGGoUqWKTvexdu1aDB48GNWrV8eUKVNgZ2eHS5cuISwsDH379sW0adMQFxeHJ0+eKFvSihUrBgA6vz+OHTuGLVu2YPTo0XBwcMjyS2HEiBHYtm0bRo8eDU9PT7x+/RqnTp3CzZs38b///S/bmrT5+++/0bx5c5ibm+Pjjz+Gm5sb7t+/j71792LOnDnK9apVq4aWLVsqg3lO4uLiNE4AkEgksLe3V1u2ceNGJCQkYPjw4ZBIJFiwYAG6deuGBw8eKA8v7t+/H7169ULNmjURGhqKt2/fYsiQIXBxcclVLapevXqF1q1bw9HREZMnT4adnR2ioqKwY8cOjXVzU1tmuny+5fRa5sTd3R0DBw7EypUrMXny5Gxbn4YOHYpff/0VPXr0wGeffYbz588jNDRU2fqu6t69e+jRoweGDBmCoKAgrF69GoMGDULdunVRvXr1HOsCjPf1LzIEI7BmzRoBgHDkyBEhOjpa+Oeff4Rt27YJjo6OglQqFf755x/lun5+fkLNmjWF5ORk5TKZTCY0adJEqFy5snLZjBkzBADCjh07NB5PJpMJgiAI69evF0xMTISTJ0+q/X358uUCAOH06dPKZRUqVBCCgoKU81OmTBHMzc2FN2/eKJelpKQIdnZ2wuDBg5XLhgwZIpQpU0aIiYlRe4zevXsLtra2QlJSkiAIghARESEAEDw8PJTLstOlSxcBgPD27dss19mxY4cAQPjuu+8EQRCEhw8fCgAEKysr4cmTJ8r1zp8/LwAQxo8fr1yW2+2seO2aNWsmpKenqz2+tudx9uxZAYCwbt065bKtW7cKAISIiAiN9Vu2bCm0bNlSOa/YTtWqVRNSUlKUy7/99lsBgHD16lVBEOSvhb29vVC/fn0hLS1Nud7atWsFAGr3qc348eMFAMKlS5eyXU8hJCREAKD22guCIHTt2lWwt7dXW6ZtuwQEBAgeHh5qyypUqCAAEMLCwjTWz819xMbGCsWLFxcaNmwovH//Xm1dxXtAEAShffv2QoUKFTTuT5f3BwDBxMREuH79usb9ABBCQkKU87a2tsKoUaM01lOVVU2KfXjNmjXKZS1atBCKFy8uPHr0KMvnqKgjp9ddEP7bp7X9k0qlGrXY29urfQ7s3r1bACDs3btXuaxmzZpCuXLlhISEBOWyyMhIAYDG88y8vRT1PHz4UBAEQdi5c6cAQLhw4UKWz0GX2rTJ7edbbl5LbRTP6cKFC8L9+/cFMzMzYezYscq/t2zZUqhevbpy/vLlywIAYejQoWr3M3HiRAGAcOzYMeUyxfvmxIkTymWvXr0SpFKp8Nlnn+W6NmN+/YsCozps5+/vD0dHR7i6uqJHjx6wsbHBnj17lK0Eb968wbFjxxAYGIiEhATExMQgJiYGr1+/RkBAAO7evas8c2P79u2oXbu2RgsJIP/1AMibUatVq4aqVasq7ysmJga+vr4AgIiIiCxr7dWrF9LS0tTS/uHDhxEbG4tevXoBkPdR2b59Ozp27AhBENQeIyAgAHFxcRrN20FBQbnq05KQkAAAKF68eJbrKP6W+dBfly5d1H7xNGjQAA0bNsSBAwcA6LadFYYNG6ZxXF71eaSlpeH169eoVKkS7Ozsct2sn5Xg4GC1VjnFId4HDx4AkDdrv379GsOGDVPrrNyvXz+1lsysKLZZdttXmxEjRqjNN2/eHK9fv1Z7DVS3i+LXbcuWLfHgwQPExcWp3d7d3V35K19Vbu4jPDwcCQkJmDx5skY/IcV7IDu6vj9atmwJT0/PHO/Xzs4O58+fx7Nnz3JcNyfR0dE4ceIEBg8ejPLly6v9LfNzFAQh161OgPyQb3h4uNq/gwcPaqzXq1cvtX0q87747NkzXL16FQMHDlRrQWvZsiVq1qyZ63oU7OzsAAD79u1DWlpatuvmVFt2t8vp801Ry4e+lh4eHhgwYABWrFiB58+fa11H8dk0YcIEteWfffYZAHnLjipPT0/lcwUAR0dHVKlSJcfnrcqYX/+iwKjCk2Jn3bZtG9q1a4eYmBi1jtr37t2DIAj48ssv4ejoqPYvJCQEgLxJE5D3k8rpVNe7d+/i+vXrGvf10Ucfqd2XNrVr10bVqlWxefNm5bLNmzfDwcFB+eUSHR2N2NhYrFixQuMxgoODtT6GtrM7tFF8qStClDZZBazKlStrrPvRRx8pD5Pqsp2zq/v9+/eYMWMGXF1dIZVK4eDgAEdHR8TGxmqEBF1l/qJUfEC8ffsWgLy/FaB5ZoyZmVmOfQwAoESJEgCy3755qQuA8jCqjY0N7Ozs4OjoiKlTpwKA1vCkTW7uQ9FXMK+nfOv6/sjtvrtgwQJcu3YNrq6uaNCgAWbOnJnnD3PF7fLjtPYGDRrA399f7Z+Pj4/GenndF7NalpOWLVuie/fumDVrFhwcHNC5c2esWbNGo+9PbmrLSm4+3wD9vZbTp09Henp6ln2fHj16BBMTE43t5ezsDDs7O+U2Vsj8vAH5c1c874yMDLx48ULtX2pqqtr6xvz6FwVG1eepQYMGyrMbunTpgmbNmqFv3764ffs2ihUrBplMBgCYOHGi1l/jgG47o0wmQ82aNbF48WKtf3d1dc329r169cKcOXMQExOD4sWLY8+ePejTp4+ypUNRb//+/TX6DijUqlVLbT63Z1JVq1YNu3btwt9//40WLVpoXefvv/8GgFy1BqjKy3bWVveYMWOwZs0ajBs3Do0bN4atrS0kEgl69+6tfIy8yursE0FPY1tVrVoVAHD16lV4eXnl+nY51XX//n34+fmhatWqWLx4MVxdXWFhYYEDBw5gyZIlGttF23bV9T7yStf3R2733cDAQDRv3hw7d+7E4cOHsXDhQsyfPx87duxA27ZtP7jugpbf+2JmEokE27Ztw7lz57B3714cOnQIgwcPxjfffINz586ptW58SG05fb4B+nstPTw80L9/f6xYsQKTJ0/O9rnnRk7P+59//tEI+xEREXkakLeovv6FnVGFJ1WmpqYIDQ2Fj48PfvjhB0yePBkeHh4AAHNzc7UO0NpUrFhR6xlkmde5cuUK/Pz8cv2mVNWrVy/MmjUL27dvR+nSpREfH4/evXsr/+7o6IjixYsjIyMjx3p11aFDB4SGhmLdunVaw1NGRgY2btyIkiVLomnTpmp/u3v3rsb6d+7cUbbI6LKds7Nt2zYEBQXhm2++US5LTk5GbGys2np52fY5qVChAgB5K5rqr8X09HRERUVphNbM2rZtC1NTU/z22286dxrPzt69e5GSkoI9e/ao/SrM7hBxXu+jYsWKAIBr165l+6Miq+3/oe+P7JQpUwYjR47EyJEj8erVK/zvf//DnDlzlF+4uX08xb6a03tdTKr7YmbaluVWo0aN0KhRI8yZMwcbN25Ev379sGnTJgwdOjTP96kqp883hZxey9yaPn06fvvtN7WTKxQqVKgAmUyGu3fvqp1M8vLlS8TGxiq3cW45OzsjPDxcbVnt2rV1uo/cKqyvf2FnVIftMvP29kaDBg2wdOlSJCcnw8nJCd7e3vj555+1HhuPjo5WTnfv3h1XrlzROAsD+C91BwYG4unTp1i5cqXGOu/fv1eeNZaVatWqoWbNmti8eTM2b96MMmXKqAUZU1NTdO/eHdu3b9f64a5ar66aNGkCf39/rFmzRm1YBIVp06bhzp07+PzzzzVaBHbt2qXWZ+mPP/7A+fPnlR92umzn7Jiammr8wvn++++RkZGhtkwxJlTmUPUh6tWrB3t7e6xcuRLp6enK5Rs2bMhVk7WrqyuGDRuGw4cPax3ATyaT4ZtvvsGTJ090qkvxS1B1u8TFxWHNmjV6v4/WrVujePHiCA0NRXJystrfVG9rY2Oj9TDqh74/tMnIyNB4LCcnJ5QtW1btsENWNWXm6OiIFi1aYPXq1RrDEGTe9/Q9VEFulS1bFjVq1MC6deuQmJioXH78+HFcvXpV5/t7+/atxnNTtI5qO3STVzl9vuX2tcytihUron///vj555/x4sULtb+1a9cOADRGnVe0irZv316nx7K0tNQ4JJebvpB5UVhf/8LOaFueFCZNmoSePXti7dq1GDFiBJYtW4ZmzZqhZs2aGDZsGDw8PPDy5UucPXsWT548wZUrV5S327ZtG3r27InBgwejbt26ePPmDfbs2YPly5ejdu3aGDBgALZs2YIRI0YgIiICTZs2RUZGBm7duoUtW7Yox9fJTq9evTBjxgxYWlpiyJAhGgNazps3DxEREWjYsCGGDRsGT09PvHnzBhcvXsSRI0fw5s2bPG+bdevWwc/PD507d0bfvn3RvHlzpKSkYMeOHYiMjESvXr0wadIkjdtVqlQJzZo1wyeffIKUlBQsXboU9vb2+Pzzz5Xr5HY7Z6dDhw5Yv349bG1t4enpibNnz+LIkSMap/p6eXnB1NQU8+fPR1xcHKRSqXIMo7yysLDAzJkzMWbMGPj6+iIwMBBRUVFYu3YtKlasmKuWjW+++Qb379/H2LFjsWPHDnTo0AElS5bE48ePsXXrVty6dUvrL/HstG7dGhYWFujYsSOGDx+OxMRErFy5Ek5OTll2ls3rfZQoUQJLlizB0KFDUb9+ffTt2xclS5bElStXkJSUpBzHp27duti8eTMmTJiA+vXro1ixYujYsaNe3h+ZJSQkoFy5cujRowdq166NYsWK4ciRI7hw4YJaC2VWNWnz3XffoVmzZvjf//6Hjz/+GO7u7oiKisL+/ftx+fJl5Xq6DlVw8OBB3Lp1S2N5kyZNlC1euTV37lx07twZTZs2RXBwMN6+fYsffvgBNWrUUPtCzY1ff/0VP/74I7p27YqKFSsiISEBK1euRIkSJZQhQ1+y+3zL7Wupi2nTpmH9+vW4ffu22pACtWvXRlBQEFasWIHY2Fi0bNkSf/zxB3799Vd06dJFa1+kD8XXv5Ar2JP7xKF62mpmGRkZQsWKFYWKFSsqT4W/f/++MHDgQMHZ2VkwNzcXXFxchA4dOgjbtm1Tu+3r16+F0aNHCy4uLoKFhYVQrlw5ISgoSG3YgNTUVGH+/PlC9erVBalUKpQsWVKoW7euMGvWLCEuLk65XuahChTu3r2rPI311KlTWp/fy5cvhVGjRgmurq6Cubm54OzsLPj5+QkrVqxQrqM4BX/r1q06bbuEhARh5syZQvXq1QUrKyuhePHiQtOmTYW1a9dqnKqtOH114cKFwjfffCO4uroKUqlUaN68uXDlyhWN+87Nds7utXv79q0QHBwsODg4CMWKFRMCAgKEW7duad2WK1euFDw8PARTU1O1YQuyGqog83bSdgq7IAjCd999J1SoUEGQSqVCgwYNhNOnTwt169YV2rRpk4utKwjp6enCqlWrhObNmwu2traCubm5UKFCBSE4OFhtGAPFUAXR0dFqt898mrEgCMKePXuEWrVqCZaWloKbm5swf/58YfXq1RrrVahQQWjfvr3WunJ7H4p1mzRpIlhZWQklSpQQGjRoIPz+++/KvycmJgp9+/YV7OzsNE6dzu37A0CWp6xD5dTrlJQUYdKkSULt2rWF4sWLCzY2NkLt2rWFH3/8Ue02WdWU1et87do1oWvXroKdnZ1gaWkpVKlSRfjyyy816vjQoQpUH1v1/ZTdc1bYtGmTULVqVUEqlQo1atQQ9uzZI3Tv3l2oWrVqtrfNvA9dvHhR6NOnj1C+fHlBKpUKTk5OQocOHYQ///xTeRtda8tKdp9vuX0ttcnucyMoKEgAoDZUgSAIQlpamjBr1izB3d1dMDc3F1xdXYUpU6aoDaciCFm/bzJ/luRUG1//wk0iCEbQs4sKRFRUFNzd3bFw4UJMnDhR7HJEIZPJ4OjoiG7dumk9HEVUkLy8vODo6KjR/4aMA1///GPUfZ6IPkRycrJG34B169bhzZs3eTqrhiiv0tLS1PreAfJLDV25coX7ohHg61/wjL7PE1FenTt3DuPHj0fPnj1hb2+Pixcv4pdffkGNGjWU19MjKghPnz6Fv78/+vfvj7Jly+LWrVtYvnw5nJ2dNQZWpaKHr3/BY3giyiM3Nze4urriu+++w5s3b1CqVCkMHDgQ8+bNy/KagUT5oWTJkqhbty5WrVqF6Oho2NjYoH379pg3b57GCRRU9PD1L3js80RERESkA/Z5IiIiItIBwxMRERGRDvIUnhYsWICqVavq7RpXmXl7exvcGQISiQQzZ84U5bEjIyOV1xsqrKKioiCRSLB27VqxSzE6YWFh8PLygqWlJSQSic4jrc+cOVNj0E83NzcMGjRIf0WKQNvzMiRF9T3zofsjZc3NzQ0dOnTI98fRZd8cNGiQxsXSxfw+VdW7d28EBgbm6bY6h6f4+HjMnz8fX3zxhcZo1/RhNm7cqHF5gIIgk8mwdu1adOrUCa6urrCxsUGNGjXw9ddfa1x2o6g4c+YMZs6cWeQ/uF+/fo3AwEBYWVlh2bJlWL9+vfJyNVQ03LhxAzNnzkRUVJTYpeSI+yMZki+++ALbt2/P1RUtMtP5bLvVq1cjPT0dffr00fnBcuvw4cP5dt+GbOPGjbh27RrGjRtXoI+blJSE4OBgNGrUCCNGjICTkxPOnj2LkJAQHD16FMeOHfvgX+gVKlTA+/fvYW5urqeqP8yZM2cwa9YsDBo0CHZ2dmKXk28uXLiAhIQEzJ49W68Xj759+zZ/PBmIGzduYNasWfD29tb4hW9o8mt/pMLl/fv3MDMT/2T/OnXqoF69evjmm2+wbt06nW6rc/Vr1qxBp06dYGlpqetNc42neRcsCwsLnD59Gk2aNFEuGzZsGNzc3JQB6kM/6CQSSb7uM4bi3bt3BvVL+tWrVwCg94AolUr1en+6SE5OhoWFBcNbHgiCgOTkZI2LeReU/NgfDeE9Zwg1FCaG9F0QGBiIkJAQ/PjjjyhWrFiub6fTp8/Dhw/x999/a3yRKvrkZL4gprbjoi9evEBwcDDKlSsHqVSKMmXKoHPnzmpNzpn7PCnuf8uWLZgzZw7KlSsHS0tL+Pn54d69exp1Llu2DB4eHrCyskKDBg1w8uTJXPejSklJwfjx4+Ho6IjixYujU6dOWV7Z/unTpxg8eDBKly4NqVSK6tWrY/Xq1Vq3zebNmzF16lQ4OzvDxsYGnTp1wj///KP2nPfv349Hjx5BIpFAIpFo/IqUyWS5ev66srCwUAtOCl27dgUA3Lx5M8f7CA8PR7NmzWBnZ4dixYqhSpUqmDp1qvLvWR0j37p1Kzw9PWFpaYkaNWpg586dGsfIFbddtGgRVqxYgYoVK0IqlaJ+/fq4cOGC2v39/fffGDRoEDw8PGBpaQlnZ2cMHjwYr1+/Vq4zc+ZM5QWN3d3dlds7Kioq22P5mY/TK/rM3LhxQ3lR3GbNmin//ttvv6Fu3bqwsrJCqVKl0Lt3b7XXHADu3r2L7t27w9nZGZaWlihXrhx69+6tcTV5bbZu3aq8fwcHB/Tv3x9Pnz5V/t3b2xtBQUEAgPr160MikeTYT+nUqVOoX78+LC0tUbFiRfz8889a18vc52nt2rWQSCQ4deoUxo4dC0dHR9jZ2WH48OFITU1FbGwsBg4ciJIlS6JkyZL4/PPPNUZn10bx/tm0aROmT58OFxcXWFtbIz4+HgBw/vx5tGnTBra2trC2tkbLli1x+vTpPD0vXV57QP7+HzJkCMqWLQupVAp3d3d88sknSE1NVa4TGxuLcePGwdXVFVKpFJUqVcL8+fM1+ovGxsZi0KBBsLW1hZ2dHYKCgnJ1SHnt2rXKAVl9fHyU+7Lis1jRB0ZxkWUrKyvlc1+zZo3yAtlSqRSenp746aefNB5DcR+nTp1CgwYNYGlpCQ8PD41f6mlpaZg1axYqV64MS0tL2Nvbo1mzZspLg+S0P+a0PwPy/jPFihXD/fv30a5dOxQvXhz9+vVTvkajR49WfqZYWVmhcePGuHr1KgDg559/RqVKlWBpaQlvb2+thzlzsz/l9L7XJjf7gernnOI7zNraGq1bt8Y///wDQRAwe/ZslCtXDlZWVujcuXOWF34/fPiwsl+Zp6cnduzYkaeaFOvldt/ctWsXatSoofZ5rk1Wn6X37t1THg2wtbVFcHAwkpKS1G77/v17jB07Fg4ODsrv6KdPn2rcZ0JCAsaNGwc3NzdIpVI4OTmhVatWuHjxotr9tWrVCu/evdP5EjY6tTydOXMGAPC///1PpwdR1b17d1y/fh1jxoyBm5sbXr16hfDwcDx+/DjHJud58+bBxMQEEydORFxcHBYsWIB+/frh/PnzynV++uknjB49Gs2bN8f48eMRFRWFLl26oGTJkihXrlyO9Q0dOhS//fYb+vbtiyZNmuDYsWNo3769xnovX75Eo0aNlG9YR0dHHDx4EEOGDEF8fLzGobc5c+ZAIpHgiy++wKtXr7B06VL4+/vj8uXLsLKywrRp0xAXF4cnT55gyZIlAKCRgnPz/JOSkjR2Nm1MTU1RsmTJbNd58eIFAMDBwSHb9a5fv44OHTqgVq1a+OqrryCVSnHv3j2tX2Kq9u/fj169eqFmzZoIDQ3F27dvMWTIELi4uGhdf+PGjUhISMDw4cMhkUiwYMECdOvWDQ8ePFAeDgwPD8eDBw8QHBwMZ2dnXL9+HStWrMD169dx7tw5SCQSdOvWDXfu3MHvv/+OJUuWKJ+fo6MjoqOjs61Zm549e6Jy5cqYO3euMhDMmTMHX375JQIDAzF06FBER0fj+++/R4sWLXDp0iXY2dkhNTUVAQEBSElJwZgxY+Ds7IynT59i3759iI2Nha2tbZaPuXbtWgQHB6N+/foIDQ3Fy5cv8e233+L06dPK+582bRqqVKmCFStW4KuvvoK7uzsqVqyY5X1evXoVrVu3hqOjI2bOnIn09HSEhISgdOnSud4Wiucxa9YsnDt3DitWrICdnR3OnDmD8uXLY+7cuThw4AAWLlyIGjVqYODAgbm639mzZ8PCwgITJ05ESkoKLCwscOzYMbRt2xZ169ZFSEgITExMlIHg5MmTaNCggd6eV2bPnj1DgwYNEBsbi48//hhVq1bF06dPsW3bNiQlJcHCwgJJSUlo2bIlnj59iuHDh6N8+fI4c+YMpkyZgufPnyv7NwqCgM6dO+PUqVMYMWIEqlWrhp07dyqDRnZatGiBsWPH4rvvvsPUqVNRrVo1AFD+D8gPsfbp0wfDhw/HsGHDUKVKFQDyz8rq1aujU6dOMDMzw969ezFy5EjIZDKMGjVK7XHu3buHHj16YMiQIQgKCsLq1asxaNAg1K1bF9WrVwcg/wIMDQ3F0KFD0aBBA8THx+PPP//ExYsX0apVq2z3x9zszwrp6ekICAhAs2bNsGjRIlhbWyv/dvLkSezZs0dZf2hoKDp06IDPP/8cP/74I0aOHIm3b99iwYIFGDx4MI4dO6a8bW73JwVt73ttcrsfKGzYsAGpqakYM2YM3rx5gwULFiAwMBC+vr6IjIzEF198gXv37uH777/HxIkTNX6w3717F7169cKIESMQFBSENWvWoGfPnggLC0OrVq10qkmXffPw4cPo3r07PD09ERoaitevXysbSnIrMDAQ7u7uCA0NxcWLF7Fq1So4OTlh/vz5ynUGDRqELVu2YMCAAWjUqBGOHz+u9Tt6xIgR2LZtG0aPHg1PT0+8fv0ap06dws2bN9UyjCJonz59WtlgkCu6XEV4+vTpAgAhISFBbbniSvSKK9UrZL5C+du3b7O8ErOqrK50X61aNSElJUW5/NtvvxUACFevXhUEQX4Vbnt7e6F+/fpCWlqacr21a9fm6ornly9fFgAII0eOVFvet29fjStFDxkyRChTpowQExOjtm7v3r0FW1tbISkpSa12FxcXIT4+Xrneli1bBADCt99+q1zWvn17tSvO6/r8BUEQQkJCsr1it+KftsfJzN/fXyhRooTw9u3bbNdbsmSJAECIjo7Och1tV6uvWbOmUK5cObX9KTIyUqM+xW3t7e2FN2/eKJfv3r1bACDs3btXuUyx3VX9/vvvAgDhxIkTymULFy5Uu5J4dnUqZN4HFNu6T58+autFRUUJpqamwpw5c9SWX716VTAzM1Muv3TpkgBA2Lp1q8ZjZSc1NVVwcnISatSoIbx//165fN++fQIAYcaMGcpl2V1dPrMuXboIlpaWwqNHj5TLbty4IZiamgqZPyoqVKggBAUFaTxOQECAIJPJlMsbN24sSCQSYcSIEcpl6enpQrly5XJ1BXrFvu/h4aH22spkMqFy5coaj5eUlCS4u7sLrVq10vl56fLaDxw4UDAxMdG6XRX1zJ49W7CxsRHu3Lmj9vfJkycLpqamwuPHjwVBEIRdu3YJAIQFCxYo10lPTxeaN2+eZT2qtm7dqvXzVxDkrxMAISwsTONv2t4rAQEBgoeHh9b7UH3/vHr1SpBKpcJnn32mXFa7dm2hffv22daqbX/UZX8OCgoSAAiTJ0/WuG8AglQqVXtP//zzzwIAwdnZWe3zd8qUKWrvf132p6ze91nJ7X6g2P8cHR2F2NhYjVpr166t9r3Wp08fwcLCQkhOTlYuU7xW27dvVy6Li4sTypQpI9SpU0fnmnTZN728vIQyZcqo1X748GGt3zdZfZYOHjxYbb2uXbsK9vb2yvm//vpLACCMGzdObb1BgwZp3Ketra0watQoITc++ugjoW3btrlaV0Gnw3avX7+GmZmZTscFVVlZWcHCwgKRkZF4+/atzrcPDg5W6w/VvHlzAMCDBw8AAH/++Sdev36NYcOGqXVG69evX46tLABw4MABAMDYsWPVlmduRRIEAdu3b0fHjh0hCAJiYmKU/wICAhAXF6fRNDhw4EAUL15cOd+jRw+UKVNG+Zi5kdPzVzxOeHh4jv82bNiQ7WPNnTsXR44cwbx583Lsn6D4++7du3M9fMWzZ89w9epVDBw4UG1/atmyJWrWrKn1Nr169VJ7HbU9f9W+HMnJyYiJiUGjRo0AQOM10ZfM147asWMHZDIZAgMD1fYNZ2dnVK5cGREREQCgbFk6dOhQrloLFf7880+8evUKI0eOVOs70L59e1StWhX79+/X+TlkZGTg0KFD6NKlC8qXL69cXq1aNQQEBOT6foYMGaJ2ckHDhg0hCAKGDBmiXGZqaop69eqpvW45CQoKUnttL1++jLt376Jv3754/fq1chu/e/cOfn5+OHHiBGQymd6elyqZTIZdu3ahY8eOqFevnsbfFc9/69ataN68OUqWLKm2H/j7+yMjIwMnTpwAIP/cMTMzwyeffKK8D1NTU4wZMyZP9WXm7u6u9bmqbs+4uDjExMSgZcuWePDggcZhY09PT+X7DZC30lapUkXtNbSzs8P169dx9+5dnerLy/6suq1U+fn5qR3BaNiwIQD5EQ/Vz1/FckX9ud2fVOX2mnG53Q8UevbsqdbqrKi1f//+at9rDRs2RGpqqsahzbJly6q1oJQoUQIDBw7EpUuXlEcT9L1vPn/+HJcvX0ZQUJBa7a1atYKnp2euthOguU2bN2+O169fKw/Th4WFAQBGjhyptp6294qdnR3Onz+PZ8+e5fi4iu2giwLt7i6VSjF//nx89tlnKF26NBo1aoQOHTpg4MCBcHZ2zvH2qh9+AJRfpIog9ujRIwBApUqV1NYzMzPL1Vkojx49gomJicahDUUzt0J0dDRiY2OxYsUKrFixQut9KTpGKlSuXFltXiKRoFKlSjqdXpzT8wcADw8PeHh45Po+tdm8eTOmT5+OIUOGZPkhpapXr15YtWoVhg4dismTJ8PPzw/dunVDjx49suzUm9VrpVimLejk5vm/efMGs2bNwqZNmzReg9z0I8oLd3d3tfm7d+9CEASN11xBcYjR3d0dEyZMwOLFi7FhwwY0b94cnTp1Qv/+/bM9ZKfYdpn3SwCoWrUqTp06pfNziI6Oxvv377XWXKVKlVyH/MyvkeJ5uLq6aixXfd2io6ORkZGhnC9WrJhaqNa2jQFke2grLi4OKSkpenleqqKjoxEfH48aNWpku97du3fx999/w9HRUevfFfvno0ePUKZMGY0fpdpe37zIvO0UTp8+jZCQEJw9e1YjvMfFxantg5lfV0D+/lN9Db/66it07twZH330EWrUqIE2bdpgwIABqFWrVrb16bo/m5mZZXkoSJf9D/jvsyO3+5Pqj7estmtmud0PFPL6HBQqVaqkcXb0Rx99BEDer8rZ2Vnv+6biNczqfZbbH67ZfcaXKFFC+R2dedtr+x5ZsGABgoKC4Orqirp166Jdu3YYOHCg1u9HQRB0PqNcp/Bkb2+P9PR0JCQkqKX4rB5U9cNQYdy4cejYsSN27dqFQ4cO4csvv0RoaCiOHTuGOnXqZPv4pqamWpcLBXx5PsUvkP79+2f5ZsvpAyMvcvP8ExMTkZiYmKv70vbGCQ8Px8CBA9G+fXssX748V3VZWVnhxIkTiIiIwP79+xEWFobNmzfD19cXhw8fzrJuXeXm+QcGBuLMmTOYNGkSvLy8UKxYMchkMrRp0yZXrWK67MsKmc9ckslkkEgkOHjwoNaaVT+IvvnmGwwaNAi7d+/G4cOHMXbsWISGhuLcuXM69RUwFFm9RtqWq75u9evXV34AA0BISIha509t2xgAFi5cCC8vL62PWaxYMaSkpOS29Dy99tmRyWRo1aoVPv/8c61/V3yh5TdtZ9bdv38ffn5+qFq1KhYvXgxXV1dYWFjgwIEDWLJkicZ7JTfvvRYtWuD+/fvKfXnVqlVYsmQJli9fjqFDh+rt+Uil0ix/lOmy/wH/1Z/b/UlVbs9Y1HU/yOtz0IWh7JuZ6fM5BgYGonnz5ti5cycOHz6MhQsXYv78+dixYwfatm2rtu7bt2+z/LGbFZ3CU9WqVQHIz7pTDQeKdJi5B77qh6GqihUr4rPPPsNnn32Gu3fvwsvLC9988w1+++03XcrRUKFCBQDyzo0+Pj7K5enp6YiKisox0FSoUAEymQz3799XS9a3b99WW09xJl5GRkauT+HP3JQtCALu3bunVpM+RjtetGgRZs2aleN6FSpU0Gj1On/+PLp27Yp69ephy5YtOo3DYWJiAj8/P/j5+WHx4sWYO3cupk2bhoiICK3bSPW1yiyvZxC+ffsWR48exaxZszBjxgzlcm2HEbLa1rruy9pUrFgRgiDA3d09Vx9CNWvWRM2aNTF9+nScOXMGTZs2xfLly/H1119rXV+x7W7fvg1fX1+1v92+fVv5d104OjrCyspK67bKvP/nhw0bNuD9+/fK+ZxaTxWtwyVKlMj2PajL88rta+/o6IgSJUrg2rVrOdaYmJiY42dEhQoVcPToUSQmJqp9Qed2u+flc2Pv3r1ISUnBnj171H7tKw4p51WpUqUQHByM4OBgJCYmokWLFpg5c2a24Sk/9mdd5XZ/yut952Y/0Jd79+5ptKTcuXMHAJRHYPS9bypeo/z+/FB8Rz98+FAt7GT1nVGmTBmMHDkSI0eOxKtXr/C///0Pc+bMUQtP6enp+Oeff9CpUyedatGpz1Pjxo0ByI9Rq6pQoQJMTU01jt3++OOPavNJSUkaI1ZXrFgRxYsX1+kXYlbq1asHe3t7rFy5Eunp6crlGzZsyFUfK8UG/e6779SWZz4bwtTUFN27d8f27du1foBqO2Nr3bp1SEhIUM5v27YNz58/V3sRbWxsPvjQUl77PN28eRPt27eHm5sb9u3bp9M4MNpOl1X8esvqdS1btixq1KiBdevWqbWUHT9+XHlqsa4Uv1oy/0rRNmq7YkyWzF+UJUqUgIODQ477cna6desGU1NTzJo1S6MWQRCUwybEx8er7aeAPEiZmJhk+36oV68enJycsHz5crX1Dh48qHwddWVqaoqAgADs2rULjx8/Vi6/efMmDh06pPP96app06bw9/dX/sspPNWtWxcVK1bEokWLtLa0Kt6Dujyv3L72JiYm6NKlC/bu3avxWQj8t/8FBgbi7NmzWrdfbGys8rVv164d0tPT1YYJyMjIwPfff5/tNlDIal/Ojrb3SlxcHNasWZPr+8hMdTgQQN5SU6lSpRw/2/Njf9ZVbvenvMjtfqAvz549UxsiID4+HuvWrYOXl5eye4y+980yZcrAy8sLv/76q9p3WHh4OG7cuKG356bou5f5PZm5noyMDI3vUicnJ5QtW1Zjf7xx4waSk5O1DteTHZ1anjw8PFCjRg0cOXIEgwcPVi63tbVFz5498f3330MikaBixYrYt2+fxrHcO3fuwM/PD4GBgfD09ISZmRl27tyJly9fonfv3joVro2FhQVmzpyJMWPGwNfXF4GBgYiKisLatWtRsWLFHH+heXl5oU+fPvjxxx8RFxeHJk2a4OjRo1pT7bx58xAREYGGDRti2LBh8PT0xJs3b3Dx4kUcOXJEI1CUKlUKzZo1Q3BwMF6+fImlS5eiUqVKGDZsmHKdunXrYvPmzZgwYQLq16+PYsWKoWPHjjptg7z0eUpISEBAQADevn2LSZMmaXTQrFixojI4a/PVV1/hxIkTaN++PSpUqIBXr17hxx9/RLly5bId/2Tu3Lno3LkzmjZtiuDgYLx9+xY//PADatSokatDj5mVKFECLVq0wIIFC5CWlgYXFxccPnwYDx8+1Fi3bt26AIBp06ahd+/eMDc3R8eOHWFjY4OhQ4di3rx5GDp0KOrVq4cTJ04of7nlRsWKFfH1119jypQpyqEyihcvjocPH2Lnzp34+OOPMXHiRBw7dgyjR49Gz5498dFHHyE9PR3r169XhvOsmJubY/78+QgODkbLli3Rp08f5andbm5uGD9+vM7bDgBmzZqFsLAwNG/eHCNHjkR6ejq+//57VK9eHX///Xee7jO/mJiYYNWqVWjbti2qV6+O4OBguLi44OnTp4iIiECJEiWwd+9eALo9r9y+9nPnzsXhw4fRsmVLfPzxx6hWrRqeP3+OrVu34tSpU7Czs8OkSZOwZ88edOjQQXla/7t373D16lVs27YNUVFRcHBwQMeOHdG0aVNMnjwZUVFRynF5cvtDysvLC6amppg/fz7i4uIglUqV4zdlpXXr1rCwsEDHjh0xfPhwJCYmYuXKlXBycsLz5891eCX+4+npCW9vb9StWxelSpXCn3/+qTxVPDv5tT/rQpf9SVe53Q/05aOPPsKQIUNw4cIFlC5dGqtXr8bLly/VgnF+7JuhoaFo3749mjVrhsGDB+PNmzfK91lePs+1qVu3Lrp3746lS5fi9evXyqEKFO9RxXd8QkICypUrhx49eqB27dooVqwYjhw5ggsXLuCbb75Ru8/w8HBYW1srh3HINZ3OzRMEYfHixUKxYsU0TnONjo4WunfvLlhbWwslS5YUhg8fLly7dk3tdMaYmBhh1KhRQtWqVQUbGxvB1tZWaNiwobBlyxa1+8pqqILMp3RndWrxd999J1SoUEGQSqVCgwYNhNOnTwt169YV2rRpk+Pze//+vTB27FjB3t5esLGxETp27Cj8888/GqdBCoIgvHz5Uhg1apTg6uoqmJubC87OzoKfn5+wYsUKjdp///13YcqUKYKTk5NgZWUltG/fXu3UaUEQhMTERKFv376CnZ2d2umduj5/XSnuJ6t/qqeka3P06FGhc+fOQtmyZQULCwuhbNmyQp8+fdROg82q1k2bNglVq1YVpFKpUKNGDWHPnj1C9+7dhapVq2rcVtsQF5lflydPnghdu3YV7OzsBFtbW6Fnz57Cs2fPtL5+s2fPFlxcXAQTExO105aTkpKEIUOGCLa2tkLx4sWFwMBA4dWrV1meXpvVEA3bt28XmjVrJtjY2Ag2NjZC1apVhVGjRgm3b98WBEEQHjx4IAwePFioWLGiYGlpKZQqVUrw8fERjhw5ku32Vti8ebNQp04dQSqVCqVKlRL69esnPHnyRG0dXYYqEARBOH78uFC3bl3BwsJC8PDwEJYvX658nqqyGqog8+NktY2CgoIEGxubHOvJat9XuHTpktCtWzfB3t5ekEqlQoUKFYTAwEDh6NGjeXpeuX3tBUEQHj16JAwcOFBwdHQUpFKp4OHhIYwaNUptOJGEhARhypQpQqVKlQQLCwvBwcFBaNKkibBo0SIhNTVVud7r16+FAQMGCCVKlBBsbW2FAQMGKIeyyM37e+XKlYKHh4dy+AXFsAUVKlTIcviAPXv2CLVq1RIsLS0FNzc3Yf78+cLq1as1hvDI6j4yf05//fXXQoMGDQQ7OzvByspKqFq1qjBnzhy155nd/pib/Tm7/QaAxqnpWX12ZLVf5WZ/yul9r01u9gNda9W2LRWv1aFDh4RatWoJUqlUqFq1qtb3T37sm9u3bxeqVasmSKVSwdPTU9ixY4cQFBSU66EKMm9TxXNU3R/fvXsnjBo1SihVqpRQrFgxoUuXLsLt27cFAMK8efMEQZAPWzRp0iShdu3aQvHixQUbGxuhdu3awo8//qixHRo2bCj0799fY3lOJP8+kVyLi4uDh4cHFixYoHb6sSGTyWRwdHREt27dsHLlygJ97MjISPj4+GDr1q3o0aNHgT52YeXl5QVHR0edR3wlIiLjc/nyZdSpUwe//fabcsT53N7uf//7Hy5evJjliQJZ0fniULa2tvj888+xcOHCXI/pU5CSk5M1+pmsW7cOb968ydXlWajgpKWlaRzvj4yMxJUrV/haERGRBtUTSxSWLl0KExMTtGjRQqf7mjdvHnr06KFzcAIAnVueDF1kZCTGjx+Pnj17wt7eHhcvXsQvv/yCatWq4a+//irwiw6z5SlrUVFR8Pf3R//+/VG2bFncunULy5cvh62tLa5duwZ7e3uxSyQiIgMya9Ys/PXXX/Dx8YGZmRkOHjyIgwcP4uOPP87yWpz5oUAHySwIbm5ucHV1xXfffYc3b96gVKlSGDhwIObNm1fgwYmyV7JkSdStWxerVq1CdHQ0bGxs0L59e8ybN4/BiYiINDRp0gTh4eGYPXs2EhMTUb58ecycORPTpk0r0DoMouXpxIkTWLhwIf766y88f/4cO3fuRJcuXbK9TWRkJCZMmIDr16/D1dUV06dPz/GK8UREREQfSuc+T/nh3bt3qF27NpYtW5ar9R8+fIj27dvDx8cHly9fxrhx4zB06NACGY+GiIiIjJtBtDypkkgkObY8ffHFF9i/f7/aAJW9e/dGbGys8sKBRERERPmhUPZ5Onv2rMaw8gEBARg3blyWt0lJSVEbWVQmk+HNmzewt7fXy2VRiIiIjIUgCEhISEDZsmWzvNZgUVYow9OLFy9QunRptWWlS5dGfHw83r9/r/XSIqGhobm65hsRERHlzj///FMoL2L+oQpleMqLKVOmYMKECcr5uLg4lC9fHnfu3EGpUqVErMx4pKWlISIiAj4+PjA3Nxe7HKNQ6LZ5CoC4f/9/C0hSJMBbANEATAGkApLbEqA4AAmA9H//ZQCSvyWALYB3gOSSBMJHgnx5ugRIByQ3JBAgAA7y9ZW3S2LLc1EmQJDvK9r+IYvlEgApgCRNAsFFkPcOluC//1X/Zf6bCYAHAIpDvg+a4L9/qrdRXRYP+X5f/b9lgkTQ/jjZ/cN//0seSSDUFrKuUct9Cib/ri/I3xfK524KZCADo1eOxpYTW/59GPn7qXjx4nl6XQq7QhmenJ2d8fLlS7VlL1++RIkSJbK8oK1UKoVUKtVYXqpUKZ4WX0DS0tJgbW0Ne3v7wvFFXgTobZsLkIeNNPwXWN4DeAe1IKL8lwbgJABLAK8A3ABwHUAVAKn//rsIoBgA2b/ra15f+sNcyGJ5jJ4fR19M//1nBsAKgBT/fcGaapmWQL5NmwIwz/Q31f+vAGgEeeBU3L8Z5K/pIwD/Q+7CRG7+Zb69BQDrf5+Lucrjlv13WW6/2FWXmf17nyqPk5aehgMHD6Bdu3YwtzBXr4c+WEZGBgYNGqQMTmZmZli5ciWCg4ONtttLoQxPjRs3xoEDB9SWhYeHZ3vxWiKjkwIgATCPNwdeQB5QEvBfePkH8i+j1wD+gDwM2QC4B+AxAMXvE80BffMu8zW2E/R431kxw39f3MmQb4dK+C9EmEEeKv4C0BzybWCa6Z8M8taElpAHAkAewmqp3Ie5/Pmku6TjwrULqN+kPswszOQBsdi/fzf/9/5KZrp/4+syon/clvkiIyMDQUFB2LBhAwB5cNqyZYvOo3kXNQYRnhITE3Hv3n+fqg8fPsTly5dRqlQplC9fHlOmTMHTp0+xbt06AMCIESPwww8/4PPPP8fgwYNx7NgxbNmyBfv37xfrKRAVjCQAiZAHo3gAtwBchvydHA7gOeRh57l8dXOYox3aiVCojipBHkrMID9M1wjylolbAFpD/rxLQd5iYfHvfAX8F0YUAcYEQGnIW25K/fu3Av5hLKQJeGXxCoKvIK+PqBCLjo7GyZMnAQDm5ubYunUrOnfujNevX4tcmbgMIjz9+eef8PHxUc4r+iYFBQVh7dq1eP78OR4/fqz8u7u7O/bv34/x48fj22+/Rbly5bBq1SoEBAQUeO1EeZIB+SGvF5CHnaeQH/p6BXmLhhTy1qFzAHZBHgrEvJRkCchbbBrhv1YcM8hba1pAe2tNOoBYAN0hfz4OAFwgb4WxwH/Bh4gMlrOzMyIjI9G6dWt888036NSpk9glGQSDCE/e3t4aF/NVtXbtWq23uXTpUj5WRfSB0gDcgbzvzQ7Iw89JyFuMdKVrcDIDUEre6fRluZdwKusEE6kJcBPyAGT37zovAdT/d7oEgKqQ942RAnDCf4eoiMhoubu74/r167zEmQqDCE9Ehd4/AE5A3hLzBMDWfHqcmpD3R2oFebCJB2APoDGAapCHooqQtwQBSE9Lx/kD59GuXTuYmLMzCBFlLy0tDd999x3GjBmjFpYYnNQxPBHlRjrk/YieQd65+gKA9ZAfhnqax/u0BFAb8v45Nf+9/8aQ92dyhDwUKc5WqgN5MCIiyidpaWno06cPtm/fjpMnT2LLli0MTVlgeCIC5H2Q7kJ+NthryE/hXg95cPk7m9vlJji1BfARAH/ID5E5gmcEEZFBSUtLQ+/evbFjxw4AQFhYGC5fvowGDRqIXJlhYngi45MO+WG2awD2Aliph/t0h7xz9McA6kLeidoJHGuGiAxeamoqevfujZ07dwKQj4u4a9cuBqdsMDyRcdgMoPcH3N4dwEMAfSDvZ9QUQFfIW5TYikREhVRqaip69eqFXbt2AZAHp927d/Ps9RwwPFHRdBnADwDWQLcz1VoDKAf5YIkVIb9cAq/eQ0RFUGpqKgIDA7F7924AgKWlJXbv3o3WrVuLXJnhY3iiwi8R8tGf7wEIQ+4Ow1lAPjp0XQD1IG9RssmvAomIDEtKSgp69uyJvXv3ApAHp71798Lf31/kygoHhicqXGIg78B9A8AMyEejzi17AFcBlMmHuoiICpGvvvpKGZysrKywd+9e+Pn5iVxV4cHeGmT4lkJ++Q4J5Geq+QEYg9wFp72Qj+AtQB68GJyIiPDFF1+gUaNGsLKywv79+xmcdMSWJzJMsZB38D6kw21GQH65jxYA2kF+GRAiItJQokQJhIWF4datW2jYsKHY5RQ6DE9kOATIr+P2GeRntmXFEkAHyAeODIC83xIREWUpOTkZ7969g729vXKZra0tg1MeMTyR+BIB9IS8s3dW6gH4HfIhA3gxWSKiXEtOTkaXLl3w/PlzHDt2TC1AUd6wzxOJQwCwHPJ+TMWRdXBaCvlQAxcg7/fE4ERElGvv379H586dcejQIfz999/o0qULBEEQu6xCjy1PVKDsr9nDvIt59it1BjAfQJWCqIiIqGhSBKfw8HAAQLFixRAaGgqJhJc++FAMT5T/0gH8DJiPNkczNNO+jhnkg1oOL8C6iIiKqKSkJHTu3BlHjhwBIA9OYWFhaNq0qciVFQ0MT5R/UiDv0H08m3X6AlgMoHSBVEREVOQlJSWhY8eOOHbsGACgePHiCAsLQ5MmTUSurOhgeCL9kwGoAOBJNutcBVCjYMohIjIW7969Q8eOHREREQFAHpwOHTqExo0bi1xZ0cIO46Q/sQB8IO/UrSU4ZUzKwMG1B5GWmsbgRESkZ0lJSejQoYMyOJUoUQKHDx9mcMoHDE+kH2sAlAQQqeVvXwEQANkcGVLtUgu0LCIiYyGVSlGuXDkA/wWnRo0aiVxV0cTDdvRhrgGomcXfhgBYVYC1EBEZMVNTU6xduxbFihVDcHAwGjRoIHZJRRbDE+VNAoDBALZp+ds2AN0gH8OJiIgKjKmpKX766SexyyjyeNiOdHMZ8n5NJaAZnCoAiAfQHQxORET5LCEhAd26dcONGzfELsXoMDxR7sRB3k5ZB9r7NYUBiIJ8tHAiIspX8fHxaNOmDXbu3AlfX1/cvHlT7JKMCsMT5WwlADsAGZmWN4M8NAmQj+dERET5ThGczpw5AwBIS0tDcnKyyFUZF/Z5oqylALDM4m+3wMunEBEVsLi4OLRp0wbnzp0DAJQqVQpHjx6Fl5eXuIUZGbY8kaYkALWgPTh1h7ylicGJiKhAxcXFISAgQBmc7O3tcezYMQYnEbDlidTFQX6ITpt4sE8TEZEIYmNjERAQgD/++AMA4ODggKNHj6JWrVoiV2ac2PJE/1kL7cHpY8hbmxiciIgKXGxsLFq3bq0WnI4dO8bgJCK2PBFwA0B1LctrAvi7gGshIiI1u3fvxoULFwAAjo6OOHbsGGrU4DWuxMSWJ2P3PbQHp9lgcCIiMgBBQUGYO3cunJycEBERweBkABiejNlZAGO1LH8MYHoB10JERFmaMmUKrl+/jurVtf3apYLG8GSsJgJokmnZEcj7NrkWfDlERCT3+vVrREZGaix3cHAo+GJIK4YnY/MS8kunfJNp+TUAfgVfDhER/ScmJgZ+fn5o06YNwsLCxC6HssDwZEzeAHDWsjwM2vs9ERFRgVEEpytXriAlJQUjR45Eamqq2GWRFjzbzliEAWibaZkl5ANi8iK+RESiio6Ohp+fH65evQoAKFOmDA4ePAgLCwuRKyNt2PJkDC5BMzgNBvAeDE5ERCJ79eoVfH19lcGpbNmyiIyMRJUqvJSDoWLLU1G3D0DHTMsOAWgtQi1ERKRGEZyuX78OAHBxcUFERAQqV64scmWUHbY8FWVPoRmc1oDBiYjIALx8+RI+Pj7K4FSuXDlERkYyOBUCbHkqqhIAlMu0bBeAzgVfChERqcvIyECbNm1w48YNAICrqysiIiJQsWJFkSuj3GDLU1H0CECJTMs+AYMTEZGBMDU1RUhICMzMzFC+fHlERkYyOBUiDE9FzS8A3DItGw7gx4IvhYiIstalSxfs3LkTkZGR8PDwELsc0gEP2xUlZwEMzbSsCYDlItRCRERq3r9/DysrK7VlHTp0EKka+hBseSoq7kPzcivfADgtQi1ERKTm6dOn8PLywrfffit2KaQHbHkqCgQAH2VaFgmgZcGXQkRE6p4+fQofHx/cvXsX48aNQ4kSJRAcHCx2WfQB2PJUFNQEIFOZPwIGJyIiA/DkyRN4e3vj7t27AAAPDw/4+fFCooUdW54Ku0EArqvM+4IX+CUiMgD//PMPfHx8cP/+fQDy4BQZGQlXV1eRK6MPxZanwuwcgF8zLTsiRiFERKTq8ePH8Pb2VganihUr4vjx4wxORQTDU2HWONP8U/BadUREInv06BG8vb3x4MEDAEClSpUQGRmJcuUyj1xMhRXDU2GVefiB6wDKilEIEREpREVFwdvbGw8fPgQAVK5cmcGpCGJ4KozeQT5iuCpPMQohIiJV8fHxiI+PBwB89NFHiIiIgIuLi8hVkb4xPBU2iQAaZloWJ0YhRESUWa1atXD06FE0adKEwakI49l2hU1pAEkq81OheR07IiISjZeXF06dOgWJhJ1Qiyq2PBUmXaAenI4AmCNOKUREBNy/fx9Tp06FTCZTW87gVLSx5amwSAWwW2W+CTieExGRiO7duwcfHx88efIEMTExWL58OUxM2CZhDPgqFxaRmeZPiVEEEREB8uDk7e2NJ0+eAABOnz6NuDh2QDUWDE+FRYDK9EJwPCciIpHcvXsXLVu2xNOnTwEA1atXR0REBEqWLClyZVRQGJ4Kg68zzfcXpQoiIqN3584deHt749mzZwCAmjVrIiIiAk5OTiJXRgWJ4akw+DLTvLMoVRARGbXbt29rBKejR4/C0dFR5MqooDE8GbrETPPxolRBRGTUbt26BR8fHzx//hyAfDynY8eOMTgZKYYnQ1c8h3kiIsp3o0aNUgYnLy8vHDt2DA4ODiJXRWLhUAWGbFGm+XmiVEFEZPQ2bNgAHx8fWFlZITw8HPb29mKXRCJieDJUMwHMyrTsCxHqICIiODs7IyIiAhYWFihVqpTY5ZDIeNjOEB2GZnBKFaMQIiLjdOfOHbx7905tmbOzM4MTAWB4MkyZhyK4DsBcjEKIiIzPtWvX0KxZM3To0EEjQBEBDE+GJxZAtMr8UQCe4pRCRGRsrl69Ch8fH0RHRyMyMhJTp04VuyQyQAxPhma2ynR9AL5iFUJEZFyuXLkCHx8fxMTEAAAaNGiAWbMy96EgYngyLAKAxSrzw8UqhIjIuFy5cgV+fn54/fo1AKBhw4Y4fPgw7OzsxC2MDBLDkyEJyjTfS5QqiIiMyuXLl+Hr66sMTo0bN8bhw4dha2srcmVkqBieDMl6lWkTAMXEKoSIyDhcvHgRvr6+ePPmDQCgSZMmCAsLQ4kSJUSujAwZw5Oh2J1pPvNlWYiISK9u3rwJf39/vH37FgDQtGlTBifKFYYnQ7Er07yVGEUQERkPd3d3NGjQAADQrFkzHDx4EMWL8xpYlDOGJ0OQBmCtyvxxkeogIjIilpaW2LVrFyZPnszgRDrh5VkMwScq0/YAmolVCBFR0SYIAiQSiXLe0tISoaGhIlZEhRFbnsSWAuAXlfmh4KtCRJQPzp8/j3r16uHJkydil0KFHL+mxeacaX621rWIiOgDnDt3Dq1bt8bFixfh7e2NZ8+eiV0SFWIMT2KKgfxyLAqfgtewIyLSs7Nnz6J169aIj48HAFSoUIGDX9IHYXgSU59M80tEqYKIqMg6c+YMWrdujYSEBACAn58f9u7dC2tra5Ero8KM4UlMR1SmlwOQZLUiERHp6vTp0wgICEBionzgPH9/fwYn0guGJ7EImeY/FqUKIqIi6eTJk2rBqVWrVtizZw+srDiIHn04hiexZB7Lia1ORER6ceLECbRt2xbv3r0DAAQEBGD37t0MTqQ3DE9i8VGZbi5aFURERc6BAweUwalNmzbYtWsXgxPplcGEp2XLlsHNzQ2WlpZo2LAh/vjjj2zXX7p0KapUqQIrKyu4urpi/PjxSE5OLqBqP9DCTPMbRamCiKhICg0NxZgxY9C2bVvs3LkTlpaWYpdERYxBjDC+efNmTJgwAcuXL0fDhg2xdOlSBAQE4Pbt23ByctJYf+PGjZg8eTJWr16NJk2a4M6dOxg0aBAkEgkWL14swjPQ0ecq02UBlBOrECKiokcikeDbb79FWloaLCwsxC6HiiCDaHlavHgxhg0bhuDgYHh6emL58uWwtrbG6tWrta5/5swZNG3aFH379oWbmxtat26NPn365NhaZRBSMs0/EKUKIqIiIyIiAnfu3FFbJpFIGJwo34je8pSamoq//voLU6ZMUS4zMTGBv78/zp49q/U2TZo0wW+//YY//vgDDRo0wIMHD3DgwAEMGDAgy8dJSUlBSsp/yUUxWFpaWhrS0tL09GxyZhpoChOVzJpmkia/MLARUGzngtzexo7bvOBxmxeso0ePomvXrjAxMUGjRo3QqFEjsUsyCsa+f4senmJiYpCRkYHSpUurLS9dujRu3bql9TZ9+/ZFTEwMmjVrBkEQkJ6ejhEjRmDq1KlZPk5oaChmzZqlsTwiIqJAx/zovKezcvpul7u4ceBGgT22oQgPDxe7BKPDbV7wuM3z3+XLlzF37lykpqYCAGbMmIFx48aJW5SRSEpKErsEUYkenvIiMjISc+fOxY8//oiGDRvi3r17+PTTTzF79mx8+eWXWm8zZcoUTJgwQTkfHx8PV1dX+Pj4wN7evkDqNu1nqjbv9rsb3EzdCuSxDUFaWhrCw8PRqlUrmJvzOjQFgdu84HGbF4zw8HCEhoYqg1ODBg2wc+dO2NjYiFyZcXj9+rXYJYhK9PDk4OAAU1NTvHz5Um35y5cv4eyc+aq5cl9++SUGDBiAoUOHAgBq1qyJd+/e4eOPP8a0adNgYqLZlUsqlUIqlWosNzc3L7gPuK0q086AuaVxfrAW6DYnANzmYuA2zz+HDh1Ct27dlF0xOnfujP79+8PGxobbvIAY+3YWvcO4hYUF6tati6NHjyqXyWQyHD16FI0bN9Z6m6SkJI2AZGoqb9URhMxDdxuI9EzzvKA3EZHOwsLC0LlzZ2Vw6tatGzZu3Gj0X+ZUsERveQKACRMmICgoCPXq1UODBg2wdOlSvHv3DsHBwQCAgQMHwsXFBaGhoQCAjh07YvHixahTp47ysN2XX36Jjh07KkOUwcncfYsjihMR6eTAgQPo2rWr8lBd9+7d8fvvv4tcFRkjgwhPvXr1QnR0NGbMmIEXL17Ay8sLYWFhyk7kjx8/Vmtpmj59OiQSCaZPn46nT5/C0dERHTt2xJw5c8R6CjnbrTLtJ1oVRESF0tOnT9G9e3dlcOrZsyc2bNgAc3Nzoz/ziwqeQYQnABg9ejRGjx6t9W+RkZFq82ZmZggJCUFISEgBVKYnv6pMNxStCiKiQsnFxQXffvsthg8fjl69euG3336DmZnBfIWRkeGeV1DuqkwHilYFEVGh9fHHH8Pd3R0+Pj4MTiQq0TuMG4XoTPO1RKmCiKhQefHihcayVq1aMTiR6BieCkIvlWl/sLM4EVEOdu3aBXd3d2zatEnsUog0MDwVhAiV6WmiVUFEVCjs3LkTPXv2RHJyMvr164czZ86IXRKRGoan/Ja51dlbjCKIiAqH7du3IzAwEOnp8sHx+vXrh4YNeZYNGRaGp/ymepZdBdGqICIyeNu2bUOvXr2UwSkoKAhr1qwx3PH7yGgxPOW3JyrTQ0WrgojIoG3duhW9e/dGRkYGAGDQoEH45ZdfGJzIIDE85TfV/k7dRauCiMhgbd68GX369FEGp+DgYAYnMmgMT/ntusq0q2hVEBEZpM2bN6Nv377K4DRkyBCsWrVK6wXeiQwF9878pHo9OxMAxcQqhIjIMJUuXRpSqRQAMHToUKxYsYLBiQweRxrLT9dUpmWiVUFEZLC8vb2xf/9+7Ny5E0uXLmVwokKB4Sk/9VSZXiRaFUREBs3Hxwc+Pj5il0GUa4z4+eVupvluolRBRGRQ1q1bh5CQEAiCIHYpRHnGlqf8kvlHlLsoVRARGYxff/0VwcHBEAQBEokEM2fOFLskojxhy1N+eaoyfVq0KoiIDMLatWuVwQkAXr9+zdYnKrQYnvJDQqb5JqJUQURkEFavXo3Bgwcrw9LYsWPx3XffQSLhVdKpcGJ4yg8PVKbtxCqCiEh8v/zyC4YOHaoMTp9++imWLl3K4ESFGsNTfkhUmW4mWhVERKJauXKlWnAaN24clixZwuBEhR7DU37YrzJdW7QqiIhEs2LFCnz88cfK+QkTJmDx4sUMTlQkMDzlh+Mq0zaiVUFEJIp3797h66+/Vs5PnDgRixYtYnCiIoPhKT/8qTLNiwETkZGxsbHB0aNHUbZsWUyaNAkLFixgcKIiheM85YdUlelKolVBRCSaypUr4/Lly3BwcGBwoiKHLU/6lpRpnluYiIzAgQMHkJaWprbM0dGRwYmKJH6169sllWkX0aogIiow3333Hdq3b48+ffpoBCiioojhSd9Oqkz7i1YFEVGBWLp0KT799FMAwPbt27Fjxw6RKyLKfwxP+qYanmqIVgURUb5bsmQJxo8fr5yfMWMGAgMDRayIqGAwPOnb3yrTfqJVQUSUrxYvXowJEyYo50NCQjBr1iz2cSKjwPCkb09Upl1Fq4KIKN8sWrQIn332mXJ+5syZmDlzpngFERUwhqf85CB2AURE+rVgwQJMmjRJOf/VV18hJCRExIqICh7HedKnBLELICLKP+vWrcMXX3yhnP/6668xbdo0ESsiEgdbnvTpkco0LwhMREVMp06dUL9+fQDA3LlzGZzIaLHlSZ8eqkzXF60KIqJ8YWdnh8OHD2PXrl0YNGiQ2OUQiYYtT/q0T2XaXbQqiIj0Jjk5WW3ezs6OwYmMHsOTPp1RmWZ4IqJC7quvvkLTpk3x5s0bsUshMigMT/p0TWW6uWhVEBF9sFmzZiEkJAQXL15E69atkZqamvONiIwE+zzpS+YLAtuKUgUR0QcRBAEzZ87EV199pVzWr18/WFhYiFgVkWFheNKXEyrTDE5EVAgJgoCQkBDMnj1buWzJkiUYN26ceEURGSCGJ32ZqDLdX7QqiIjyRBAEzJgxA19//bVymepFf4noPwxP+nJdZbqnaFUQEelMEARMnz4dc+fOVS777rvvMGbMGBGrIjJcDE/68Gem+RaiVEFEpDNBEDB16lTMmzdPuez777/H6NGjRayKyLAxPOnDLpVpJwC8qDgRFRIymQxRUVHK+R9++AGjRo0SryCiQoDhSR+uqkx/J1oVREQ6MzU1xfr16wEALVq0wCeffCJyRUSGj+FJHy6rTNcUqwgiorwxMzPDxo0bIZGw2ZwoNzhIpj64qkxXEq0KIqIcCYKAr776Crdv31ZbzuBElHsMT/pwWmWa48gRkYESBAHjx49HSEgIfHx8cOfOHbFLIiqUGJ70gT/YiMjACYKAcePG4dtvvwUAvHjxAn/88YfIVREVTuzz9KEyAAj/TkvFLISISDtBEPDpp5/i+++/ByA/RLdq1Sr0788RfYnyguHpQ6n+cON1M4nIwAiCgDFjxmDZsmUA5MFp9erVGDRokLiFERViDE8f6oLKNM+0IyIDIggCRo8ejR9//BGAPDitWbMGQUFBIldGVLgxPH2oNyrTbAEnIgMhk8kwevRo/PTTTwDkwenXX3/FgAEDRK6MqPBjh/EPtVdlurFoVRARqQkLC1MGJxMTE6xbt47BiUhPGJ4+1EWV6WqiVUFEpKZdu3aYNWsWTExMsH79enYOJ9IjHrb7EM8zzduLUgURkVYzZsxAt27dUKNGDbFLISpS2PL0If5Wme4qWhVERJDJZLh27ZrGcgYnIv1jePoQ4SrTxUWrgoiMnEwmw9ChQ1G/fn0cPXpU7HKIijyGpw+helmWZqJVQURGLCMjA0OGDMGaNWuQnJyMrl274s2bNznfkIjyjH2ePsQ5lem2olVBREYqIyMDgwcPxrp16wAAZmZmWLNmDUqVKiVyZURFG8OTvriIXQARGZOMjAwEBwdj/fr1AOTBafPmzejWrZvIlREVfQxPeSVAftBT9u88Lw5MRAUkIyMDQUFB2LBhAwB5cNqyZQu6duWZK0QFgeEpr1LxX3CqK2YhRGRM0tPTERQUhI0bNwIAzM3NsXXrVnTu3FnkyoiMB8NTXj1SmS4hWhVEZGQGDx6sFpy2bduGTp06iVwVkXHh2XZ5dUZl+q1oVRCRkenUqRNMTU1hbm6O7du3MzgRiYAtT3l1SmWawxQQUQHp0aMHfv/9d1hZWaFDhw5il0NklBie8uoXlek2olVBREWcIAiQSNTPSOnZs6dI1RARwMN2+tFI7AKIqChKS0tDYGAgfvrpJ7FLISIVbHnKi7RM87wgMBHpWWpqKnr37o2dO3di27ZtMDExwfDhw8Uui4jA8JQ3z1Smq4lWBREVUampqejVqxd27doFALC0tIS7u7u4RRGREsNTXlxVmbYWrQoiKoJSU1MRGBiI3bt3A5AHpz179qBVq1YiV0ZECgxPeRGlMl1RrCKIqKhJSUlBz549sXfvXgDy4LR37174+/uLXBkRqWJ4yovjKtP1RauCiIqQlJQU9OjRA/v27QMAWFlZYe/evfDz8xO5MiLKjOEpLw6oTFcVrQoiKiJSUlLQvXt37N+/H4A8OO3btw++vr4iV0ZE2uhlqILk5GR93E3hkAIgSWW+qViFEFFR8fDhQ5w+fRoAYG1tjQMHDjA4ERmwPIcnmUyG2bNnw8XFBcWKFcODBw8AAF9++SV++eWXHG5diF3MNF9SlCqIqAipWrUqjhw5AhcXFxw4cADe3t5il0RE2chzePr666+xdu1aLFiwABYWFsrlNWrUwKpVq/RSnEG6rzL9mWhVEFERU7duXdy7dw8tW7YUuxQiykGew9O6deuwYsUK9OvXD6ampsrltWvXxq1bt/RSnEG6qTLNMZ6IKA/ev3+PZcuWQRAEteWWlpYiVUREushzeHr69CkqVaqksVwmkyEtLfMQ3EXIeZVpXtyGiHT0/v17dO7cGaNHj8bo0aM1AhQRGb48f/17enri5MmTGsu3bduGOnXqfFBRBu2oynR10aogokIoKSkJnTp1Qnh4OAB5C/79+/dzuBURGZo8D1UwY8YMBAUF4enTp5DJZNixYwdu376NdevWKccpKXLeZ5r3EqMIIiqMkpKS0LFjRxw7dgwAULx4cYSFhWltwSciw5bnlqfOnTtj7969OHLkCGxsbDBjxgzcvHkTe/fuLbqXEfgn07yF1rWIiNS8e/cOHTp0UAtOhw4dQpMmTUSujIjy4oMGyWzevLmy+dkoXFGZHiBaFURUiCiCU2RkJACgRIkSOHToEBo1aiRuYUSUZ3luefLw8MDr1681lsfGxsLDw+ODijJYgSrTiaJVQUSFRGJiItq1a6cMTra2tggPD2dwIirk8hyeoqKikJGRobE8JSUFT58+1fn+li1bBjc3N1haWqJhw4b4448/sl0/NjYWo0aNQpkyZSCVSvHRRx/hwIED2d5Grz4puIciosJp7NixOHHiBID/glODBg1EroqIPpTOh+327NmjnD506BBsbW2V8xkZGTh69Cjc3Nx0us/NmzdjwoQJWL58ORo2bIilS5ciICAAt2/fhpOTk8b6qampaNWqFZycnLBt2za4uLjg0aNHsLOz0/Xp5F7mRrYi2q2LiPTn66+/xqlTpxAdHY3w8HDUq1dP7JKISA90Dk9dunQBAEgkEgQFBan9zdzcHG5ubvjmm290us/Fixdj2LBhCA4OBgAsX74c+/fvx+rVqzF58mSN9VevXo03b97gzJkzMDc3BwCdA5vOnqtMcxw7IsqFsmXLIiIiAq9evSraQ7gQGRmdD9vJZDLIZDKUL18er169Us7LZDKkpKTg9u3b6NChQ67vLzU1FX/99Rf8/f3/K8rEBP7+/jh79qzW2+zZsweNGzfGqFGjULp0adSoUQNz587VehhRb16oTHfLv4chosIrISEBSUlJastcXFwYnIiKmDyfbffw4UO9FBATE4OMjAyULl1abXnp0qWzvMzLgwcPcOzYMfTr1w8HDhzAvXv3MHLkSKSlpSEkJETrbVJSUpCSkqKcj4+PBwCkpaXlakR0kwsmMIX8MjQyKxky0vIxqBVRiu1cpEegNzDc5gUnPj4eHTp0gJWVFUaMGMFtXoC4nxc8Y9/WHzRUwbt373D8+HE8fvwYqampan8bO3bsBxWWHZlMBicnJ6xYsQKmpqaoW7cunj59ioULF2YZnkJDQzFr1iyN5REREbC2ts7xMRtta4TSkAe8G7IbuH+AowLnlVENb2EguM3z17t37/DVV1/h9u3bAORfLFKpVOSqjA/384KTuYXV2OQ5PF26dAnt2rVDUlIS3r17h1KlSiEmJgbW1tZwcnLKdXhycHCAqakpXr58qbb85cuXcHZ21nqbMmXKwNzcXO2CxNWqVcOLFy+QmpoKCwvN0SunTJmCCRMmKOfj4+Ph6uoKHx8f2Nvb51inWeB/m6rKp1VQxbNKjrchdWlpaQgPD0erVq2UfdUof3Gb57+4uDi0b99eGZzs7e3RvXt3bvMCxP284GkbqsiY5Dk8jR8/Hh07dsTy5ctha2uLc+fOwdzcHP3798enn36a6/uxsLBA3bp1cfToUWVndJlMhqNHj2L06NFab9O0aVNs3LgRMpkMJibyblt37txBmTJltAYnAJBKpVp/CZqbm+fuzabSsGZewxwwzXpVyl6utznpDbd5/oiNjUX79u2VQ6s4ODggLCwMT5484TYXAbd5wTH27ZzncZ4uX76Mzz77DCYmJjA1NUVKSgpcXV2xYMECTJ06Vaf7mjBhAlauXIlff/0VN2/exCeffIJ3794pz74bOHAgpkyZolz/k08+wZs3b/Dpp5/izp072L9/P+bOnYtRo0bl9elkL/NFzxmciIze27dv0apVK7XgdOzYMdSqVUvkyogov+W55cnc3FzZ6uPk5ITHjx+jWrVqsLW1xT//ZL4IXPZ69eqF6OhozJgxAy9evICXlxfCwsKUncgfP36sfCwAcHV1xaFDhzB+/HjUqlULLi4u+PTTT/HFF1/k9elk70n+3C0RFU6K4PTXX38BABwdHXHs2DHUqFHD6DvSEhmDPIenOnXq4MKFC6hcuTJatmyJGTNmICYmBuvXr0eNGjV0vr/Ro0dneZhOcWkDVY0bN8a5c+d0fpw8+Vplun7BPCQRGaa3b9/C398fFy9eBCD/8Xjs2DFUr15d5MqIqKDk+bDd3LlzUaZMGQDAnDlzULJkSXzyySeIjo7Gzz//rLcCDcIFlemPRauCiAyAVCpVXs3AyckJERERDE5ERibPLU+qlxlwcnJCWFiYXgoySKoXAR4sWhVEZACsra2xd+9eDBkyBF9++SU8PT3FLomIClieW56ycvHiRZ1GGC8U7v77vxPyYYsRUWFjbW2N33//ncGJyEjlKQocOnQIEydOxNSpU/HgwQMAwK1bt9ClSxfUr18fMplMr0WK6pnKdCnRqiAikcTExKBHjx549uxZzisTkVHQOTz98ssvaNu2LdauXYv58+ejUaNG+O2339C4cWM4Ozvj2rVrOHDgQH7UKo5HKtN3s1yLiIqg6Oho+Pr6Yvv27fDx8WGAIiIAeQhP3377LebPn4+YmBhs2bIFMTEx+PHHH3H16lUsX74c1apVy486xfNWZXqSaFUQUQF79eoVfH19cfXqVQBAYmIi3r17J3JVRGQIdA5P9+/fR8+ePQEA3bp1g5mZGRYuXIhy5crpvTiDcEVl2kq0KoioACmC07Vr1wAALi4uiIyMROXKlUWujIgMgc7h6f3798oL6UokEkilUuWQBUVSlMp0ztcPJqJC7uXLl/Dx8cH169cBAOXKlWNwIiI1eRqqYNWqVShWrBgAID09HWvXroWDg4PaOrm9MLDBK64yzRNriIq0Fy9ewNfXFzdv3gQgv5pBREQEKlasKHJlRGRIdA5P5cuXx8qVK5Xzzs7OWL9+vdo6Eomk6ISnZJVpR9GqIKJ89vz5c/j6+uLWrVsA5J91ERER8PDwELkyIjI0OoenqKiofCjDgO1TmbYUrQoiymerV69WC06RkZFwd3cXuSoiMkR5HmHcaJTBf8MVuIpZCBHlpylTpuDJkyc4ePAgIiIiGJyIKEsMTzlRvfawnVhFEFF+MzExwbJlyxATEwMnJyexyyEiA8aLjWRHUJmWiFYFEeWDp0+f4tKlS2rLTExMGJyIKEcMT9lRHUxYyHItIipknjx5Am9vb/j5+eHixYtil0NEhQzDU3ZuqkzzxyhRkfDPP//A29sb9+7dw9u3bzFixAgIAn8dEVHufVB4un//PqZPn44+ffrg1atXAICDBw8qB5cr9KJVpjuKVgUR6cnjx4/h7e2N+/fvAwAqVqyIHTt2QCLhcXkiyr08h6fjx4+jZs2aOH/+PHbs2IHExEQAwJUrVxASEqK3AkX1t8p0EbtkH5GxefToEby9vfHgwQMAQKVKlRAZGVl0Ly1FRPkmz+Fp8uTJ+PrrrxEeHg4LCwvlcl9fX5w7dy6bWxYiJVSmOcYTUaEVFRUFb29vPHz4EABQuXJlBiciyrM8h6erV6+ia9euGsudnJwQExPzQUUZjBSV6UqiVUFEH0ARnBQD/H700UeIjIyEi4uLuIURUaGV5/BkZ2eH58+fayy/dOlS0flQilCZlopWBRHl0bt37+Dj44NHj+Qj3VapUgWRkZEoW7asyJURUWGW5/DUu3dvfPHFF3jx4gUkEglkMhlOnz6NiRMnYuDAgfqsUTyPVaZtRKuCiPLIxsYGEydOBABUrVoVERERKFOmjMhVEVFhl+cRxufOnYtRo0bB1dUVGRkZ8PT0REZGBvr27Yvp06frs0bxqPZ54kXViQqlUaNGoUSJEmjVqhWcnZ3FLoeIioA8hycLCwusXLkSX375Ja5du4bExETUqVMHlStX1md94lI9266UaFUQkQ5SUlIglaofZx8wYIBI1RBRUZTnw3anTp0CIL/6eLt27RAYGFi0ghMRFTr37t1D1apVsX37drFLIaIiLM/hydfXF+7u7pg6dSpu3Lihz5oMQ7rYBRCRLu7evas8q653794ICwsTuyQiKqLyHJ6ePXuGzz77DMePH0eNGjXg5eWFhQsX4smTJ/qsTzzJKtM8ZEdk0O7cuQNvb288ffoUAFCtWjXUrVtX5KqIqKjKc3hycHDA6NGjcfr0ady/fx89e/bEr7/+Cjc3N/j6+uqzRnGojvHUULQqiCgHt2/fhre3N549k1/Ju1atWjh69CgcHR1FroyIiiq9XBjY3d0dkydPxrx581CzZk0cP35cH3crrrcq0+aiVUFE2bh16xZ8fHyUY87Vrl2bwYmI8t0Hh6fTp09j5MiRKFOmDPr27YsaNWpg//79+qhNXKrhKUqsIogoKzdv3lQLTl5eXjh69CgcHBxEroyIiro8D1UwZcoUbNq0Cc+ePUOrVq3w7bffonPnzrC2ttZnfeJRHSCzgWhVEJEWN27cgK+vL16+fAkAqFOnDsLDw2Fvby9yZURkDPIcnk6cOIFJkyYhMDCwaP7Sk6lMFxOtCiLS4sWLF4iLiwMgD05HjhxBqVI8s4OICkaew9Pp06f1WYfhua4yzYsCExkUX19f7NmzB7NmzcKePXsYnIioQOkUnvbs2YO2bdvC3Nwce/bsyXbdTp06fVBholO9NEuCaFUQURZatWoFf39/SCQSsUshIiOjU3jq0qULXrx4AScnJ3Tp0iXL9SQSCTIyMj60NnGlqkxXFa0KIgLw999/Izw8HJ999pnacgYnIhKDTuFJJpNpnS6SXqtMS7Nci4jy2ZUrV+Dn54fXr18jOTkZ06ZNE7skIjJyeR6qYN26dUhJSdFYnpqainXr1n1QUQbhgsq0hWhVEBm1y5cvK4MTAOzdu1fr5w4RUUHKc3gKDg5Wnu2iKiEhAcHBwR9UlEE4pTJdBE8mJDJ0ly5dUgtOjRo1wqFDhyCVsimYiMSV5/AkCILW/gZPnjyBra3tBxVlEGxUpj8SrQoio3Tx4kX4+fnhzZs3AIDGjRvj0KFDReOzhYgKPZ2HKqhTpw4kEgkkEgn8/PxgZvbfXWRkZODhw4do06aNXosUhQ2A+H+nrcQshMi4XLx4Ef7+/nj7Vj7Mf5MmTRAWFobixYuLXBkRkZzO4Ulxlt3ly5cREBCAYsX+G0HSwsICbm5u6N69u94KFM3zf//3ELUKIqPy119/wd/fH7GxsQCApk2b4uDBgwxORGRQdA5PISEhAAA3Nzf06tULlpaWei9KdKrDFDwQrQoio5KRkYGBAwcqg1Pz5s2xf/9+BiciMjh57vMUFBRUNIMTABxWma4jWhVERsXU1BTbt2+Hs7MzWrRogQMHDjA4EZFB0qnlqVSpUrhz5w4cHBxQsmTJbAeoU3T0LJRWqkzbZLkWEelZ1apVcfLkSZQpUwY2NnzzEZFh0ik8LVmyRPlLcMmSJUV3dN/zKtOfilYFUZF38+ZNVK5cWe3Ek0qVeDFJIjJsOoWnoKAg5fSgQYP0XYvhMFWZbi9aFURF2tmzZxEQEID27dtj/fr1agGKiMiQ5bnP08WLF3H16lXl/O7du9GlSxdMnToVqamp2dyyEEhXmeYwBUR6d+bMGbRu3RoJCQnYtGkTvvnmG7FLIiLKtTyHp+HDh+POnTsAgAcPHqBXr16wtrbG1q1b8fnnn+utwAKXAuDVv9M1xCyEqGg6ffo0AgICkJiYCADw9/fHmDFjRK6KiCj38hye7ty5Ay8vLwDA1q1b0bJlS2zcuBFr167F9u3b9VVfwXukMs1r2hHp1cmTJ9WCU6tWrbBnzx5YW1uLXBkRUe590OVZZDIZAODIkSNo164dAMDV1RUxMTH6qU4M71WmeZY0kd6cOHECbdu2xbt37wAAAQEB2L17N6yseGyciAqXPIenevXq4euvv8b69etx/PhxtG8v71n98OFDlC5dWm8FFrhklelaolVBVKQcP34c7dq1UwanNm3aYNeuXQxORFQo5Tk8LV26FBcvXsTo0aMxbdo05enF27ZtQ5MmTfRWYIE7rTLNk3+IPtjZs2fVglPbtm2xc+fOojvILhEVeXmOB7Vq1VI7205h4cKFMDU11XKLQuIPlekM0aogKjKqVKmCKlWq4NKlS2jXrh127NgBqVQqdllERHn2wW0rf/31F27evAkA8PT0xP/+978PLkpU9irTLUWrgqjIKFWqFI4cOYK5c+dizpw5DE5EVOjlOTy9evUKvXr1wvHjx2FnZwcAiI2NhY+PDzZt2gRHR0d91ViwVIeoqixaFUSFmiAIalcgKFWqFBYtWiRiRURE+pPnPk9jxoxBYmIirl+/jjdv3uDNmze4du0a4uPjMXbsWH3WWLCeqkzzBzKRzsLDw+Hj44O4uDixSyEiyhd5bnkKCwvDkSNHUK1aNeUyT09PLFu2DK1bt9ZLcaK4rjJtLloVRIXS4cOH0blzZyQnJ6N169YIDw9HiRIlxC6LiEiv8tzyJJPJYG6umS7Mzc2V4z8VSqqXZnEWrQqiQufQoUPo1KkTkpPl4324uLhwKAIiKpLyHJ58fX3x6aef4tmzZ8plT58+xfjx4+Hn56eX4kSRpjLNM6mJciUsLAydO3dGSkoKAKB79+7YvHmz1h9YRESFXZ7D0w8//ID4+Hi4ubmhYsWKqFixItzd3REfH4/vv/9enzUWrOh//zcBIMluRSICgAMHDqgFpx49euD3339ncCKiIivPfZ5cXV1x8eJFHD16VDlUQbVq1eDv76+34gqc6pl25UWrgqjQ2L9/P7p164bUVPmbp2fPntiwYQODExEVaXkKT5s3b8aePXuQmpoKPz+/onNF9CSV6TeiVUFUKOzbtw/du3dXBqfAwEBs2LABZmYcmp+IijadP+V++uknjBo1CpUrV4aVlRV27NiB+/fvY+HChflRX8FSbXniAJlE2fr999+Vwal3795Yv349gxMRGQWdP+l++OEHhISEICQkBADw22+/Yfjw4UUjPCWqTHOMJ6JsrV27Fu/fv4elpSXWrVvH4ERERkPnDuMPHjxAUFCQcr5v375IT0/H8+fP9VqYKN6qTEeJVQRR4WBubo7NmzczOBGR0dE5PKWkpMDGxua/OzAxgYWFBd6/f6/XwkTxUGW6jmhVEBmkvXv34t69e2rLzM3NGZyIyOjk6VPvyy+/hLW1tXI+NTUVc+bMga2trXLZ4sWLP7y6gqbaYTxFtCqIDM727dvRu3dvODs7IzIyEhUrVhS7JCIi0egcnlq0aIHbt2+rLWvSpAkePHignFe9IGihkqEyXVe0KogMyrZt29C7d29kZGTgyZMnWL58edHo40hElEc6h6fIyMh8KMNAqF5VhleVIMLWrVvRp08fZGTIf1kMGjQI8+bNE7kqIiJx5XmE8SJJdWwnU9GqIDIImzdvVgtOwcHB+OWXX2BqyjcHERk3hidVMSrT7PNERmzTpk3o16+fMjgNGTIEq1atgokJPzKIiPhJqMpGZdpBtCqIRLVx40a14DR06FCsWLGCwYmI6F/8NFQVqTLN8ERG6O+//8aAAQMgk8k7AH788cf4+eefGZyIiFTwE1FVZZVpXteUjFDNmjUxZcoUAMCIESPw008/MTgREWXyQZ+KJ0+eRP/+/dG4cWM8ffoUALB+/XqcOnVKL8UVuHSV6VKiVUEkGolEgtmzZ2P37t1YtmwZgxMRkRZ5/mTcvn07AgICYGVlhUuXLiElRd7DOi4uDnPnztVbgQVKNTxx0GQyEtHR0WrzEokEnTp1YnAiIspCnj8dv/76ayxfvhwrV66Eufl/x7iaNm2Kixcv6qW4AsfwREZm7dq18PDwwPHjx8UuhYio0MhzeLp9+zZatGihsdzW1haxsbEfUpN4GJ7IiKxevRqDBw9GYmIi2rVrh7t374pdEhFRoZDn8OTs7KxxkVAAOHXqFDw8PD6oKNGEq0wzPFER9ssvv2Do0KEQBAEAMGzYMFSqVEnkqoiICoc8h6dhw4bh008/xfnz5yGRSPDs2TNs2LABEydOxCeffKLPGgtOcZVpO7GKIMpfK1euVAtO48ePx5IlSwrvNSmJiApYnttXJk+eDJlMBj8/PyQlJaFFixaQSqWYOHEixowZo88aC47qhYGtRauCKN+sWLECw4cPV85PmDABixYtYnAiItJBnsOTRCLBtGnTMGnSJNy7dw+JiYnw9PREsWLF9FlfwVL0eXITswii/PHzzz9jxIgRyvmJEydiwYIFDE5ERDr64HORLSws4OnpiQYNGnxQcFq2bBnc3NxgaWmJhg0b4o8//sjV7TZt2gSJRIIuXbrk+bGVFOGJ/Z2oiFmxYoVacPr8888ZnIiI8ijPMcHHxyfbD95jx47l+r42b96MCRMmYPny5WjYsCGWLl2KgIAA3L59G05OTlneLioqChMnTkTz5s11qj1LivDEi8ZTEWNrawsTExPIZDJ88cUXCA0NZXAiIsqjPIcnLy8vtfm0tDRcvnwZ165dQ1BQkE73tXjxYgwbNgzBwcEAgOXLl2P//v1YvXo1Jk+erPU2GRkZ6NevH2bNmoWTJ0/qZ3iEuH//Z8sTFTG9evWCqakprl+/jtmzZzM4ERF9gDzHhCVLlmhdPnPmTCQmJub6flJTU/HXX38pr6cFACYmJvD398fZs2ezvN1XX30FJycnDBkyBCdPnszxcVJSUpSjoANAfHw8AHnoS0tLA9IB838vaCc8FJCelq71fijv0tLS1P6n/Ke6zXv06IEePXogPZ37dn7ifl7wuM0LnrFva723sfTv3x8NGjTAokWLcrV+TEwMMjIyULp0abXlpUuXxq1bt7Te5tSpU/jll19w+fLlXNcVGhqKWbNmaSyPiIiAtbU1TFNM0QEdAAAp0hQcOnAo1/dNugkPD895Jfoge/fuhVQqRevWrQFwm4uB27zgcZsXnKSkJLFLEJXew9PZs2dhaWmp77tVSkhIwIABA7By5Uo4ODjk+nZTpkzBhAkTlPPx8fFwdXWFj48P7O3t/ztkB8CingXatWunz7IJ8l8q4eHhaNWqldolfUi/vv32W/zyyy8AgOrVq8PFxYXbvABxPy943OYF7/Xr12KXIKo8h6du3bqpzQuCgOfPn+PPP//El19+mev7cXBwgKmpKV6+fKm2/OXLl3B2dtZY//79+4iKikLHjh2Vy2QyGQDAzMwMt2/fRsWKFTVuJ5VKIZVKNZabm5trvNlMLExgYs6LouYXbduc9OObb77BpEmTlPPPnz+Hi4sLt7kIuM0LHrd5wTH27ZznhGBra6v2r1SpUvD29saBAwcQEhKS6/uxsLBA3bp1cfToUeUymUyGo0ePonHjxhrrV61aFVevXsXly5eV/zp16gQfHx9cvnwZrq6ueXtCsSrT7DBOhdCiRYswceJE5fzMmTMxY8YMESsiIiqa8hQTMjIyEBwcjJo1a6JkyZIfXMSECRMQFBSEevXqoUGDBli6dCnevXunPPtu4MCBcHFxQWhoKCwtLVGjRg2129vZ2QGAxnKdxKtMa16yj8igLViwAF988YVy/quvvsKXX35p9J06iYjyQ57Ck6mpKVq3bo2bN2/qJTz16tUL0dHRmDFjBl68eAEvLy+EhYUpO5E/fvwYJib5fBjtisp0q/x9KCJ9mjdvntrZqrNnz8b06dNFrIiIqGjL8wGqGjVq4MGDB3B3d9dLIaNHj8bo0aO1/i0yMjLb265du/bDC4hWmX734XdHVBBCQ0MxdepU5fycOXPU5omISP/y3Jzz9ddfY+LEidi3bx+eP3+O+Ph4tX+FjuqWaChaFUS59vz5c8ybN085nzlIERFR/tA5PH311Vd49+4d2rVrhytXrqBTp04oV64cSpYsiZIlS8LOzk4vh/IKXKrKdNZXhCEyGGXKlMHhw4dRokQJzJs3L8vR+ImISL90Pmw3a9YsjBgxAhEREflRj3guqExbiFYFkU4aNmyIW7duoUyZMmKXQkRkNHQOT4IgAABatmyp92JEZacyzQsDk4EKCwtDQECA2rXpGJyIiApWnvo8FcmLiqqO6l9etCqItBIEASEhIWjbti3Gjx+v/BFDREQFL09n23300Uc5Bqg3b97kqSDRVAPw9N9pezELIVKnCE6zZ88GIL/8SteuXYte6y8RUSGRp/A0a9Ys2Nra6rsWcR1RmS5iT40KL0EQ8OWXX2LOnDnKZUuXLmVwIiISUZ7CU+/eveHkVIROSVM9AmIJXp6FDIIgCJg+fTrmzp2rXPbdd99hzJgxIlZFREQ6x4Qi2d8pRWU6WbQqiJQEQcDUqVPVxnH64YcfMGrUKBGrIiIi4APOtitSXqpMe4tVBJGcIAiYPHkyFixYoFy2bNkyjBw5UsSqiIhIQefwJJPJ8qMOccWpTBeyfu5U9CxatEgtOP30008YMWKEiBUREZGqfL7abiHxWGW6vmhVEAGQ9yn08PAAACxfvpzBiYjIwLBrNADEqEwXwS5dVLi4uroiIiICJ0+eRL9+/cQuh4iIMmHLE6AentzEKoKMlSAISE1NVVtWvnx5BiciIgPF8AQAf6tMu4lVBBkjQRAwfvx4dO7cGcnJPNWTiKgwYHgC1Md5chStCjIygiDg008/xbfffouwsDD06NGjaJ7NSkRUxLDPEwD8pjJdRbQqyIgIgoAxY8Zg2bJlAOTjp/Xs2bNojqNGRFTEsOUJAIqrTPMC9ZTPBEHA6NGj1YLT2rVrERQUJHJlRESUGwxPgHp4shCtCjICMpkMo0aNwo8//ghAHpx+/fVXDBw4UOTKiIgot3jYDgCe/ft/JVGroCJOJpNh5MiR+PnnnwEAJiYm+PXXX9G/f3+RKyMiIl0wPKn2z+XWoHwik8kwYsQIrFy5EoA8OK1fvx59+/YVuTIiItIV40K6yvQt0aqgIi4lJQV37twBIA9Ov/32G/r06SNyVURElBfs85ShMt1StCqoiLOyssL+/fvh6+uLDRs2MDgRERVibHlSDU+molVBRsDGxgZHjhzhcARERIUcW54YnigfZGRkYMaMGXjx4oXacgYnIqLCj+FJNTyxHY70ICMjA4MHD8bs2bPh5+eHly9fil0SERHpEcMTW55IjzIyMhAcHIx169YBAO7cuYOLFy+KXBUREekT21oYnkhPMjIyEBQUhA0bNgAAzMzMsGXLFrRt21bkyoiISJ8YnuJVppNEq4IKufT0dAQFBWHjxo0AAHNzc2zduhWdO3cWuTIiItI3hqc0lenXolVBhVh6ejoGDhyI33//HYA8OG3btg2dOnUSuTIiIsoPDE+q4amRaFVQIZWeno7+/ftj8+bNAOTBafv27ejYsaPIlRERUX4x+g7jklSVU8fNxauDCqc1a9Yog5OFhQV27NjB4EREVMQZfXjCI5VptsORjoYMGYLBgwfDwsICO3fuRIcOHcQuiYiI8pnRhydJlErLU7J4dVDhZGJigpUrV+L8+fNo166d2OUQEVEBMPrwhGiVaU/RqqBCIjU1FXfv3lVbZmJiAi8vL3EKIiKiAmf04UkSrdLyVFm8OsjwpaamolevXmjUqBGuXLkidjlERCQSow9PaoNkfiRaFWTgUlNTERgYiF27duHNmzfo2LEjUlJSxC6LiIhEYPRdpCWXVFqerMSrgwxXSkoKevbsib179wIALC0tsXr1akilUpErIyIiMRh9eIKlyjS/CymTlJQU9OjRA/v27QMAWFlZYe/evfDz8xO5MiIiEovRhyfBTvhvpoR4dZDhSUlJQffu3bF//34A8uC0b98++Pr6ilwZERGJyejDk8nFf7t9WQGQZLsqGZHk5GR0794dBw4cAABYW1tj//798Pb2FrcwIiISndGHJ8FckF+iJS3HVclIyGQydOvWDQcPHgQgD04HDhxAy5YtRa6MiIgMAc+2K/bv/+miVkEGxMTEBK1btwYA2NjY4ODBgwxORESkZPQtT5K3/x6r4wCZpGLcuHEwNTWFl5cXmjdvLnY5RERkQIw+PCnxTDujJggCJBL1Tm9jxowRqRoiIjJkPGyncEnsAkgsSUlJaN++Pfbs2SN2KUREVAgwPCl0ELsAEkNSUhI6duyIgwcPokePHsqBMImIiLLC8KTAs+2Mzrt379ChQwccO3YMgHwcJ0dHR5GrIiIiQ8c+TwoJYhdABUkRnCIjIwEAJUqUwOHDh9GwYUNxCyMiIoPH8KRQX+wCqKAkJiaiffv2OHHiBADA1tYWhw8fRoMGDUSujIiICgOGJwVzsQuggpCYmIh27drh5MmTAOTBKTw8HPXrMz0TEVHusM+TAsNTkZeQkIC2bdsqg5OdnR2OHDnC4ERERDpheFKIErsAym9XrlzBH3/8AQAoWbIkjhw5gnr16olcFRERFTYMTwpsfCjymjVrhp07d8LZ2RlHjhxB3bp1xS6JiIgKIfZ5UuCWMArt2rXD/fv3YW1tLXYpRERUSLHlScFU7AJI3+Li4rBu3TqN5QxORET0IdjeosAtUaTExcUhICAA58+fx4sXL/D555+LXRIRERURbHlSYHgqMmJjY9G6dWucP38eALBw4ULExMSIXBURERUVjAwKHGG8SHj79i1at26NP//8EwDg4OCAY8eOwcHBQeTKiIioqGB4UqgkdgH0od6+fYtWrVrhr7/+AgA4Ojri2LFjqFGjhsiVERFRUcLDdgqWYhdAH+LNmzfw9/dXBicnJydEREQwOBERkd6x5UlBInYBlFeK4HTp0iUAQOnSpXHs2DF4enqKXBkRERVFDE8KpcUugPKqX79+asEpIiIC1apVE7kqIiIqqnjYToExstBatGgRHB0d4ezsjMjISAYnIiLKV4wMCtwShVb16tVx7NgxmJmZoWrVqmKXQ0RERRwjgwK3RKHx9u1blChRAqam/w0Lz47hRERUUHjYToHhqVB49eoVWrRogeDgYGRkZIhdDhERGSFGBgWp2AVQTl69egVfX19cv34d165dg5OTExYtWiR2WUREZGTY8qRQTOwCKDsvX76Ej48Prl+/DgAoV64cRowYIXJVRERkjBieFNgGZ7BevHgBHx8f3LhxAwDg6uqKyMhIVKrEYeGJiKjgMTIomOa8ChU8RXC6desWAKB8+fKIiIiAh4eHyJUREZGxYssTIN8KHGHc4Dx//lwtOFWoUAGRkZEMTkREJCq2PAHcCgZIEZxu374N4L/g5ObmJm5hRERk9NjyRAbJ3NwcUqn8FEg3NzcGJyIiMhgMTwCQKnYBlJmDgwOOHj2KTp06MTgREZFB4QErACgvdgGkjYODA3bv3i12GURERGrY8gQwQhqAf/75B/3790d8fLzYpRAREWWLsQHgVhDZ48eP4ePjgwcPHuDhw4cICwtD8eLFxS6LiIhIK7Y8AQxPInr06BG8vb3x4MEDAPJLsLD1iYiIDBnDEwC8FbsA4xQVFQVvb288fPgQAFC5cmVERkbCxcVF5MqIiIiyxjYXAIgWuwDjowhOjx49AgB89NFHiIiIQNmyZUWujIiIKHsG0/K0bNkyuLm5wdLSEg0bNsQff/yR5borV65E8+bNUbJkSZQsWRL+/v7Zrp+jFnm/Kenu4cOHaNmypTI4ValShcGJiIgKDYMIT5s3b8aECRMQEhKCixcvonbt2ggICMCrV6+0rh8ZGYk+ffogIiICZ8+ehaurK1q3bo2nT5/mrQC2vxWYFy9eoFWrVnj8+DEAoGrVqgxORERUqBhEeFq8eDGGDRuG4OBgeHp6Yvny5bC2tsbq1au1rr9hwwaMHDkSXl5eqFq1KlatWgWZTIajR4/mrQCGpwKza9cuZXCqVq0aIiIiUKZMGZGrIiIiyj3RY0Nqair++usvTJkyRbnMxMQE/v7+OHv2bK7uIykpCWlpaShVqlSW66SkpCAlJUU5r3pGl8xEhoy0jDxUT7pIS0vDkCFDYGJigocPH+Lw4cOwt7dHWlqa2KUVWYpty21ccLjNCx63ecEz9m0teniKiYlBRkYGSpcurba8dOnSuHXrVq7u44svvkDZsmXh7++f5TqhoaGYNWuW1r+9ev4K5w+cz33RlGfm5uYIDg7G+/fv8ddff4ldjtEIDw8XuwSjw21e8LjNC05SUpLYJYhK9PD0oebNm4dNmzYhMjISlpaWWa43ZcoUTJgwQTkfHx8PV1dXAEDpuNJo165dvtdqjO7evQszMzO4u7sjLS0N4eHhaNeuHczNzcUuzSgotnmrVq24zQsIt3nB4zYveK9fvxa7BFGJHp4cHBxgamqKly9fqi1/+fIlnJ2ds73tokWLMG/ePBw5cgS1atXKdl2pVAqpVKr1b5KWEr7h8sGdO3eUH2aq4zeZm5tzexcwbvOCx21e8LjNC46xb2fRO4xbWFigbt26ap29FZ2/GzdunOXtFixYgNmzZyMsLAz16tX7sCJMP+zmpOn27dvw9vbGs2fP8OjRI4wdO1bskoiIiPRC9JYnAJgwYQKCgoJQr149NGjQAEuXLsW7d+8QHBwMABg4cCBcXFwQGhoKAJg/fz5mzJiBjRs3ws3NDS9evAAAFCtWDMWKFdO9AIYnvbp16xZ8fHyUr0utWrWyPHOSiIiosDGI8NSrVy9ER0djxowZePHiBby8vBAWFqbsRP748WOYmPzXSPbTTz8hNTUVPXr0ULufkJAQzJw5U/cCGJ705ubNm/D19VUGJy8vLxw5coRn1RERUZFhEOEJAEaPHo3Ro0dr/VtkZKTafFRUlH4fXPSDl0XDjRs34Ovrq+y/VqdOHYSHh8Pe3l7kyoiIiPTHYMKTqNjy9MGuX78OX19f5ajwderUwZEjR7Ide4uIiKgwYpsLwPD0gV68eAEfHx9lcKpbty6DExERFVkMTwDwSOwCCrfSpUtj0KBBAIB69eohPDycwYmIiIosHrYDgGpiF1C4SSQSzJ8/H+XLl0f//v1hZ2cndklERET5huEJAIqLXUDhk5qaCgsLC+W8RCLJssM/ERFRUcLDdgAjpI6uXLmCKlWq4PTp02KXQkREVOAYngCGJx1cvnwZvr6+iIqKQps2bXhxXyIiMjoMTwDPtsulixcvwtfXF2/evAEgHzm8cuXKIldFRERUsBieACBW7AIM38WLF+Hv74+3b98CAJo2bYqwsDCUKFFC5MqIiIgKFsMTALiLXYBh++uvv+Dn56cMTs2aNcPBgwdRvDh72hMRkfFheAIAS7ELMFwXLlyAv78/YmNjAQDNmzfHgQMHGJyIiMhoMTwB7DCehT/++AOtWrVSBqcWLVowOBERkdFjeAIAc7ELMEz37t1DfHw8AKBly5bYv38/ihUrJnJVRERE4mKbC2Wpb9++SEtLw7p167Bnzx7Y2NiIXRIREZHo2PIEcITxbAQFBSE8PJzBiYiI6F8MTwDAI1EAgDNnzmD9+vUay01MuJsQEREp8LAdwD5PAE6fPo02bdrg3bt3kMlkCAoKErskIiIig8QmBcDoI+SpU6cQEBCAxMRECIKATZs2QRAEscsiIiIySAxPgFG3PJ08eVLZ4gQAAQEB2LFjByQSiciVERERGSaGJwBwErsAcRw/fhxt27ZVBqc2bdpg165dsLKyErkyIiIiw8XwBABGmBUiIyPRrl07ZXBq27Ytdu7cCUtLDrdORESUHaMPT4JEAIzsCNWxY8fQrl07JCUlAQDatWvH4ERERJRLRh+ejK2zeFJSEvr27Yv3798DANq3b48dO3ZAKpWKXBkREVHhwPBkZOHJ2toa27dvh42NDTp27Ijt27czOBEREenAyKKDFqZiF1DwmjZtijNnzqBq1aqwsLAQuxwiIqJCxehbniSJRb/D0+3btzXGbapVqxaDExERUR4YfXgq6g4dOoTatWvj888/58CXREREemD04UmoUHQDRVhYGDp37oyUlBQsWrQIGzZsELskIiKiQs/ow1NR7fN04MABZXACgO7du6NXr14iV0VERFT4MTwVQfv370fXrl2RmpoKAOjZsyd+//13mJsb8XVoiIiI9IThqYhtgX379qkFp8DAQGzcuJHBiYiISE+KWHTIgyJ02G7v3r3o1q0b0tLSAAC9e/fGhg0bYGbGESmIiIj0heGpiGyBgwcPonv37srg1KdPH6xfv57BiYiISM+KSHT4AEWk5alatWooW7YsAKBv375Yt24dgxMREVE+YHgqIlvAzc0NkZGR+OyzzxiciIiI8hG/YRPFLiDvBEGARPLfCOlubm5YtGiRiBUREREVfUWk3eUDRIldQN5s27YN3bt3V47jRERERAXD6MOT0KLwjTC+detW9O7dGzt37kTPnj2VwxIQERFR/jP68FTYOoxv3rwZffr0QUZGBgDAwcGB/ZuIiIgKEMNTIdoCmzZtQr9+/ZTBaciQIVi1ahVMTArRkyAiIirk+K1bSLbAxo0b1YLT0KFDsWLFCgYnIiKiAsZv3kJw2G7Dhg0YMGAAZDIZAODjjz/Gzz//zOBEREQkAn77GvgW+O233zBw4EBlcBo+fDh++uknBiciIiKR8BvYgFueZDIZVq5cqQxOn3zyCX788UcGJyIiIhEZ/WlaknuSnFcSiYmJCfbt24c2bdrAy8sLP/zwg9qgmERERFTwjD48CRUMe5yn4sWLIzw8HFZWVgxOREREBsDoj/8IDoYVnrZt24bo6Gi1ZdbW1gxOREREBsLow5Mh+eWXX9CzZ0/4+fkhJiZG7HKIiIhIC4YnA2nQWblyJYYOHQoAuHr1Kn799VeRKyIiIiJtGJ4MwIoVK/Dxxx8r5ydMmIAJEyaIWBERERFlheFJ5Jan5cuXY/jw4cr5zz77DIsWLWIfJyIiIgPF8CSin376CZ988olyftKkSVi4cCGDExERkQFjeBIppyxbtgwjR45Uzn/xxReYP38+gxMREZGBM/pxnsQIT+Hh4Rg9erRyfsqUKZgzZw6DExEVGEEQkJ6errzYeGGWlpYGMzMzJCcnF4nnYwhMTU1hZmbG76UsMDyJwNfXFwMGDMD69esxbdo0zJ49mzsoERWY1NRUPH/+HElJSWKXoheCIMDZ2Rn//PMPP0v1yNraGmXKlIGFhYXYpRgchicR3mempqZYs2YNOnfujG7duvHNTkQFRiaT4eHDhzA1NUXZsmVhYWFR6D+DZDIZEhMTUaxYMV77Uw8EQUBqaiqio6Px8OFDVK5cmds1E4anAvLmzRuUKlVKOW9qaoru3buLWBERGaPU1FTIZDK4urrC2tpa7HL0QiaTITU1FZaWlvyS1xMrKyuYm5vj0aNHym1L/+FeVgA/uL755htUrVoV169fz/8HIyLKBYYMygn3kaxxy+SzRYsWYeLEiYiOjoavr6/GdeuIiIiocGF4yseWpwULFmDSpEnK+dGjR8PR0TH/HpCIiIjyHfs85VN4mj9/PiZPnqyc//rrrzFt2rT8eTAiIiIqMGx5ygehoaFqwWnOnDkMTkREenD27FmYmpqiffv2Gn+LjIyERCJBbGysxt/c3NywdOlStWURERFo164d7O3tYW1tDU9PT3z22Wd4+vRpPlUPJCcnY9SoUbC3t0exYsXQvXt3vHz5MtvbvHz5EoMGDULZsmVhbW2NNm3a4O7du2rreHv/v707j4riyv4A/u1u6UWkQQYRUMQtgGM0DhgNmOioJKBJxC0aV1QEI6COJO7E1jiKMVGjkQRXcEFBMi6ZaHBcI6KJiuCGoggGPQLGrYkKNND394c/6qSFRhulW5r7OafOSb16VXXr2to3r15X/RMikUhn+eSTT4TtZ8+exbBhw+Ds7AyFQoF27dphxYoVtXKN9QEXTy955GnRokWYPXu2sB4ZGamzzhhjrObWr1+PSZMm4ejRo7h161aNj7N69Wr4+PjAwcEB//nPf5CRkYHo6Gio1WosXbr0JUasa+rUqfjvf/+LxMRE/PLLL7h16xYGDhyotz8RoX///sjOzsbu3buRlpYGFxcX+Pj44NGjRzp9g4KCkJeXJyxLliwRtqWmpsLe3h5btmzBxYsXMWfOHMyaNQurVq2qtWs1Z3zb7iVauHAhIiIihPUvv/wS06dPN2FEjDFmPh4+fIiEhAScPn0a+fn5iI2NrdH/nN68eROTJ0/G5MmTsXz5cqG9ZcuW6N69e5UjVy+DWq3G+vXrsXXrVvTq1QsAEBMTg3bt2uHXX3/FW2+9VWmfq1ev4tdff8WFCxfQvn17AE/ei+rg4IBt27Zh/PjxQt+GDRvCwcGhynOPGzdOZ71169Y4ceIEduzYofPGC/Z8eOTpJfrrU1iXLFnChRNjrG7oDKC5CZbOhoW5fft2uLu7w83NDSNHjsSGDRtARAZfbmJiIjQajd5/o21sbPTu26dPHzRq1EjvUlHgVCU1NRWlpaXw8fER2tzd3dGiRQucOHGiyn1KSkoAQOc5S2KxGDKZDMeOHdPpGxcXBzs7O7z++uuYNWvWM58gr1ardZ4/yJ4fjzy9RNOmTQMRQSKR4NNPPzV1OIwx9nzyAdTeNJ+XZv369Rg5ciQAwM/PD2q1Gr/88gu6d+9u0HGuXr0KpVIJR0dHg2NYt24dioqK9G63sLDQuy0/Px9SqbRScda0aVPk5+dXuU9FcTVr1iysXr0alpaWWL58OW7evIm8vDyh3/Dhw+Hi4gInJyecO3cOM2bMQGZmJnbs2FHlcY8fP46EhATs2bOnmqtl+nDx9JLnPPFoE2Oszqn6Ts8rdd7MzEycPHkSO3fuBAA0aNAAQ4cOxfr16w0unoioxq+kadasWY32qykLCwvs2LEDgYGBsLW1hUQigY+PD/r06aMz6hYcHCz8d4cOHeDo6IjevXvj2rVraNOmjc4xL1y4AH9/f6hUKrz33ntGuxZzwsVTDYsnIsKCBQvg6elZ5a8+GGOszjht6gCebf369SgrK4OTk5PQRkSQyWRYuXIlRCIRlEolgCe3o54e3Xnw4AGsra0BAK6urlCr1cjLyzN49KlPnz5ITk7Wu93FxUXv2yQcHByg0Wjw4MEDnfgKCgr0zlUCAE9PT6Snp0OtVkOj0aBJkybo2rUrOnfWf9+za9euAICsrCyd4ikjIwO9e/dGcHCwzhxdZhie81QDRIS5c+dCpVJh4MCB2Lt3r6lDYowxs1VWVoZNmzZh6dKlSE9PF5azZ8/CyckJ27ZtAwDhBbapqak6+2dnZ0OtVsPV1RUAMHjwYEilUp1fo/1VdRPG161bpxPD00t13weenp6wsLDAwYMHhbbMzEzk5ubCy8vrmXmwtrZGkyZNcPXqVZw+fRr+/v56+6anpwOATnF48eJF9OzZEwEBAVi4cOEzz8f045EnA0eeiAgRERFYtGgRgCcv2czJyamFwBhjjAHATz/9hPv37yMwMFAYPaowaNAgxMTEYPjw4bCyssL48ePx6aefokGDBujQoQNu3LiBGTNm4K233oK3tzcAwNnZGcuXL0dYWBgKCwsxevRotGzZEjdv3sSmTZvQqFEjvY8reJHbdtbW1ggMDER4eDhsbW2hVCoxadIkeHl56fzSzt3dHZGRkRgwYACAJxPcmzRpghYtWuD8+fOYMmUK+vfvL9xyu3btGrZu3So8s+rcuXOYOnUqunfvjo4dOwJ4cquuV69e8PX1RXh4uDDHSiKR8JsvaoLqKbVaTQDoXsi9595Hq9XSzJkzCYCwrFq1qhajNC8ajYZ27dpFGo3G1KHUG5xz43vVc15UVEQZGRlUVFRk6lCe2wcffEB9+/atcttvv/1GACg5OZnKy8upqKiIVCoVubu7k0KhoFatWlFwcDD98ccflfbdv38/+fr6UuPGjUkul5O7uzt99tlndOvWrVq7lqKiIgoJCaHGjRtTw4YNacCAAZSXl6fTBwDFxMQI6ytWrKDmzZuThYUFtWjRgiIiIqikpETYnpubS927dydbW1uSyWTUtm1bmjZtGqnVaqGPSqXS+e6qWFxcXKqNVd9n5c6dOwRA5xz1iYioBr/zNAOFhYWwtrbGvdB7aLyq8TP7ExFmzZqFL7/8UmiLiopCSEhIbYZpVkpLS7F371707du32l+ksJeHc258r3rOi4uLkZOTg1atWun8/L0u02q1KCwshFKphFjMs1Feluo+K3fv3oWdnR3UarUw16w+4dt2z4GIMGPGDHz11VdC23fffYeJEyeaMCrGGGOMmQIXT89ARJg2bZrO/e/o6GhMmDDBhFExxhhjzFS4eHrGhPGMjAysXLlSWF+9erXO8zQYY4wxVr/wzeFnaN++PX744QdIpVKsXbuWCyfGGGOsnuORp+d4VEG/fv2QlZUFZ2fn2o+HMcaMoJ7+VogZgD8j+vHI01OICIcOHarUzoUTY8wcVPwC8FkvjWWs4jPyKv5q1NR45OkvI09EhEmTJiEqKgrLly/Hv/71L5OFxRhjtUEikcDGxga3b98GADRs2LDG73l7VWi1Wmg0GhQXF/OjCl4CIsLjx49x+/Zt2NjYQCKRmDqkVw4XT/9Pq9UiLCwM33//PQDg008/hZ+fH9zd3U0cGWOMvVwV71GrKKDqOiJCUVERFApFnS8EXyU2NjbVvnOvPuPiSfSkcAoNDUV0dPSTJpEIsbGxXDgxxsySSCSCo6Mj7O3tUVpaaupwXlhpaSmOHj2K7t278y2ml8TCwoJHnKpR74snLbSYOHEi1qxZAwAQi8XYuHEjRo4caeLIGGOsdkkkErP4gpRIJCgrK4NcLufiiRnFK3NzOCoqCi1btoRcLkfXrl1x8uTJavsnJibC3d0dcrkcHTp0qPZN1tUJPxiuUzht3ryZCyfGGGOM6fVKFE8JCQkIDw+HSqXCmTNn8MYbb8DX11fv/fjjx49j2LBhCAwMRFpaGvr374/+/fvjwoULBp9704VNAJ4UTlu2bMHw4cNf6FoYY4wxZt5eieJp2bJlCAoKwtixY/H3v/8d0dHRaNiwITZs2FBl/xUrVsDPzw/Tpk1Du3btsGDBAnh4eGDVqlU1Or9EIsHWrVsxbNiwF7kMxhhjjNUDJp/zpNFokJqailmzZgltYrEYPj4+OHHiRJX7nDhxAuHh4Tptvr6+2LVrl97zlJSUoKSkRFhXq9UAABFEiI6Oho+PD+7evfsCV8KepbS0FI8fP8bdu3d5XoKRcM6Nj3NufJxz47t37x6A+vsgTZMXT3fu3EF5eTmaNm2q0960aVNcvny5yn3y8/Or7J+fn6/3PJGRkZg/f36ldgIhKCgIQUFBNYieMcYYq7/u3r0La2trU4dhdCYvnoxl1qxZOqNVDx48gIuLC3Jzc+vlH7wpFBYWwtnZGTdu3IBSqTR1OPUC59z4OOfGxzk3PrVajRYtWsDW1tbUoZiEyYsnOzs7SCQSFBQU6LQXFBTofTiXg4ODQf0BQCaTQSaTVWq3trbmv2xGplQqOedGxjk3Ps658XHOja++PtHd5FctlUrh6emJgwcPCm1arRYHDx6El5dXlft4eXnp9AeA/fv36+3PGGOMMfaymHzkCQDCw8MREBCAzp07o0uXLvjmm2/w6NEjjB07FgAwevRoNGvWDJGRkQCAKVOmoEePHli6dCnef/99xMfH4/Tp08LzmhhjjDHGassrUTwNHToUf/zxB+bOnYv8/Hx06tQJSUlJwqTw3NxcnaFBb29vbN26FREREZg9ezZee+017Nq1C6+//vpzn1Mmk0GlUlV5K4/VDs658XHOjY9zbnycc+Or7zkXUX39nSFjjDHGWA2YfM4TY4wxxlhdwsUTY4wxxpgBuHhijDHGGDMAF0+MMcYYYwYw6+IpKioKLVu2hFwuR9euXXHy5Mlq+ycmJsLd3R1yuRwdOnTA3r17jRSp+TAk52vXrsU777yDxo0bo3HjxvDx8XnmnxGrzNDPeYX4+HiIRCL079+/dgM0Q4bm/MGDBwgNDYWjoyNkMhlcXV353xcDGZrzb775Bm5ublAoFHB2dsbUqVNRXFxspGjrvqNHj+LDDz+Ek5MTRCJRte+OrXDkyBF4eHhAJpOhbdu2iI2NrfU4TYbMVHx8PEmlUtqwYQNdvHiRgoKCyMbGhgoKCqrsn5KSQhKJhJYsWUIZGRkUERFBFhYWdP78eSNHXncZmvPhw4dTVFQUpaWl0aVLl2jMmDFkbW1NN2/eNHLkdZehOa+Qk5NDzZo1o3feeYf8/f2NE6yZMDTnJSUl1LlzZ+rbty8dO3aMcnJy6MiRI5Senm7kyOsuQ3MeFxdHMpmM4uLiKCcnh/bt20eOjo40depUI0ded+3du5fmzJlDO3bsIAC0c+fOavtnZ2dTw4YNKTw8nDIyMujbb78liURCSUlJxgnYyMy2eOrSpQuFhoYK6+Xl5eTk5ESRkZFV9h8yZAi9//77Om1du3alCRMm1Gqc5sTQnD+trKyMrKysaOPGjbUVotmpSc7LysrI29ub1q1bRwEBAVw8GcjQnH///ffUunVr0mg0xgrR7Bia89DQUOrVq5dOW3h4OHXr1q1W4zRXz1M8TZ8+ndq3b6/TNnToUPL19a3FyEzHLG/baTQapKamwsfHR2gTi8Xw8fHBiRMnqtznxIkTOv0BwNfXV29/pqsmOX/a48ePUVpaWm9fNGmomub8iy++gL29PQIDA40RplmpSc5//PFHeHl5ITQ0FE2bNsXrr7+ORYsWoby83Fhh12k1ybm3tzdSU1OFW3vZ2dnYu3cv+vbta5SY66P69h36Sjxh/GW7c+cOysvLhSeUV2jatCkuX75c5T75+flV9s/Pz6+1OM1JTXL+tBkzZsDJyanSX0BWtZrk/NixY1i/fj3S09ONEKH5qUnOs7OzcejQIYwYMQJ79+5FVlYWQkJCUFpaCpVKZYyw67Sa5Hz48OG4c+cO3n77bRARysrK8Mknn2D27NnGCLle0vcdWlhYiKKiIigUChNFVjvMcuSJ1T2LFy9GfHw8du7cCblcbupwzNKff/6JUaNGYe3atbCzszN1OPWGVquFvb091qxZA09PTwwdOhRz5sxBdHS0qUMzW0eOHMGiRYvw3Xff4cyZM9ixYwf27NmDBQsWmDo0ZibMcuTJzs4OEokEBQUFOu0FBQVwcHCoch8HBweD+jNdNcl5ha+//hqLFy/GgQMH0LFjx9oM06wYmvNr167h+vXr+PDDD4U2rVYLAGjQoAEyMzPRpk2b2g26jqvJ59zR0REWFhaQSCRCW7t27ZCfnw+NRgOpVFqrMdd1Ncn5559/jlGjRmH8+PEAgA4dOuDRo0cIDg7GnDlzdN6Vyl4Ofd+hSqXS7EadADMdeZJKpfD09MTBgweFNq1Wi4MHD8LLy6vKfby8vHT6A8D+/fv19me6apJzAFiyZAkWLFiApKQkdO7c2Rihmg1Dc+7u7o7z588jPT1dWPr164eePXsiPT0dzs7Oxgy/TqrJ57xbt27IysoSClUAuHLlChwdHblweg41yfnjx48rFUgVxSvx61xrRb37DjX1jPXaEh8fTzKZjGJjYykjI4OCg4PJxsaG8vPziYho1KhRNHPmTKF/SkoKNWjQgL7++mu6dOkSqVQqflSBgQzN+eLFi0kqldIPP/xAeXl5wvLnn3+a6hLqHENz/jT+tZ3hDM15bm4uWVlZUVhYGGVmZtJPP/1E9vb29O9//9tUl1DnGJpzlUpFVlZWtG3bNsrOzqb//e9/1KZNGxoyZIipLqHO+fPPPyktLY3S0tIIAC1btozS0tLo999/JyKimTNn0qhRo4T+FY8qmDZtGl26dImioqL4UQV11bfffkstWrQgqVRKXbp0oV9//VXY1qNHDwoICNDpv337dnJ1dSWpVErt27enPXv2GDnius+QnLu4uBCASotKpTJ+4HWYoZ/zv+LiqWYMzfnx48epa9euJJPJqHXr1rRw4UIqKyszctR1myE5Ly0tpXnz5lGbNm1ILpeTs7MzhYSE0P37940feB11+PDhKv99rshzQEAA9ejRo9I+nTp1IqlUSq1bt6aYmBijx20sIiIew2SMMcYYe15mOeeJMcYYY6y2cPHEGGOMMWYALp4YY4wxxgzAxRNjjDHGmAG4eGKMMcYYMwAXT4wxxhhjBuDiiTHGGGPMAFw8MWamYmNjYWNjY+owakwkEmHXrl3V9hkzZgz69+9vlHgYY6wCF0+MvcLGjBkDkUhUacnKyjJ1aIiNjRXiEYvFaN68OcaOHYvbt2+/lOPn5eWhT58+AIDr169DJBIhPT1dp8+KFSsQGxv7Us6nz7x584TrlEgkcHZ2RnBwMO7du2fQcbjQY8x8NDB1AIyx6vn5+SEmJkanrUmTJiaKRpdSqURmZia0Wi3Onj2LsWPH4tatW9i3b98LH9vBweGZfaytrV/4PM+jffv2OHDgAMrLy3Hp0iWMGzcOarUaCQkJRjk/Y+zVwiNPjL3iZDIZHBwcdBaJRIJly5ahQ4cOsLS0hLOzM0JCQvDw4UO9xzl79ix69uwJKysrKJVKeHp64vTp08L2Y8eO4Z133oFCoYCzszMmT56MR48eVRubSCSCg4MDnJyc0KdPH0yePBkHDhxAUVERtFotvvjiCzRv3hwymQydOnVCUlKSsK9Go0FYWBgcHR0hl8vh4uKCyMhInWNX3LZr1aoVAOAf//gHRCIR/vnPfwLQHc1Zs2YNnJycoNVqdWL09/fHuHHjhPXdu3fDw8MDcrkcrVu3xvz581FWVlbtdTZo0AAODg5o1qwZfHx88NFHH2H//v3C9vLycgQGBqJVq1ZQKBRwc3PDihUrhO3z5s3Dxo0bsXv3bmEU68iRIwCAGzduYMiQIbCxsYGtrS38/f1x/fr1auNhjJkWF0+M1VFisRgrV67ExYsXsXHjRhw6dAjTp0/X23/EiBFo3rw5Tp06hdTUVMycORMWFhYAgGvXrsHPzw+DBg3CuXPnkJCQgGPHjiEsLMygmBQKBbRaLcrKyrBixQosXboUX3/9Nc6dOwdfX1/069cPV69eBQCsXLkSP/74I7Zv347MzEzExcWhZcuWVR735MmTAIADBw4gLy8PO3bsqNTno48+wt27d3H48GGh7d69e0hKSsKIESMAAMnJyRg9ejSmTJmCjIwMrF69GrGxsVi4cOFzX+P169exb98+SKVSoU2r1aJ58+ZITExERkYG5s6di9mzZ2P79u0AgM8++wxDhgyBn58f8vLykJeXB29vb5SWlsLX1xdWVlZITk5GSkoKGjVqBD8/P2g0mueOiTFmZKZ+MzFjTL+AgACSSCRkaWkpLIMHD66yb2JiIv3tb38T1mNiYsja2lpYt7KyotjY2Cr3DQwMpODgYJ225ORkEovFVFRUVOU+Tx//ypUr5OrqSp07dyYiIicnJ1q4cKHOPm+++SaFhIQQEdGkSZOoV69epNVqqzw+ANq5cycREeXk5BAASktL0+kTEBBA/v7+wrq/vz+NGzdOWF+9ejU5OTlReXk5ERH17t2bFi1apHOMzZs3k6OjY5UxEBGpVCoSi8VkaWlJcrlceLv8smXL9O5DRBQaGkqDBg3SG2vFud3c3HRyUFJSQgqFgvbt21ft8RljpsNznhh7xfXs2RPff/+9sG5paQngyShMZGQkLl++jMLCQpSVlaG4uBiPHz9Gw4YNKx0nPDwc48ePx+bNm4VbT23atAHw5JbeuXPnEBcXJ/QnImi1WuTk5KBdu3ZVxqZWq9GoUSNotVoUFxfj7bffxrp161BYWIhbt26hW7duOv27deuGs2fPAnhyy+3dd9+Fm5sb/Pz88MEHH+C99957oVyNGDECQUFB+O677yCTyRAXF4ePP/4YYrFYuM6UlBSdkaby8vJq8wYAbm5u+PHHH1FcXIwtW7YgPT0dkyZN0ukTFRWFDRs2IDc3F0VFRdBoNOjUqVO18Z49exZZWVmwsrLSaS8uLsa1a9dqkAHGmDFw8cTYK87S0hJt27bVabt+/To++OADTJw4EQsXLoStrS2OHTuGwMBAaDSaKouAefPmYfjw4dizZw9+/vlnqFQqxMfHY8CAAXj48CEmTJiAyZMnV9qvRYsWemOzsrLCmTNnIBaL4ejoCIVCAQAoLCx85nV5eHggJycHP//8Mw4cOIAhQ4bAx8cHP/zwwzP31efDDz8EEWHPnj148803kZycjOXLlwvbHz58iPnz52PgwIGV9pXL5XqPK5VKhT+DxYsX4/3338f8+fOxYMECAEB8fDw+++wzLF26FF5eXrCyssJXX32F3377rdp4Hz58CE9PT52itcKr8qMAxlhlXDwxVgelpqZCq9Vi6dKlwqhKxfya6ri6usLV1RVTp07FsGHDEBMTgwEDBsDDwwMZGRmVirRnEYvFVe6jVCrh5OSElJQU9OjRQ2hPSUlBly5ddPoNHToUQ4cOxeDBg+Hn54d79+7B1tZW53gV84vKy8urjUcul2PgwIGIi4tDVlYW3Nzc4OHhIWz38PBAZmamwdf5tIiICPTq1QsTJ04UrtPb2xshISFCn6dHjqRSaaX4PTw8kJCQAHt7eyiVyheKiTFmPDxhnLE6qG3btigtLcW3336L7OxsbN68GdHR0Xr7FxUVISwsDEeOHMHvv/+OlJQUnDp1SrgdN2PGDBw/fhxhYWFIT0/H1atXsXv3boMnjP/VtGnT8OWXXyIhIQGZmZmYOXMm0tPTMWXKFADAsmXLsG3bNly+fBlXrlxBYmIiHBwcqnywp729PRQKBZKSklBQUAC1Wq33vCNGjMCePXuwYcMGYaJ4hblz52LTpk2YP38+Ll68iEuXLiE+Ph4REREGXZuXlxc6duyIRYsWAQBee+01nD59Gvv27cOVK1fw+eef49SpUzr7tGzZEufOnUNmZibu3LmD0tJSjBgxAnZ2dvD390dycjJycnJw5MgRTJ48GTdv3jQoJsaYEZl60hVjTL+qJhlXWLZsGTk6OpJCoSBfX1/atGkTAaD79+8Tke6E7pKSEvr444/J2dmZpFIpOTk5UVhYmM5k8JMnT9K7775LjRo1IktLS+rYsWOlCd9/9fSE8aeVl5fTvHnzqFmzZmRhYUFvvPEG/fzzz8L2NWvWUKdOncjS0pKUSiX17t2bzpw5I2zHXyaMExGtXbuWnJ2dSSwWU48ePfTmp7y8nBwdHQkAXbt2rVJcSUlJ5O3tTQqFgpRKJXXp0oXWrFmj9zpUKhW98cYbldq3bdtGMpmMcnNzqbi4mMaMGUPW1tZkY2NDEydOpJkzZ+rsd/v2bSG/AOjw4cNERJSXl0ejR48mOzs7kslk1Lp1awoKCiK1Wq03JsaYaYmIiExbvjHGGGOM1R18244xxhhjzABcPDHGGGOMGYCLJ8YYY4wxA3DxxBhjjDFmAC6eGGOMMcYMwMUTY4wxxpgBuHhijDHGGDMAF0+MMcYYYwbg4okxxhhjzABcPDHGGGOMGYCLJ8YYY4wxA3DxxBhjjDFmgP8DdpUfX0BN3moAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.952 +/- 0.001\n"
     ]
    }
   ],
   "source": [
    "auc, se = compute_auc_given_dists(\n",
    "    scores_inliers,\n",
    "    scores_outliers,\n",
    "    bootstrap=True,\n",
    "    bootstrap_samples=bootstrap_samples,\n",
    "    random_state=seed,\n",
    "    plot=True,\n",
    "    title=(\n",
    "        \"English vs Non-English\\n\"\n",
    "        f\"(using depth={depth} signatures of dim-reduced transformer embeddings)\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"AUC: {auc:.3f} +/- {se:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc4f521-4a24-4dd2-aa4e-257ab4b80b83",
   "metadata": {},
   "source": [
    "As noted above, in Section 5.2.4 of [[2]](https://arxiv.org/abs/2006.03487), we consider the same language dataset example as we do here and achieved an AUC score of $0.878$ +/- $0.002$, so we have achieved a boost in performance using our dimension-reduced transformer embeddings.\n",
    "\n",
    "Looking at the CDF we can see there is good separation between the two classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9cc33f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot CDF\n",
    "plot_cdf_given_dists(\n",
    "    scores_inliers,\n",
    "    scores_outliers,\n",
    "    bins=10000000,\n",
    "    title=f\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253964b4-8ccf-4e9e-ad8c-6ae86bc375ce",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "The computations described in this notebook were performed using the Baskerville Tier 2 HPC service (https://www.baskerville.ac.uk/). Baskerville was funded by the EPSRC and UKRI through the World Class Labs scheme (EP/T022221/1) and the Digital Research Infrastructure programme (EP/W032244/1) and is operated by Advanced Research Computing at the University of Birmingham.\n",
    "\n",
    "## References\n",
    "\n",
    "[1] Devlin, J., Chang, M.W., Lee, K. and Toutanova, K., 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. _arXiv preprint arXiv:1810.04805_.\n",
    "\n",
    "[2] Shao, Z., Chan, R.S.Y., Cochrane, T., Foster, P., Lyons, T. 2023. Dimensionless Anomaly Detection on Multivariate Streams with Variance Norm and Path Signature. _arXiv preprint arXiv:2006.03487_.\n",
    "\n",
    "[3] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L. and Stoyanov, V., 2019. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_.\n",
    "\n",
    "[4] McInnes, L., and Healy, J. 2018. UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction, _arXiv preprint arXiv:1802.03426_.\n",
    "\n",
    "[5] Tipping, M. E., and Bishop, C. M., 1999. Probabilistic principal component analysis. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 61(3), 611-622.\n",
    "\n",
    "[6] van der Maaten, L.J.P., and Hinton, G.E., 2008. Visualizing High-Dimensional Data using t-SNE. _Journal of Machine Learning Research_, 9:2579-2605.\n",
    "\n",
    "[7] Mu, J., Bhat, S., and Viswanath, P. (2017). All-but-the-top: Simple and effective postprocessing for word representations. _arXiv preprint arXiv:1702.01417_.\n",
    "\n",
    "[8] Raunak, V., Gupta, V., and Metze, F. (2019). Effective dimensionality reduction for word embeddings. In _Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP- 2019)_, 235–243."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
