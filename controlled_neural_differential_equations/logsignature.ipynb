{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img align=\"left\" src=\"data_sig_logo.jpg\" width=\"450\"/>\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://datasig.ac.uk/examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Logsignature example\n",
    "This notebook is based on the examples from the `torchcde` package by Kidger and Morrill which can be found at\n",
    "[https://github.com/patrick-kidger/torchcde](https://github.com/patrick-kidger/torchcde).\n",
    "Further information about the techniques described in this notebook can be found\n",
    "\n",
    "> Morrill, J., Salvi, C., Kidger, P., Foster, J. and Lyons, T., 2020.\n",
    "  Neural rough differential equations for long time series.\n",
    "  arXiv preprint arXiv:[2009.08295](https://arxiv.org/abs/2009.08295)\n",
    "\n",
    "> Morrill, J., Kidger, P., Yang, L. and Lyons, T., 2021.\n",
    "  Neural Controlled Differential Equations for Online Prediction Tasks.\n",
    "  arXiv preprint arXiv:[2106.11028](https://arxiv.org/abs/2106.11028).\n",
    "\n",
    "> Kidger, P., Foster, J., Li, X., Oberhauser, H. and Lyons, T., 2021.\n",
    "  Neural sdes as infinite-dimensional gans.\n",
    "  arXiv preprint arXiv:[2102.03657](https://arxiv.org/abs/2102.03657).\n",
    "\n",
    "In this notebook we code up a Neural CDE using the log-ode method for a long time series thus becoming a Neural RDE.\n",
    "We will only describe the differences from that example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Set up the notebook\n",
    "\n",
    "### Install dependencies\n",
    "This notebook requires PyTorch and the torchcde package.\n",
    "The dependencies are listed in the `requirements.txt` file.\n",
    "They can be installed using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip uninstall -y enum34\n",
    "# !{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torchcde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/philipparubin/Documents/Projects/FY-21-22/Rough-Paths/repos/signature_applications/controlled_neural_differential_equations/.venv_cdes/lib/python3.11/site-packages/signatory/_impl.cpython-311-darwin.so, 0x0002): symbol not found in flat namespace '___kmpc_dispatch_deinit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msignatory\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/FY-21-22/Rough-Paths/repos/signature_applications/controlled_neural_differential_equations/.venv_cdes/lib/python3.11/site-packages/signatory/__init__.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m  \u001b[38;5;66;03m# must be imported before anything from signatory\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m impl\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified procedure could not be found\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[0;32m~/Documents/Projects/FY-21-22/Rough-Paths/repos/signature_applications/controlled_neural_differential_equations/.venv_cdes/lib/python3.11/site-packages/signatory/impl.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Provides an interface to _impl.\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# noinspection PyUnresolvedReferences\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _impl\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# For some reason some exceptions on a Mac are converted to RuntimeErrors rather than ValueErrors.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# So we have to make a conversion.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# This isn't perfect; any genuine RuntimeErrors will now always be ValueErrors.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# So for consistency across platforms we _always_ convert RuntimeErrors to ValueErrors.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_wrap\u001b[39m(fn):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# We'd like to perform a check that fn is actually a function here\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# But that throws an error with the mocking used in the documentation\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# but again it fails with autodoc.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Not super important, as nothing in this module should be public anyway.\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Users/philipparubin/Documents/Projects/FY-21-22/Rough-Paths/repos/signature_applications/controlled_neural_differential_equations/.venv_cdes/lib/python3.11/site-packages/signatory/_impl.cpython-311-darwin.so, 0x0002): symbol not found in flat namespace '___kmpc_dispatch_deinit'"
     ]
    }
   ],
   "source": [
    "import signatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also set some parameters that can be changed when experimenting with the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "HIDDEN_LAYER_WIDTH = 64  # This is the width of the hidden layer of the neural network\n",
    "NUM_EPOCHS = 10  # This is the number of training iterations we will use later\n",
    "NUM_TIMEPOINTS = 5000  # Number of time points to use in generated data.\n",
    "#  This is large to demonstrate the utility of logsignature features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We use the `CDEFunc` and `NeuralCDE` classes, and the `get_data` function defined in the _time series classificiation_\n",
    "notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CDEFunc(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels):\n",
    "        ######################\n",
    "        # input_channels is the number of input channels in the data X. (Determined by the data.)\n",
    "        # hidden_channels is the number of channels for z_t. (Determined by you!)\n",
    "        ######################\n",
    "        super(CDEFunc, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(hidden_channels, HIDDEN_LAYER_WIDTH)\n",
    "        self.linear2 = torch.nn.Linear(\n",
    "            HIDDEN_LAYER_WIDTH, input_channels * hidden_channels\n",
    "        )\n",
    "\n",
    "    ######################\n",
    "    # For most purposes the t argument can probably be ignored; unless you want your CDE to behave differently at\n",
    "    # different times, which would be unusual. But it's there if you need it!\n",
    "    ######################\n",
    "    def forward(self, t, z):\n",
    "        # z has shape (batch, hidden_channels)\n",
    "        z = self.linear1(z)\n",
    "        z = z.relu()\n",
    "        z = self.linear2(z)\n",
    "        ######################\n",
    "        # Easy-to-forget gotcha: Best results tend to be obtained by adding a final tanh nonlinearity.\n",
    "        ######################\n",
    "        z = z.tanh()\n",
    "        ######################\n",
    "        # Ignoring the batch dimension, the shape of the output tensor must be a matrix,\n",
    "        # because we need it to represent a linear map from R^input_channels to R^hidden_channels.\n",
    "        ######################\n",
    "        z = z.view(z.size(0), self.hidden_channels, self.input_channels)\n",
    "        return z\n",
    "\n",
    "\n",
    "class NeuralCDE(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, input_channels, hidden_channels, output_channels, interpolation=\"cubic\"\n",
    "    ):\n",
    "        super(NeuralCDE, self).__init__()\n",
    "\n",
    "        self.func = CDEFunc(input_channels, hidden_channels)\n",
    "        self.initial = torch.nn.Linear(input_channels, hidden_channels)\n",
    "        self.readout = torch.nn.Linear(hidden_channels, output_channels)\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def forward(self, coeffs):\n",
    "        if self.interpolation == \"cubic\":\n",
    "            X = torchcde.NaturalCubicSpline(coeffs)\n",
    "        elif self.interpolation == \"linear\":\n",
    "            X = torchcde.LinearInterpolation(coeffs)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Only 'linear' and 'cubic' interpolation methods are implemented.\"\n",
    "            )\n",
    "\n",
    "        ######################\n",
    "        # Easy to forget gotcha: Initial hidden state should be a function of the first observation.\n",
    "        ######################\n",
    "        X0 = X.evaluate(X.interval[0])\n",
    "        z0 = self.initial(X0)\n",
    "\n",
    "        ######################\n",
    "        # Actually solve the CDE.\n",
    "        ######################\n",
    "        z_T = torchcde.cdeint(X=X, z0=z0, func=self.func, t=X.interval)\n",
    "\n",
    "        ######################\n",
    "        # Both the initial value and the terminal value are returned from cdeint; extract just the terminal value,\n",
    "        # and then apply a linear map.\n",
    "        ######################\n",
    "        z_T = z_T[:, 1]\n",
    "        pred_y = self.readout(z_T)\n",
    "        return pred_y\n",
    "\n",
    "\n",
    "def get_data(num_timepoints=100):\n",
    "    t = torch.linspace(0.0, 4 * math.pi, num_timepoints)\n",
    "\n",
    "    start = torch.rand(HIDDEN_LAYER_WIDTH) * 2 * math.pi\n",
    "    x_pos = torch.cos(start.unsqueeze(1) + t.unsqueeze(0)) / (1 + 0.5 * t)\n",
    "    x_pos[: HIDDEN_LAYER_WIDTH // 2] *= -1\n",
    "    y_pos = torch.sin(start.unsqueeze(1) + t.unsqueeze(0)) / (1 + 0.5 * t)\n",
    "    x_pos += 0.01 * torch.randn_like(x_pos)\n",
    "    y_pos += 0.01 * torch.randn_like(y_pos)\n",
    "    ######################\n",
    "    # Easy to forget gotcha: time should be included as a channel; Neural CDEs need to be explicitly told the\n",
    "    # rate at which time passes. Here, we have a regularly sampled dataset, so appending time is pretty simple.\n",
    "    ######################\n",
    "    X = torch.stack([t.unsqueeze(0).repeat(HIDDEN_LAYER_WIDTH, 1), x_pos, y_pos], dim=2)\n",
    "    y = torch.zeros(HIDDEN_LAYER_WIDTH)\n",
    "    y[: HIDDEN_LAYER_WIDTH // 2] = 1\n",
    "\n",
    "    perm = torch.randperm(HIDDEN_LAYER_WIDTH)\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "\n",
    "    ######################\n",
    "    # X is a tensor of observations, of shape (batch=128, sequence=100, channels=3)\n",
    "    # y is a tensor of labels, of shape (batch=128,), either 0 or 1 corresponding to anticlockwise or clockwise respectively.\n",
    "    ######################\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can define a function that will train the model and evaluate the performance on our data set using logsignatures\n",
    "up to a specified depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "    train_X, train_y, test_X, test_y, depth, num_epochs, window_length\n",
    "):\n",
    "    # Time the training process\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Logsignature computation step\n",
    "    train_logsig = torchcde.logsig_windows(train_X, depth, window_length=window_length)\n",
    "    print(\"Logsignature shape: {}\".format(train_logsig.size()))\n",
    "\n",
    "    model = NeuralCDE(\n",
    "        input_channels=train_logsig.size(-1),\n",
    "        hidden_channels=8,\n",
    "        output_channels=1,\n",
    "        interpolation=\"linear\",\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "    train_coeffs = torchcde.linear_interpolation_coeffs(train_logsig)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_coeffs, train_y)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_dataloader:\n",
    "            batch_coeffs, batch_y = batch\n",
    "            pred_y = model(batch_coeffs).squeeze(-1)\n",
    "            loss = torch.nn.functional.binary_cross_entropy_with_logits(pred_y, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(\"Epoch: {}   Training loss: {}\".format(epoch, loss.item()))\n",
    "\n",
    "    # Remember to compute the logsignatures of the test data too!\n",
    "    test_logsig = torchcde.logsig_windows(test_X, depth, window_length=window_length)\n",
    "    test_coeffs = torchcde.linear_interpolation_coeffs(test_logsig)\n",
    "    pred_y = model(test_coeffs).squeeze(-1)\n",
    "    binary_prediction = (torch.sigmoid(pred_y) > 0.5).to(test_y.dtype)\n",
    "    prediction_matches = (binary_prediction == test_y).to(test_y.dtype)\n",
    "    proportion_correct = prediction_matches.sum() / test_y.size(0)\n",
    "    print(\"Test Accuracy: {}\".format(proportion_correct))\n",
    "\n",
    "    # Total time\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    return proportion_correct, elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we load a high frequency version of the spiral data using in `torchcde.example`.\n",
    "Each sample contains `NUM_TIMEPOINTS` time points.\n",
    "This is too long to sensibly expect a Neural CDE to handle, training time will be very long and it\n",
    "will struggle to remember information from early on in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_X, train_y = get_data(num_timepoints=NUM_TIMEPOINTS)\n",
    "test_X, test_y = get_data(num_timepoints=NUM_TIMEPOINTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We test the model over logsignature depths 1, 2, and 3, with a window length of 50. This reduces the effective\n",
    "length of the path to just 100. The only change is an application of `torchcde.logsig_windows`\n",
    "\n",
    "The raw signal has 3 input channels. Taking logsignatures of depths 1, 2, and 3 results in a path of logsignatures\n",
    "of dimension 3, 6, and 14 respectively. We see that higher logsignature depths contain more information about the\n",
    "path over the intervals, at a cost of increased numbers of channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for logsignature depth: 1\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "signatory has not been installed. Please install it from https://github.com/patrick-kidger/signatory to use the log-ODE method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m depth \u001b[38;5;129;01min\u001b[39;00m depths:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning for logsignature depth: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdepth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     acc, elapsed \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_length\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     training_times\u001b[38;5;241m.\u001b[39mappend(elapsed)\n\u001b[1;32m     11\u001b[0m     accuracies\u001b[38;5;241m.\u001b[39mappend(acc)\n",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(train_X, train_y, test_X, test_y, depth, num_epochs, window_length)\u001b[0m\n\u001b[1;32m      3\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Logsignature computation step\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m train_logsig \u001b[38;5;241m=\u001b[39m \u001b[43mtorchcde\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogsig_windows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogsignature shape: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(train_logsig\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m NeuralCDE(\n\u001b[1;32m     10\u001b[0m     input_channels\u001b[38;5;241m=\u001b[39mtrain_logsig\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), hidden_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, output_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Projects/FY-21-22/Rough-Paths/repos/signature_applications/controlled_neural_differential_equations/.venv_cdes/lib/python3.11/site-packages/torchcde/log_ode.py:133\u001b[0m, in \u001b[0;36mlogsig_windows\u001b[0;34m(x, depth, window_length, t)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlogsig_windows\u001b[39m(x, depth, window_length, t\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    111\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculates logsignatures over multiple windows, for the batch of controls given, as in the log-ODE method.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    This corresponds to a transform of the time series, and should be used prior to applying one of the interpolation\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m        always scaled such that the corresponding times are just `tensor([0., 1., ..., length - 1])`.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_logsignature_windows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/FY-21-22/Rough-Paths/repos/signature_applications/controlled_neural_differential_equations/.venv_cdes/lib/python3.11/site-packages/torchcde/log_ode.py:53\u001b[0m, in \u001b[0;36m_logsignature_windows\u001b[0;34m(x, depth, window_length, t, _version)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Flatten batch dimensions for compatibility with Signatory\u001b[39;00m\n\u001b[1;32m     52\u001b[0m flatten_X \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 53\u001b[0m first_increment \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m*\u001b[39mbatch_dimensions, \u001b[43msignatory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogsignature_channels\u001b[49m(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), depth), dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m     54\u001b[0m                               device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     55\u001b[0m first_increment[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m     56\u001b[0m logsignatures \u001b[38;5;241m=\u001b[39m [first_increment]\n",
      "File \u001b[0;32m~/Documents/Projects/FY-21-22/Rough-Paths/repos/signature_applications/controlled_neural_differential_equations/.venv_cdes/lib/python3.11/site-packages/torchcde/log_ode.py:6\u001b[0m, in \u001b[0;36mDummyModule.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignatory has not been installed. Please install it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/patrick-kidger/signatory to use the log-ODE method.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: signatory has not been installed. Please install it from https://github.com/patrick-kidger/signatory to use the log-ODE method."
     ]
    }
   ],
   "source": [
    "depths = [1, 2, 3]\n",
    "window_length = 50\n",
    "accuracies = []\n",
    "training_times = []\n",
    "for depth in depths:\n",
    "    print(f\"Running for logsignature depth: {depth}\")\n",
    "    acc, elapsed = train_and_evaluate(\n",
    "        train_X, train_y, test_X, test_y, depth, NUM_EPOCHS, window_length\n",
    "    )\n",
    "    training_times.append(elapsed)\n",
    "    accuracies.append(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, log the results to the console for a comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results\n"
     ]
    }
   ],
   "source": [
    "print(\"Final results\")\n",
    "for acc, elapsed, depth in zip(accuracies, training_times, depths):\n",
    "    print(\n",
    "        f\"Depth: {depth}\\n\\tAccuracy on test set: {acc*100:.1f}%\\n\\tTime per epoch: {elapsed/NUM_EPOCHS:.1f}s\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
