{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Logsignature example\n",
    "This notebook is based on the examples from the `torchcde` package by Kidger and Morrill which can be found at\n",
    "[https://github.com/patrick-kidger/torchcde](https://github.com/patrick-kidger/torchcde).\n",
    "Further information about the techniques described in this notebook can be found\n",
    "\n",
    "> Morrill, J., Salvi, C., Kidger, P., Foster, J. and Lyons, T., 2020.\n",
    "  Neural rough differential equations for long time series.\n",
    "  arXiv preprint arXiv:[2009.08295](https://arxiv.org/abs/2009.08295)\n",
    "\n",
    "> Morrill, J., Kidger, P., Yang, L. and Lyons, T., 2021.\n",
    "  Neural Controlled Differential Equations for Online Prediction Tasks.\n",
    "  arXiv preprint arXiv:[2106.11028](https://arxiv.org/abs/2106.11028).\n",
    "\n",
    "> Kidger, P., Foster, J., Li, X., Oberhauser, H. and Lyons, T., 2021.\n",
    "  Neural sdes as infinite-dimensional gans.\n",
    "  arXiv preprint arXiv:[2102.03657](https://arxiv.org/abs/2102.03657).\n",
    "\n",
    "In this notebook we code up a Neural CDE using the log-ode method for a long time series thus becoming a Neural RDE.\n",
    "We will only describe the differences from that example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Set up the notebook\n",
    "\n",
    "### Install dependencies\n",
    "This notebook requires PyTorch and the torchcde package.\n",
    "The dependencies are listed in the `requirements.txt` file.\n",
    "They can be installed using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip uninstall -y enum34\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torchcde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also set some parameters that can be changed when experimenting with the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "HIDDEN_LAYER_WIDTH = 64  # This is the width of the hidden layer of the neural network\n",
    "NUM_EPOCHS = 10  # This is the number of training iterations we will use later\n",
    "NUM_TIMEPOINTS = 5000  # Number of time points to use in generated data.\n",
    "#  This is large to demonstrate the utility of logsignature features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We use the `CDEFunc` and `NeuralCDE` classes, and the `get_data` function defined in the _time series classificiation_\n",
    "notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CDEFunc(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels):\n",
    "        ######################\n",
    "        # input_channels is the number of input channels in the data X. (Determined by the data.)\n",
    "        # hidden_channels is the number of channels for z_t. (Determined by you!)\n",
    "        ######################\n",
    "        super(CDEFunc, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(hidden_channels, HIDDEN_LAYER_WIDTH)\n",
    "        self.linear2 = torch.nn.Linear(HIDDEN_LAYER_WIDTH, input_channels * hidden_channels)\n",
    "\n",
    "    ######################\n",
    "    # For most purposes the t argument can probably be ignored; unless you want your CDE to behave differently at\n",
    "    # different times, which would be unusual. But it's there if you need it!\n",
    "    ######################\n",
    "    def forward(self, t, z):\n",
    "        # z has shape (batch, hidden_channels)\n",
    "        z = self.linear1(z)\n",
    "        z = z.relu()\n",
    "        z = self.linear2(z)\n",
    "        ######################\n",
    "        # Easy-to-forget gotcha: Best results tend to be obtained by adding a final tanh nonlinearity.\n",
    "        ######################\n",
    "        z = z.tanh()\n",
    "        ######################\n",
    "        # Ignoring the batch dimension, the shape of the output tensor must be a matrix,\n",
    "        # because we need it to represent a linear map from R^input_channels to R^hidden_channels.\n",
    "        ######################\n",
    "        z = z.view(z.size(0), self.hidden_channels, self.input_channels)\n",
    "        return z\n",
    "\n",
    "\n",
    "class NeuralCDE(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, output_channels, interpolation=\"cubic\"):\n",
    "        super(NeuralCDE, self).__init__()\n",
    "\n",
    "        self.func = CDEFunc(input_channels, hidden_channels)\n",
    "        self.initial = torch.nn.Linear(input_channels, hidden_channels)\n",
    "        self.readout = torch.nn.Linear(hidden_channels, output_channels)\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def forward(self, coeffs):\n",
    "        if self.interpolation == 'cubic':\n",
    "            X = torchcde.NaturalCubicSpline(coeffs)\n",
    "        elif self.interpolation == 'linear':\n",
    "            X = torchcde.LinearInterpolation(coeffs)\n",
    "        else:\n",
    "            raise ValueError(\"Only 'linear' and 'cubic' interpolation methods are implemented.\")\n",
    "\n",
    "        ######################\n",
    "        # Easy to forget gotcha: Initial hidden state should be a function of the first observation.\n",
    "        ######################\n",
    "        X0 = X.evaluate(X.interval[0])\n",
    "        z0 = self.initial(X0)\n",
    "\n",
    "        ######################\n",
    "        # Actually solve the CDE.\n",
    "        ######################\n",
    "        z_T = torchcde.cdeint(X=X,\n",
    "                              z0=z0,\n",
    "                              func=self.func,\n",
    "                              t=X.interval)\n",
    "\n",
    "        ######################\n",
    "        # Both the initial value and the terminal value are returned from cdeint; extract just the terminal value,\n",
    "        # and then apply a linear map.\n",
    "        ######################\n",
    "        z_T = z_T[:, 1]\n",
    "        pred_y = self.readout(z_T)\n",
    "        return pred_y\n",
    "\n",
    "\n",
    "def get_data(num_timepoints=100):\n",
    "    t = torch.linspace(0., 4 * math.pi, num_timepoints)\n",
    "\n",
    "    start = torch.rand(HIDDEN_LAYER_WIDTH) * 2 * math.pi\n",
    "    x_pos = torch.cos(start.unsqueeze(1) + t.unsqueeze(0)) / (1 + 0.5 * t)\n",
    "    x_pos[:HIDDEN_LAYER_WIDTH//2] *= -1\n",
    "    y_pos = torch.sin(start.unsqueeze(1) + t.unsqueeze(0)) / (1 + 0.5 * t)\n",
    "    x_pos += 0.01 * torch.randn_like(x_pos)\n",
    "    y_pos += 0.01 * torch.randn_like(y_pos)\n",
    "    ######################\n",
    "    # Easy to forget gotcha: time should be included as a channel; Neural CDEs need to be explicitly told the\n",
    "    # rate at which time passes. Here, we have a regularly sampled dataset, so appending time is pretty simple.\n",
    "    ######################\n",
    "    X = torch.stack([t.unsqueeze(0).repeat(HIDDEN_LAYER_WIDTH, 1), x_pos, y_pos], dim=2)\n",
    "    y = torch.zeros(HIDDEN_LAYER_WIDTH)\n",
    "    y[:HIDDEN_LAYER_WIDTH//2] = 1\n",
    "\n",
    "    perm = torch.randperm(HIDDEN_LAYER_WIDTH)\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "\n",
    "    ######################\n",
    "    # X is a tensor of observations, of shape (batch=128, sequence=100, channels=3)\n",
    "    # y is a tensor of labels, of shape (batch=128,), either 0 or 1 corresponding to anticlockwise or clockwise respectively.\n",
    "    ######################\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can define a function that will train the model and evaluate the performance on our data set using logsignatures\n",
    "up to a specified depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(train_X, train_y, test_X, test_y, depth, num_epochs, window_length):\n",
    "    # Time the training process\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Logsignature computation step\n",
    "    train_logsig = torchcde.logsig_windows(train_X, depth, window_length=window_length)\n",
    "    print(\"Logsignature shape: {}\".format(train_logsig.size()))\n",
    "\n",
    "    model = NeuralCDE(\n",
    "        input_channels=train_logsig.size(-1), hidden_channels=8, output_channels=1, interpolation=\"linear\"\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "    train_coeffs = torchcde.linear_interpolation_coeffs(train_logsig)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_coeffs, train_y)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_dataloader:\n",
    "            batch_coeffs, batch_y = batch\n",
    "            pred_y = model(batch_coeffs).squeeze(-1)\n",
    "            loss = torch.nn.functional.binary_cross_entropy_with_logits(pred_y, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(\"Epoch: {}   Training loss: {}\".format(epoch, loss.item()))\n",
    "\n",
    "    # Remember to compute the logsignatures of the test data too!\n",
    "    test_logsig = torchcde.logsig_windows(test_X, depth, window_length=window_length)\n",
    "    test_coeffs = torchcde.linear_interpolation_coeffs(test_logsig)\n",
    "    pred_y = model(test_coeffs).squeeze(-1)\n",
    "    binary_prediction = (torch.sigmoid(pred_y) > 0.5).to(test_y.dtype)\n",
    "    prediction_matches = (binary_prediction == test_y).to(test_y.dtype)\n",
    "    proportion_correct = prediction_matches.sum() / test_y.size(0)\n",
    "    print(\"Test Accuracy: {}\".format(proportion_correct))\n",
    "\n",
    "    # Total time\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    return proportion_correct, elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we load a high frequency version of the spiral data using in `torchcde.example`.\n",
    "Each sample contains `NUM_TIMEPOINTS` time points.\n",
    "This is too long to sensibly expect a Neural CDE to handle, training time will be very long and it\n",
    "will struggle to remember information from early on in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_X, train_y = get_data(num_timepoints=NUM_TIMEPOINTS)\n",
    "test_X, test_y = get_data(num_timepoints=NUM_TIMEPOINTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We test the model over logsignature depths 1, 2, and 3, with a window length of 50. This reduces the effective\n",
    "length of the path to just 100. The only change is an application of `torchcde.logsig_windows`\n",
    "\n",
    "The raw signal has 3 input channels. Taking logsignatures of depths 1, 2, and 3 results in a path of logsignatures\n",
    "of dimension 3, 6, and 14 respectively. We see that higher logsignature depths contain more information about the\n",
    "path over the intervals, at a cost of increased numbers of channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for logsignature depth: 1\n",
      "Logsignature shape: torch.Size([64, 101, 3])\n",
      "Epoch: 0   Training loss: 1.7253673076629639\n",
      "Epoch: 1   Training loss: 2.6841232776641846\n",
      "Epoch: 2   Training loss: 1.1095588207244873\n",
      "Epoch: 3   Training loss: 1.8698482513427734\n",
      "Epoch: 4   Training loss: 0.8444149494171143\n",
      "Epoch: 5   Training loss: 1.102584719657898\n",
      "Epoch: 6   Training loss: 0.9590306282043457\n",
      "Epoch: 7   Training loss: 1.0678613185882568\n",
      "Epoch: 8   Training loss: 0.7616084814071655\n",
      "Epoch: 9   Training loss: 0.6925854086875916\n",
      "Test Accuracy: 0.796875\n",
      "Running for logsignature depth: 2\n",
      "Logsignature shape: torch.Size([64, 101, 6])\n",
      "Epoch: 0   Training loss: 3.9483087062835693\n",
      "Epoch: 1   Training loss: 2.967172384262085\n",
      "Epoch: 2   Training loss: 1.3951165676116943\n",
      "Epoch: 3   Training loss: 0.6525543332099915\n",
      "Epoch: 4   Training loss: 0.5654739141464233\n",
      "Epoch: 5   Training loss: 0.6235690712928772\n",
      "Epoch: 6   Training loss: 0.643418550491333\n",
      "Epoch: 7   Training loss: 0.7490644454956055\n",
      "Epoch: 8   Training loss: 0.6644153594970703\n",
      "Epoch: 9   Training loss: 0.6092175841331482\n",
      "Test Accuracy: 0.703125\n",
      "Running for logsignature depth: 3\n",
      "Logsignature shape: torch.Size([64, 101, 14])\n",
      "Epoch: 0   Training loss: 9.29626750946045\n",
      "Epoch: 1   Training loss: 2.3605875968933105\n",
      "Epoch: 2   Training loss: 0.9953503608703613\n",
      "Epoch: 3   Training loss: 1.4490458965301514\n",
      "Epoch: 4   Training loss: 0.6993889212608337\n",
      "Epoch: 5   Training loss: 1.3962339162826538\n",
      "Epoch: 6   Training loss: 0.7141188979148865\n",
      "Epoch: 7   Training loss: 0.7587863206863403\n",
      "Epoch: 8   Training loss: 0.8748772144317627\n",
      "Epoch: 9   Training loss: 0.6787529587745667\n",
      "Test Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "depths = [1, 2, 3]\n",
    "window_length = 50\n",
    "accuracies = []\n",
    "training_times = []\n",
    "for depth in depths:\n",
    "    print(f'Running for logsignature depth: {depth}')\n",
    "    acc, elapsed = train_and_evaluate(\n",
    "        train_X, train_y, test_X, test_y, depth, NUM_EPOCHS, window_length\n",
    "    )\n",
    "    training_times.append(elapsed)\n",
    "    accuracies.append(acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, log the results to the console for a comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results\n",
      "Depth: 1\n",
      "\tAccuracy on test set: 79.7%\n",
      "\tTime per epoch: 4.7s\n",
      "Depth: 2\n",
      "\tAccuracy on test set: 70.3%\n",
      "\tTime per epoch: 6.9s\n",
      "Depth: 3\n",
      "\tAccuracy on test set: 50.0%\n",
      "\tTime per epoch: 5.2s\n"
     ]
    }
   ],
   "source": [
    "print(\"Final results\")\n",
    "for acc, elapsed, depth in zip(accuracies, training_times, depths):\n",
    "    print(\n",
    "        f\"Depth: {depth}\\n\\tAccuracy on test set: {acc*100:.1f}%\\n\\tTime per epoch: {elapsed/NUM_EPOCHS:.1f}s\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
