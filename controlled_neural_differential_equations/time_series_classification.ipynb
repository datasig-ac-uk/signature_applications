{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Controlled Neural Differential Equations - Time Series Classification\n",
    "This notebook is based on the examples from the `torchcde` package by Kidger and Morrill which can be found at\n",
    "[https://github.com/patrick-kidger/torchcde](https://github.com/patrick-kidger/torchcde).\n",
    "Further information about the techniques described in this notebook can be found\n",
    "\n",
    "> Kidger, P., Morrill, J., Foster, J. and Lyons, T., 2020.\n",
    " Neural controlled differential equations for irregular time series.\n",
    " arXiv preprint arXiv:[2005.08926](https://arxiv.org/abs/2005.08926).\n",
    "\n",
    "> Morrill, J., Kidger, P., Yang, L. and Lyons, T., 2021.\n",
    " Neural Controlled Differential Equations for Online Prediction Tasks.\n",
    " arXiv preprint arXiv:[2106.11028](https://arxiv.org/abs/2106.11028).\n",
    "\n",
    "> Kidger, P., Foster, J., Li, X., Oberhauser, H. and Lyons, T., 2021.\n",
    " Neural sdes as infinite-dimensional gans.\n",
    " arXiv preprint arXiv:[2102.03657](https://arxiv.org/abs/2102.03657).\n",
    "\n",
    "In this notebook we explore the classification of time series data using controlled neural differential equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Set up the notebook\n",
    "\n",
    "### Install dependencies\n",
    "This notebook requires PyTorch and the torchcde package.\n",
    "The dependencies are listed in the `requirements.txt` file.\n",
    "They can be installed using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip uninstall -y enum34\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchcde\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Also set some parameters that can be changed when experimenting with the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "HIDDEN_LAYER_WIDTH = 64  # This is the width of the hidden layer of the neural network\n",
    "NUM_EPOCHS = 30  # This is the number of training iterations we will use later\n",
    "BATCH_SIZE = 32  # Size of batch used in the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Set up the helper classes\n",
    "A controlled differential equation (CDE) model looks like\n",
    "\n",
    "$$z_t = z_0 + \\int_0^t f_\\theta(z_s) dX_s$$\n",
    "\n",
    "where $X$ is your data and $f_\\theta$ is a neural network.\n",
    "So the first thing we need to do is define such an $f_\\theta$.\n",
    "That's what this `CDEFunc` class does.\n",
    "Here we've built a small single-hidden-layer neural network, whose hidden layer is of width `HIDDEN_LAYER_WIDTH`\n",
    "set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CDEFunc(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels):\n",
    "        ######################\n",
    "        # input_channels is the number of input channels in the data X. (Determined by the data.)\n",
    "        # hidden_channels is the number of channels for z_t. (Determined by you!)\n",
    "        ######################\n",
    "        super(CDEFunc, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(hidden_channels, HIDDEN_LAYER_WIDTH)\n",
    "        self.linear2 = torch.nn.Linear(HIDDEN_LAYER_WIDTH, input_channels * hidden_channels)\n",
    "\n",
    "    ######################\n",
    "    # For most purposes the t argument can probably be ignored; unless you want your CDE to behave differently at\n",
    "    # different times, which would be unusual. But it's there if you need it!\n",
    "    ######################\n",
    "    def forward(self, t, z):\n",
    "        # z has shape (batch, hidden_channels)\n",
    "        z = self.linear1(z)\n",
    "        z = z.relu()\n",
    "        z = self.linear2(z)\n",
    "        ######################\n",
    "        # Easy-to-forget gotcha: Best results tend to be obtained by adding a final tanh nonlinearity.\n",
    "        ######################\n",
    "        z = z.tanh()\n",
    "        ######################\n",
    "        # Ignoring the batch dimension, the shape of the output tensor must be a matrix,\n",
    "        # because we need it to represent a linear map from R^input_channels to R^hidden_channels.\n",
    "        ######################\n",
    "        z = z.view(z.size(0), self.hidden_channels, self.input_channels)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we need to package CDEFunc up into a model that computes the integral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralCDE(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, output_channels, interpolation=\"cubic\"):\n",
    "        super(NeuralCDE, self).__init__()\n",
    "\n",
    "        self.func = CDEFunc(input_channels, hidden_channels)\n",
    "        self.initial = torch.nn.Linear(input_channels, hidden_channels)\n",
    "        self.readout = torch.nn.Linear(hidden_channels, output_channels)\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def forward(self, coeffs):\n",
    "        if self.interpolation == 'cubic':\n",
    "            X = torchcde.NaturalCubicSpline(coeffs)\n",
    "        elif self.interpolation == 'linear':\n",
    "            X = torchcde.LinearInterpolation(coeffs)\n",
    "        else:\n",
    "            raise ValueError(\"Only 'linear' and 'cubic' interpolation methods are implemented.\")\n",
    "\n",
    "        ######################\n",
    "        # Easy to forget gotcha: Initial hidden state should be a function of the first observation.\n",
    "        ######################\n",
    "        X0 = X.evaluate(X.interval[0])\n",
    "        z0 = self.initial(X0)\n",
    "\n",
    "        ######################\n",
    "        # Actually solve the CDE.\n",
    "        ######################\n",
    "        z_T = torchcde.cdeint(X=X,\n",
    "                              z0=z0,\n",
    "                              func=self.func,\n",
    "                              t=X.interval)\n",
    "\n",
    "        ######################\n",
    "        # Both the initial value and the terminal value are returned from cdeint; extract just the terminal value,\n",
    "        # and then apply a linear map.\n",
    "        ######################\n",
    "        z_T = z_T[:, 1]\n",
    "        pred_y = self.readout(z_T)\n",
    "        return pred_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we need some data.\n",
    "Here we have a simple example which generates some spirals, some going clockwise, some going anticlockwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(num_timepoints=100):\n",
    "    \"\"\"\n",
    "    Generate time series data with `num_timepoints` data points\n",
    "    \"\"\"\n",
    "    t = torch.linspace(0., 4 * math.pi, num_timepoints)\n",
    "\n",
    "    start = torch.rand(HIDDEN_LAYER_WIDTH) * 2 * math.pi\n",
    "    x_pos = torch.cos(start.unsqueeze(1) + t.unsqueeze(0)) / (1 + 0.5 * t)\n",
    "    x_pos[:HIDDEN_LAYER_WIDTH//2] *= -1\n",
    "    y_pos = torch.sin(start.unsqueeze(1) + t.unsqueeze(0)) / (1 + 0.5 * t)\n",
    "    x_pos += 0.01 * torch.randn_like(x_pos)\n",
    "    y_pos += 0.01 * torch.randn_like(y_pos)\n",
    "    ######################\n",
    "    # Easy to forget gotcha: time should be included as a channel; Neural CDEs need to be explicitly told the\n",
    "    # rate at which time passes. Here, we have a regularly sampled dataset, so appending time is pretty simple.\n",
    "    ######################\n",
    "    X = torch.stack([t.unsqueeze(0).repeat(HIDDEN_LAYER_WIDTH, 1), x_pos, y_pos], dim=2)\n",
    "    y = torch.zeros(HIDDEN_LAYER_WIDTH)\n",
    "    y[:HIDDEN_LAYER_WIDTH//2] = 1\n",
    "\n",
    "    perm = torch.randperm(HIDDEN_LAYER_WIDTH)\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "\n",
    "    ######################\n",
    "    # X is a tensor of observations, of shape (batch=128, sequence=100, channels=3)\n",
    "    # y is a tensor of labels, of shape (batch=128,), either 0 or 1 corresponding to anticlockwise or clockwise respectively.\n",
    "    ######################\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Getting the training data\n",
    "Now we use the `get_data` function we just defined to create a set of data to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_X, train_y = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To illustrate the kind of data, we plot examples of the anticlockwise and clockwise spirals from this training set.\n",
    "The radius of the spiral decreases as time progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, tight_layout=True)\n",
    "idx, = np.where(train_y == 0)\n",
    "ax1.plot(train_X[idx[0], :, 1], train_X[idx[0], :, 2])\n",
    "ax1.set_title(\"Anticlockwise spiral\")\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"y\")\n",
    "\n",
    "idx, = np.where(train_y == 1)\n",
    "ax2.plot(train_X[idx[0], :, 1], train_X[idx[0], :, 2])\n",
    "ax2.set_title(\"Clockwise spiral\")\n",
    "ax2.set_xlabel(\"x\")\n",
    "ax2.set_ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "input_channels=3 because we have both the horizontal and vertical position of a point in the spiral, and time.\n",
    "hidden_channels=8 is the number of hidden channels for the evolving $z_t$, which we get to choose.\n",
    "output_channels=1 because we're doing binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = NeuralCDE(input_channels=3, hidden_channels=8, output_channels=1)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we turn our dataset into a continuous path. We do this here via natural cubic spline interpolation.\n",
    "The resulting `train_coeffs` is a tensor describing the path.\n",
    "For most problems, it's probably easiest to save this tensor and treat it as the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_coeffs = torchcde.natural_cubic_coeffs(train_X)\n",
    "train_dataset = torch.utils.data.TensorDataset(train_coeffs, train_y)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch in train_dataloader:\n",
    "        batch_coeffs, batch_y = batch\n",
    "        pred_y = model(batch_coeffs).squeeze(-1)\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(pred_y, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print('Epoch: {:2d}   Training loss: {}'.format(epoch, loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Validating the model\n",
    "Now we need to validate the model that we have trained.\n",
    "First we create some new data using the same function as before.\n",
    "Then we proceed as before, fitting cubic splines and evaluating the newly trained model on these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_X, test_y = get_data()\n",
    "\n",
    "test_coeffs = torchcde.natural_cubic_coeffs(test_X)\n",
    "pred_y = model(test_coeffs).squeeze(-1)\n",
    "binary_prediction = (torch.sigmoid(pred_y) > 0.5).to(test_y.dtype)\n",
    "prediction_matches = (binary_prediction == test_y).to(test_y.dtype)\n",
    "proportion_correct = prediction_matches.sum() / test_y.size(0)\n",
    "print('Test Accuracy: {}'.format(proportion_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
